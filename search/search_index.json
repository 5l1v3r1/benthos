{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Welcome to the Benthos documentation site, if you haven't used Benthos before check out the getting started guide . Core Components Inputs Buffers Pipeline Processors Conditions Outputs Other Components Caches Rate Limits Metrics Tracers Other Sections Core Concepts describes various Benthos concepts such as: Configuration Monitoring Mutating And Filtering Content Content Based Multiplexing Sharing Resources Across Processors Maximising IO Throughput Maximising CPU Utilisation Message Batching explains how multiple part messages and message batching works within Benthos. Error Handling explains how you can handle errors from processor steps in order to recover or reroute the data. Workflows explains how Benthos can be configured to easily support complex processor flows using automatic DAG resolution. Making Configuration Easier explains some of the tools provided by Benthos that help make writing configs easier. Config Interpolation explains how to incorporate environment variables and dynamic values into your config files.","title":"Home"},{"location":"#documentation","text":"Welcome to the Benthos documentation site, if you haven't used Benthos before check out the getting started guide .","title":"Documentation"},{"location":"#core-components","text":"Inputs Buffers Pipeline Processors Conditions Outputs","title":"Core Components"},{"location":"#other-components","text":"Caches Rate Limits Metrics Tracers","title":"Other Components"},{"location":"#other-sections","text":"Core Concepts describes various Benthos concepts such as: Configuration Monitoring Mutating And Filtering Content Content Based Multiplexing Sharing Resources Across Processors Maximising IO Throughput Maximising CPU Utilisation Message Batching explains how multiple part messages and message batching works within Benthos. Error Handling explains how you can handle errors from processor steps in order to recover or reroute the data. Workflows explains how Benthos can be configured to easily support complex processor flows using automatic DAG resolution. Making Configuration Easier explains some of the tools provided by Benthos that help make writing configs easier. Config Interpolation explains how to incorporate environment variables and dynamic values into your config files.","title":"Other Sections"},{"location":"batching/","text":"Message Batching Benthos is able to read and write over protocols that support multiple part messages, and all payloads travelling through Benthos are represented as a multiple part message. Therefore, all components within Benthos are able to work with multiple parts in a message as standard. When messages reach an output that doesn't support multiple parts the message is broken down into an individual message per part, and then one of two behaviours happen depending on the output. If the output supports batch sending messages then the collection of messages are sent as a single batch. Otherwise, Benthos falls back to sending the messages sequentially in multiple, individual requests. This behaviour means that not only can multiple part message protocols be easily matched with single part protocols, but also the concept of multiple part messages and message batches are interchangeable within Benthos. Creating Batches Most input types have some way of creating batches of messages from their input. For example, the kafka and kafka_balanced inputs have the field max_batch_count , which specifies the maximum count of prefetched messages each batch should contain (defaults at 1). These input specific methods are usually the most efficient and should therefore be preferred. Alternatively, there are also processors within Benthos that can expand and contract batches, these are the batch and split processors. The batch processor continuously reads messages until a target size has been reached, then the batch continues through the pipeline. As messages are read and stored in a batch the input they originated from is told to grab the next message but defer from acknowledging the current one, this allows the entire batch of messages to be acknowledged at the same time and only when they have reached their final destination. It is good practice to always expand or contract batches within the input section. Like this, for example: input: type: broker broker: inputs: - type: foo - type: bar processors: - type: batch batch: byte_size: 20_000_000 You should never configure processors that remove payloads ( filter , dedupe , etc) before a batch processor as this can potentially break acknowledgement propagation. Instead, always batch first and then configure your processors to work with the batch . Archiving Batches Batches of messages can be condensed into a single message part using a selected scheme such as tar with the archive processor, the opposite is also possible with the unarchive processor. This allows you to send and receive message batches over protocols that do not support batching or multiple part messages. Processing Batches Most processors will perform the same action on each message part of a batch, and therefore they behave the same regardless of whether they see a single message or a batch of plenty. However, some processors act on an entire batch and this might be undesirable. For example, imagine we batch our messages for the purpose of throughput optimisation but also need to perform deduplication on the payloads. The dedupe processor works batch wide, so in this case we need to force the processor to work as though each batched message is its own batch. We can do this with the process_batch processor: input: type: foo processors: - type: batch batch: byte_size: 20_000_000 - type: process_batch process_batch: - type: dedupe dedupe: cache: foocache key: ${!json_field:foo.bar} The dedupe processor will now treat all items of the batch as if it were a batch of one message. Whatever messages result from the child processors will continue as their own batch.","title":"Message Batching"},{"location":"batching/#message-batching","text":"Benthos is able to read and write over protocols that support multiple part messages, and all payloads travelling through Benthos are represented as a multiple part message. Therefore, all components within Benthos are able to work with multiple parts in a message as standard. When messages reach an output that doesn't support multiple parts the message is broken down into an individual message per part, and then one of two behaviours happen depending on the output. If the output supports batch sending messages then the collection of messages are sent as a single batch. Otherwise, Benthos falls back to sending the messages sequentially in multiple, individual requests. This behaviour means that not only can multiple part message protocols be easily matched with single part protocols, but also the concept of multiple part messages and message batches are interchangeable within Benthos.","title":"Message Batching"},{"location":"batching/#creating-batches","text":"Most input types have some way of creating batches of messages from their input. For example, the kafka and kafka_balanced inputs have the field max_batch_count , which specifies the maximum count of prefetched messages each batch should contain (defaults at 1). These input specific methods are usually the most efficient and should therefore be preferred. Alternatively, there are also processors within Benthos that can expand and contract batches, these are the batch and split processors. The batch processor continuously reads messages until a target size has been reached, then the batch continues through the pipeline. As messages are read and stored in a batch the input they originated from is told to grab the next message but defer from acknowledging the current one, this allows the entire batch of messages to be acknowledged at the same time and only when they have reached their final destination. It is good practice to always expand or contract batches within the input section. Like this, for example: input: type: broker broker: inputs: - type: foo - type: bar processors: - type: batch batch: byte_size: 20_000_000 You should never configure processors that remove payloads ( filter , dedupe , etc) before a batch processor as this can potentially break acknowledgement propagation. Instead, always batch first and then configure your processors to work with the batch .","title":"Creating Batches"},{"location":"batching/#archiving-batches","text":"Batches of messages can be condensed into a single message part using a selected scheme such as tar with the archive processor, the opposite is also possible with the unarchive processor. This allows you to send and receive message batches over protocols that do not support batching or multiple part messages.","title":"Archiving Batches"},{"location":"batching/#processing-batches","text":"Most processors will perform the same action on each message part of a batch, and therefore they behave the same regardless of whether they see a single message or a batch of plenty. However, some processors act on an entire batch and this might be undesirable. For example, imagine we batch our messages for the purpose of throughput optimisation but also need to perform deduplication on the payloads. The dedupe processor works batch wide, so in this case we need to force the processor to work as though each batched message is its own batch. We can do this with the process_batch processor: input: type: foo processors: - type: batch batch: byte_size: 20_000_000 - type: process_batch process_batch: - type: dedupe dedupe: cache: foocache key: ${!json_field:foo.bar} The dedupe processor will now treat all items of the batch as if it were a batch of one message. Whatever messages result from the child processors will continue as their own batch.","title":"Processing Batches"},{"location":"concepts/","text":"Benthos Concepts Contents Configuration Monitoring Mutating And Filtering Content Content Based Multiplexing Sharing Resources Across Processors Maximising IO Throughput Maximising CPU Utilisation Configuration A Benthos configuration consists of a number of root sections, the key parts being: input buffer pipeline output There are also sections for metrics , logging and http server options. Config examples for every input, output and processor type can be found here . Benthos provides lots of tooling to try and make writing configuration easier, you can read about them here . Monitoring Health Checks Benthos serves two HTTP endpoints for health checks: /ping can be used as a liveness probe as it always returns a 200. /ready can be used as a readiness probe as it serves a 200 only when both the input and output are connected, otherwise a 503 is returned. Metrics Benthos exposes lots of metrics either to Statsd, Prometheus or for debugging purposes an HTTP endpoint that returns a JSON formatted object. The target destination of Benthos metrics is configurable from the metrics section , where it's also possible to rename, whitelist or blacklist certain metric paths. Tracing Benthos also emits opentracing events to a tracer of your choice, which can be used to visualise the processors within a pipeline. Mutating And Filtering Content Benthos bridges different transport and storage mediums of data, but these often require the data to be represented in different ways. For example, we might be reading .tgz archives of messages from Amazon S3, but we need to decompress and unarchive the messages before sending them to Kafka. For this purpose we can use processors, which you can read about in more detail here . Processors can be attributed to both inputs and outputs, meaning you can be specific about which processors apply to data from specific sources or to specific sinks. Content Based Multiplexing It is possible to perform content based multiplexing of messages to specific outputs using a switch output with two or more conditional outputs. Conditions are content aware logical operators that can be combined using boolean logic. For example, say we have an output foo that we only want to receive messages that contain the word foo , and an output bar that we wish to send everything that foo doesn't receive, we can achieve that with this config: output: type: switch switch: outputs: - output: type: foo foo: foo_field_1: value1 condition: type: text text: operator: contains arg: foo - output: type: bar bar: bar_field_1: value2 Another method of content based multiplexing is with an output broker with the fan_out pattern and a filter processor on each output, which is a processor that drops messages if the condition does not pass. For example, the equivalent config for the previous example would be: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: foo_field_1: value1 processors: - type: filter_parts filter_parts: type: text text: operator: contains arg: foo - type: bar bar: bar_field_1: value2 processors: - type: filter_parts filter_parts: type: not not: type: text text: operator: contains arg: foo For more information regarding conditions, including the full list of conditions available, please read the docs here . Sharing Resources Across Processors Sometimes it is advantageous to share configurations for resources such as caches or complex conditions between processors when they would otherwise be duplicated. For this purpose there is a resource section in a Benthos config where caches , conditions and rate limits can be configured to a label that is referred to by any components that wish to use them. For example, let's imagine we have three inputs, two of which we wish to deduplicate using a shared cache. We also have two outputs, one of which only receives messages that satisfy a condition and the other receives the logical NOT of that same condition. In this example we can save ourselves the trouble of configuring the same cache and condition twice by referring to them as resources like this: input: type: broker broker: inputs: - type: foo processors: - type: dedupe dedupe: cache: foobarcache hash: none parts: [0] - type: bar processors: - type: dedupe dedupe: cache: foobarcache hash: none parts: [0] - type: baz output: type: broker broker: pattern: fan_out outputs: - type: quz quz: processors: - type: filter filter: type: resource resource: foobarcondition - type: qux qux: processors: - type: filter filter: type: not not: type: resource resource: foobarcondition resources: caches: foobarcache: type: memcached memcached: addresses: - localhost:11211 ttl: 60 conditions: foobarcondition: type: text text: operator: equals_cs part: 1 arg: filter me please It is also worth noting that when conditions are used as resources in this way they will only be executed once per message, regardless of how many times they are referenced (unless the content is modified). Therefore, resource conditions can act as a runtime optimisation as well as a config optimisation. Maximising IO Throughput This section outlines a few common throughput issues and ways in which they can be solved within Benthos. It is assumed here that your Benthos instance is performing only minor processing steps, and therefore has minimal reliance on your CPU resource. If this is not the case the following still applies to an extent, but you should also refer to the next section regarding CPU utilisation . Firstly, before venturing into Benthos configurations, you should take an in-depth look at your sources and sinks. Benthos is generally much simpler architecturally than the inputs and outputs it supports. Spend some time understanding how to squeeze the most out of these services and it will make it easier (or unnecessary) to tune your bridge within Benthos. Benthos Reads Too Slowly If Benthos isn't reading fast enough from your source it might not necessarily be due to a slow consumer. If the sink is slow this can cause back pressure that throttles the amount Benthos can read. Try consuming a test feed with the output replaced with stdout and pipe it to /dev/null (or use file with the path set to /dev/null ). If you notice that the input consumption suddenly speeds up then the issue is likely with the output, in which case try the next section . If the /dev/null output pipe didn't help then take a quick look at the basic configuration fields for the input source type. Sometimes there are fields for setting a number of background prefetches or similar concepts that can increase your throughput. For example, increasing the value of prefetch_count for an AMQP consumer can greatly increase the rate at which it is consumed. Next, if your source supports multiple parallel consumers then you can try doing that within Benthos by using a broker . For example, if you started with: input: type: foo foo: field1: etc You could change to: input: type: broker broker: copies: 4 inputs: - type: foo foo: field1: etc Which would create the exact same consumer as before with four copies in total. Try increasing the number of copies to see how that affects the throughput. If your multiple consumers would require different configurations then set copies to 1 and write each consumer as a separate object in the inputs array. Read the broker documentation for more tips on simplifying broker configs. If your source doesn't support multiple parallel consumers then unfortunately your options are more limited. A logical next step might be to look at your network/disk configuration to see if that's a potential cause of contention. Benthos Writes Too Slowly If you have an output sink that regularly places back pressure on your source there are a few solutions depending on the details of the issue. Firstly, you should check the config parameters of your output sink. There are often fields specifically for controlling the level of acknowledgement to expect before moving onto the next message, if these levels of guarantee are overkill you can disable them for greater throughput. For example, setting the ack_replicas field to false in the Kafka sink can have a high impact on throughput. If the config parameters for an output sink aren't enough then you can try the following: Send messages in batches Some output sinks do not support multipart messages and when receiving one will send each part as an individual message as a batch (the Kafka output will do this). You can use this to your advantage by using the batch processor to create batches of messages to send. For example, given the following input and output combination: input: type: foo output: type: kafka This bridge will send messages one at a time, wait for acknowledgement from the output and propagate that acknowledgement to the input. Instead, using this config: input: type: foo processors: - type: batch batch: count: 8 output: type: kafka The bridge will read 8 messages from the input, send those 8 messages to the output as a batch, receive the acknowledgement from the output for all messages together, then propagate the acknowledgement for all those messages to the input together. Therefore, provided the input is able to send messages and acknowledge them outside of lock-step (or doesn't support acknowledgement at all), you can improve throughput without losing delivery guarantees. NOTE: For most input types you can specify a target batch count instead of using a batch processor, which is a preferable approach to batching. Increase the number of parallel output sinks If your output sink supports multiple parallel writers then it can greatly increase your throughput to have multiple outputs configured. However, one thing to keep in mind is that due to the lock-step of reading/sending/acknowledging of a Benthos bridge, if the number of output writers exceeds the number of input consumers you will need a buffer between them in order to keep all outputs busy, the buffer doesn't need to be large. Increasing the number of parallel output sinks is similar to doing the same for input sources and is done using a broker . The output broker type supports a few different routing patterns depending on your intention. In this case we want to maximize throughput so our best choice is a greedy pattern. For example, if you started with: output: type: foo foo: field1: etc You could change to: output: type: broker broker: pattern: greedy copies: 4 outputs: - type: foo foo: field1: etc Which would create the exact same output writer as before with four copies in total. Try increasing the number of copies to see how that affects the throughput. If your multiple output writers would require different configurations (client ids, for example) then set copies to 1 and write each consumer as a separate object in the outputs array. Read the broker documentation for more tips on simplifying broker configs. Level out input spikes with a buffer There are many reasons why an input source might have spikes or inconsistent throughput rates. It is possible that your output is capable of keeping up with the long term average flow of data, but fails to keep up when an intermittent spike occurs. In situations like these it is sometimes a better use of your hardware and resources to level out the flow of data rather than try and match the peak throughput. This would depend on the frequency and duration of the spikes as well as your latency requirements, and is therefore a matter of judgement. Leveling out the flow of data can be done within Benthos using a buffer . Buffers allow an input source to store a bounded amount of data temporarily, which a consumer can work through at its own pace. Buffers always have a fixed capacity, which when full will proceed to block the input just like a busy output would. Therefore, it's still important to have an output that can keep up with the flow of data, the difference that a buffer makes is that the output only needs to keep up with the average flow of data versus the instantaneous flow of data. For example, if your input usually produces 10 msgs/s, but occasionally spikes to 100 msgs/s, and your output can handle up to 50 msgs/s, it might be possible to configure a buffer large enough to store spikes in their entirety. As long as the average flow of messages from the input remains below 50 msgs/s then your bridge should be able to continue indefinitely without ever blocking the input source. Benthos offers a range of buffer strategies and it is worth studying them all in order to find the correct combination of resilience, throughput and capacity that you need. Maximising CPU Utilisation Some processors within Benthos are relatively heavy on your CPU, and can potentially become the bottleneck of a bridge. In these circumstances it is worth configuring your bridge so that your processors are running on each available core of your machine without contention. An array of processors in any section of a Benthos config becomes a single logical pipeline of steps running on a single logical thread. When the target of the processors (an input or output) is a broker type the pipeline will be duplicated once for each discrete input/output. This is one way to create parallel processing threads but they will be tightly coupled to the input or output they are bound to. Using processing pipelines in this way results in uneven and varying loads which is unideal for distributing processing work across logical CPUs. The other way to create parallel processor threads is to configure them inside the pipeline configuration block, where we can explicitly set any number of parallel processor threads independent of how many inputs or outputs we want to use. Please refer to the documentation regarding pipelines for some examples.","title":"Concepts"},{"location":"concepts/#benthos-concepts","text":"","title":"Benthos Concepts"},{"location":"concepts/#contents","text":"Configuration Monitoring Mutating And Filtering Content Content Based Multiplexing Sharing Resources Across Processors Maximising IO Throughput Maximising CPU Utilisation","title":"Contents"},{"location":"concepts/#configuration","text":"A Benthos configuration consists of a number of root sections, the key parts being: input buffer pipeline output There are also sections for metrics , logging and http server options. Config examples for every input, output and processor type can be found here . Benthos provides lots of tooling to try and make writing configuration easier, you can read about them here .","title":"Configuration"},{"location":"concepts/#monitoring","text":"","title":"Monitoring"},{"location":"concepts/#health-checks","text":"Benthos serves two HTTP endpoints for health checks: /ping can be used as a liveness probe as it always returns a 200. /ready can be used as a readiness probe as it serves a 200 only when both the input and output are connected, otherwise a 503 is returned.","title":"Health Checks"},{"location":"concepts/#metrics","text":"Benthos exposes lots of metrics either to Statsd, Prometheus or for debugging purposes an HTTP endpoint that returns a JSON formatted object. The target destination of Benthos metrics is configurable from the metrics section , where it's also possible to rename, whitelist or blacklist certain metric paths.","title":"Metrics"},{"location":"concepts/#tracing","text":"Benthos also emits opentracing events to a tracer of your choice, which can be used to visualise the processors within a pipeline.","title":"Tracing"},{"location":"concepts/#mutating-and-filtering-content","text":"Benthos bridges different transport and storage mediums of data, but these often require the data to be represented in different ways. For example, we might be reading .tgz archives of messages from Amazon S3, but we need to decompress and unarchive the messages before sending them to Kafka. For this purpose we can use processors, which you can read about in more detail here . Processors can be attributed to both inputs and outputs, meaning you can be specific about which processors apply to data from specific sources or to specific sinks.","title":"Mutating And Filtering Content"},{"location":"concepts/#content-based-multiplexing","text":"It is possible to perform content based multiplexing of messages to specific outputs using a switch output with two or more conditional outputs. Conditions are content aware logical operators that can be combined using boolean logic. For example, say we have an output foo that we only want to receive messages that contain the word foo , and an output bar that we wish to send everything that foo doesn't receive, we can achieve that with this config: output: type: switch switch: outputs: - output: type: foo foo: foo_field_1: value1 condition: type: text text: operator: contains arg: foo - output: type: bar bar: bar_field_1: value2 Another method of content based multiplexing is with an output broker with the fan_out pattern and a filter processor on each output, which is a processor that drops messages if the condition does not pass. For example, the equivalent config for the previous example would be: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: foo_field_1: value1 processors: - type: filter_parts filter_parts: type: text text: operator: contains arg: foo - type: bar bar: bar_field_1: value2 processors: - type: filter_parts filter_parts: type: not not: type: text text: operator: contains arg: foo For more information regarding conditions, including the full list of conditions available, please read the docs here .","title":"Content Based Multiplexing"},{"location":"concepts/#sharing-resources-across-processors","text":"Sometimes it is advantageous to share configurations for resources such as caches or complex conditions between processors when they would otherwise be duplicated. For this purpose there is a resource section in a Benthos config where caches , conditions and rate limits can be configured to a label that is referred to by any components that wish to use them. For example, let's imagine we have three inputs, two of which we wish to deduplicate using a shared cache. We also have two outputs, one of which only receives messages that satisfy a condition and the other receives the logical NOT of that same condition. In this example we can save ourselves the trouble of configuring the same cache and condition twice by referring to them as resources like this: input: type: broker broker: inputs: - type: foo processors: - type: dedupe dedupe: cache: foobarcache hash: none parts: [0] - type: bar processors: - type: dedupe dedupe: cache: foobarcache hash: none parts: [0] - type: baz output: type: broker broker: pattern: fan_out outputs: - type: quz quz: processors: - type: filter filter: type: resource resource: foobarcondition - type: qux qux: processors: - type: filter filter: type: not not: type: resource resource: foobarcondition resources: caches: foobarcache: type: memcached memcached: addresses: - localhost:11211 ttl: 60 conditions: foobarcondition: type: text text: operator: equals_cs part: 1 arg: filter me please It is also worth noting that when conditions are used as resources in this way they will only be executed once per message, regardless of how many times they are referenced (unless the content is modified). Therefore, resource conditions can act as a runtime optimisation as well as a config optimisation.","title":"Sharing Resources Across Processors"},{"location":"concepts/#maximising-io-throughput","text":"This section outlines a few common throughput issues and ways in which they can be solved within Benthos. It is assumed here that your Benthos instance is performing only minor processing steps, and therefore has minimal reliance on your CPU resource. If this is not the case the following still applies to an extent, but you should also refer to the next section regarding CPU utilisation . Firstly, before venturing into Benthos configurations, you should take an in-depth look at your sources and sinks. Benthos is generally much simpler architecturally than the inputs and outputs it supports. Spend some time understanding how to squeeze the most out of these services and it will make it easier (or unnecessary) to tune your bridge within Benthos.","title":"Maximising IO Throughput"},{"location":"concepts/#benthos-reads-too-slowly","text":"If Benthos isn't reading fast enough from your source it might not necessarily be due to a slow consumer. If the sink is slow this can cause back pressure that throttles the amount Benthos can read. Try consuming a test feed with the output replaced with stdout and pipe it to /dev/null (or use file with the path set to /dev/null ). If you notice that the input consumption suddenly speeds up then the issue is likely with the output, in which case try the next section . If the /dev/null output pipe didn't help then take a quick look at the basic configuration fields for the input source type. Sometimes there are fields for setting a number of background prefetches or similar concepts that can increase your throughput. For example, increasing the value of prefetch_count for an AMQP consumer can greatly increase the rate at which it is consumed. Next, if your source supports multiple parallel consumers then you can try doing that within Benthos by using a broker . For example, if you started with: input: type: foo foo: field1: etc You could change to: input: type: broker broker: copies: 4 inputs: - type: foo foo: field1: etc Which would create the exact same consumer as before with four copies in total. Try increasing the number of copies to see how that affects the throughput. If your multiple consumers would require different configurations then set copies to 1 and write each consumer as a separate object in the inputs array. Read the broker documentation for more tips on simplifying broker configs. If your source doesn't support multiple parallel consumers then unfortunately your options are more limited. A logical next step might be to look at your network/disk configuration to see if that's a potential cause of contention.","title":"Benthos Reads Too Slowly"},{"location":"concepts/#benthos-writes-too-slowly","text":"If you have an output sink that regularly places back pressure on your source there are a few solutions depending on the details of the issue. Firstly, you should check the config parameters of your output sink. There are often fields specifically for controlling the level of acknowledgement to expect before moving onto the next message, if these levels of guarantee are overkill you can disable them for greater throughput. For example, setting the ack_replicas field to false in the Kafka sink can have a high impact on throughput. If the config parameters for an output sink aren't enough then you can try the following:","title":"Benthos Writes Too Slowly"},{"location":"concepts/#send-messages-in-batches","text":"Some output sinks do not support multipart messages and when receiving one will send each part as an individual message as a batch (the Kafka output will do this). You can use this to your advantage by using the batch processor to create batches of messages to send. For example, given the following input and output combination: input: type: foo output: type: kafka This bridge will send messages one at a time, wait for acknowledgement from the output and propagate that acknowledgement to the input. Instead, using this config: input: type: foo processors: - type: batch batch: count: 8 output: type: kafka The bridge will read 8 messages from the input, send those 8 messages to the output as a batch, receive the acknowledgement from the output for all messages together, then propagate the acknowledgement for all those messages to the input together. Therefore, provided the input is able to send messages and acknowledge them outside of lock-step (or doesn't support acknowledgement at all), you can improve throughput without losing delivery guarantees. NOTE: For most input types you can specify a target batch count instead of using a batch processor, which is a preferable approach to batching.","title":"Send messages in batches"},{"location":"concepts/#increase-the-number-of-parallel-output-sinks","text":"If your output sink supports multiple parallel writers then it can greatly increase your throughput to have multiple outputs configured. However, one thing to keep in mind is that due to the lock-step of reading/sending/acknowledging of a Benthos bridge, if the number of output writers exceeds the number of input consumers you will need a buffer between them in order to keep all outputs busy, the buffer doesn't need to be large. Increasing the number of parallel output sinks is similar to doing the same for input sources and is done using a broker . The output broker type supports a few different routing patterns depending on your intention. In this case we want to maximize throughput so our best choice is a greedy pattern. For example, if you started with: output: type: foo foo: field1: etc You could change to: output: type: broker broker: pattern: greedy copies: 4 outputs: - type: foo foo: field1: etc Which would create the exact same output writer as before with four copies in total. Try increasing the number of copies to see how that affects the throughput. If your multiple output writers would require different configurations (client ids, for example) then set copies to 1 and write each consumer as a separate object in the outputs array. Read the broker documentation for more tips on simplifying broker configs.","title":"Increase the number of parallel output sinks"},{"location":"concepts/#level-out-input-spikes-with-a-buffer","text":"There are many reasons why an input source might have spikes or inconsistent throughput rates. It is possible that your output is capable of keeping up with the long term average flow of data, but fails to keep up when an intermittent spike occurs. In situations like these it is sometimes a better use of your hardware and resources to level out the flow of data rather than try and match the peak throughput. This would depend on the frequency and duration of the spikes as well as your latency requirements, and is therefore a matter of judgement. Leveling out the flow of data can be done within Benthos using a buffer . Buffers allow an input source to store a bounded amount of data temporarily, which a consumer can work through at its own pace. Buffers always have a fixed capacity, which when full will proceed to block the input just like a busy output would. Therefore, it's still important to have an output that can keep up with the flow of data, the difference that a buffer makes is that the output only needs to keep up with the average flow of data versus the instantaneous flow of data. For example, if your input usually produces 10 msgs/s, but occasionally spikes to 100 msgs/s, and your output can handle up to 50 msgs/s, it might be possible to configure a buffer large enough to store spikes in their entirety. As long as the average flow of messages from the input remains below 50 msgs/s then your bridge should be able to continue indefinitely without ever blocking the input source. Benthos offers a range of buffer strategies and it is worth studying them all in order to find the correct combination of resilience, throughput and capacity that you need.","title":"Level out input spikes with a buffer"},{"location":"concepts/#maximising-cpu-utilisation","text":"Some processors within Benthos are relatively heavy on your CPU, and can potentially become the bottleneck of a bridge. In these circumstances it is worth configuring your bridge so that your processors are running on each available core of your machine without contention. An array of processors in any section of a Benthos config becomes a single logical pipeline of steps running on a single logical thread. When the target of the processors (an input or output) is a broker type the pipeline will be duplicated once for each discrete input/output. This is one way to create parallel processing threads but they will be tightly coupled to the input or output they are bound to. Using processing pipelines in this way results in uneven and varying loads which is unideal for distributing processing work across logical CPUs. The other way to create parallel processor threads is to configure them inside the pipeline configuration block, where we can explicitly set any number of parallel processor threads independent of how many inputs or outputs we want to use. Please refer to the documentation regarding pipelines for some examples.","title":"Maximising CPU Utilisation"},{"location":"config_interpolation/","text":"String Interpolation in Configs Benthos is able to perform string interpolation on your config files. There are two types of expression for this; functions and environment variables. Environment variables are resolved and interpolated into the config only once at start up. Functions are resolved each time they are used. However, only certain fields in a config will actually support and interpolate these expressions ( insert_part.contents , for example). If you aren't sure that a field in a config section supports functions you should read its respective documentation. Environment Variables You can use environment variables to replace Benthos config values using ${variable-name} or ${variable-name:default-value} syntax. A good example of this is the environment variable config , which creates environment variables for each default field value in a standard single-in-single-out bridge config. Example Let's say you plan to bridge a Kafka deployment to a RabbitMQ exchange but we want to resolve the addresses of these respective services after deployment using environment variables. In this case we can replace the broker list in a Kafka config section with an environment variable, and do the same with the RabbitMQ URL: input: type: kafka_balanced kafka_balanced: addresses: - ${KAFKA_BROKERS} consumer_group: benthos_bridge_consumer topics: - haha_business output: type: amqp amqp: url: amqp://${RABBITMQ}/ exchange: kafka_bridge exchange_type: direct key: benthos-key We can now write multiple brokers into KAFKA_BROKERS by separating them with commas, Benthos will automatically split them. We can now run with our environment variables: KAFKA_BROKERS= foo:9092,bar:9092 \\ RABBITMQ= baz:5672 \\ benthos -c ./our_config.yaml Functions The syntax for functions is ${!function-name} , or ${!function-name:arg} if the function takes an argument, where function-name should be replaced with one of the following function names: content Resolves to the content of a message part. The message referred to will depend on the context of where the function is called. When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part index with an integer argument, e.g. ${!content:2} would print the contents of the third message part. json_field Resolves to the value of a JSON field within the message payload located by a dot-path specified as an argument. The message referred to will depend on the context of where the function is called. With a message containing {\"foo\":{\"bar\":\"hello world\"}} the function ${!json_field:foo.bar} would resolve to hello world . When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part by following the path with a comma and part number, e.g. ${!json_field:foo.bar,2} would target the field foo.bar within the third message part in the batch. metadata Resolves to the value of a metadata key within the message payload. The message referred to will depend on the context of where the function is called. If a message contains the metadata key/value pair foo: bar the function ${!metadata:foo} would resolve to bar . When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part by following the key with a comma and part number, e.g. ${!metadata:foo,2} would target the key foo within the third message part in the batch. Message metadata can be modified using the metadata processor . metadata_json_object Resolves to all metadata key/value pairs of a payload as a JSON object. The message referred to will depend on the context of where the function is called. When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part with an integer argument e.g. ${!metadata_json_object:2} would target the metadata of the third message part in the batch. Message metadata can be modified using the metadata processor . uuid_v4 Generates a new RFC-4122 UUID each time it is invoked and prints a string representation. timestamp_unix_nano Resolves to the current unix timestamp in nanoseconds. E.g. foo ${!timestamp_unix_nano} bar prints foo 1517412152475689615 bar . timestamp_unix Resolves to the current unix timestamp in seconds. E.g. foo ${!timestamp_unix} bar prints foo 1517412152 bar . You can add fractional precision up to the nanosecond by specifying the precision as an argument, e.g. ${!timestamp_unix:3} for millisecond precision. timestamp Prints the current time in a custom format specified by the argument. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. A fractional second is represented by adding a period and zeros to the end of the seconds section of layout string, as in 15:04:05.000 to format a time stamp with millisecond precision. count The count function is a counter starting at 1 which increments after each time it is called. Count takes an argument which is an identifier for the counter, allowing you to specify multiple unique counters in your configuration. hostname Resolves to the hostname of the machine running Benthos. E.g. foo ${!hostname} bar might resolve to foo glados bar .","title":"Config Interpolation"},{"location":"config_interpolation/#string-interpolation-in-configs","text":"Benthos is able to perform string interpolation on your config files. There are two types of expression for this; functions and environment variables. Environment variables are resolved and interpolated into the config only once at start up. Functions are resolved each time they are used. However, only certain fields in a config will actually support and interpolate these expressions ( insert_part.contents , for example). If you aren't sure that a field in a config section supports functions you should read its respective documentation.","title":"String Interpolation in Configs"},{"location":"config_interpolation/#environment-variables","text":"You can use environment variables to replace Benthos config values using ${variable-name} or ${variable-name:default-value} syntax. A good example of this is the environment variable config , which creates environment variables for each default field value in a standard single-in-single-out bridge config.","title":"Environment Variables"},{"location":"config_interpolation/#example","text":"Let's say you plan to bridge a Kafka deployment to a RabbitMQ exchange but we want to resolve the addresses of these respective services after deployment using environment variables. In this case we can replace the broker list in a Kafka config section with an environment variable, and do the same with the RabbitMQ URL: input: type: kafka_balanced kafka_balanced: addresses: - ${KAFKA_BROKERS} consumer_group: benthos_bridge_consumer topics: - haha_business output: type: amqp amqp: url: amqp://${RABBITMQ}/ exchange: kafka_bridge exchange_type: direct key: benthos-key We can now write multiple brokers into KAFKA_BROKERS by separating them with commas, Benthos will automatically split them. We can now run with our environment variables: KAFKA_BROKERS= foo:9092,bar:9092 \\ RABBITMQ= baz:5672 \\ benthos -c ./our_config.yaml","title":"Example"},{"location":"config_interpolation/#functions","text":"The syntax for functions is ${!function-name} , or ${!function-name:arg} if the function takes an argument, where function-name should be replaced with one of the following function names:","title":"Functions"},{"location":"config_interpolation/#content","text":"Resolves to the content of a message part. The message referred to will depend on the context of where the function is called. When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part index with an integer argument, e.g. ${!content:2} would print the contents of the third message part.","title":"content"},{"location":"config_interpolation/#json_field","text":"Resolves to the value of a JSON field within the message payload located by a dot-path specified as an argument. The message referred to will depend on the context of where the function is called. With a message containing {\"foo\":{\"bar\":\"hello world\"}} the function ${!json_field:foo.bar} would resolve to hello world . When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part by following the path with a comma and part number, e.g. ${!json_field:foo.bar,2} would target the field foo.bar within the third message part in the batch.","title":"json_field"},{"location":"config_interpolation/#metadata","text":"Resolves to the value of a metadata key within the message payload. The message referred to will depend on the context of where the function is called. If a message contains the metadata key/value pair foo: bar the function ${!metadata:foo} would resolve to bar . When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part by following the key with a comma and part number, e.g. ${!metadata:foo,2} would target the key foo within the third message part in the batch. Message metadata can be modified using the metadata processor .","title":"metadata"},{"location":"config_interpolation/#metadata_json_object","text":"Resolves to all metadata key/value pairs of a payload as a JSON object. The message referred to will depend on the context of where the function is called. When applied to a batch of message parts this function targets the first message part by default. It is possible to specify a target part with an integer argument e.g. ${!metadata_json_object:2} would target the metadata of the third message part in the batch. Message metadata can be modified using the metadata processor .","title":"metadata_json_object"},{"location":"config_interpolation/#uuid_v4","text":"Generates a new RFC-4122 UUID each time it is invoked and prints a string representation.","title":"uuid_v4"},{"location":"config_interpolation/#timestamp_unix_nano","text":"Resolves to the current unix timestamp in nanoseconds. E.g. foo ${!timestamp_unix_nano} bar prints foo 1517412152475689615 bar .","title":"timestamp_unix_nano"},{"location":"config_interpolation/#timestamp_unix","text":"Resolves to the current unix timestamp in seconds. E.g. foo ${!timestamp_unix} bar prints foo 1517412152 bar . You can add fractional precision up to the nanosecond by specifying the precision as an argument, e.g. ${!timestamp_unix:3} for millisecond precision.","title":"timestamp_unix"},{"location":"config_interpolation/#timestamp","text":"Prints the current time in a custom format specified by the argument. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. A fractional second is represented by adding a period and zeros to the end of the seconds section of layout string, as in 15:04:05.000 to format a time stamp with millisecond precision.","title":"timestamp"},{"location":"config_interpolation/#count","text":"The count function is a counter starting at 1 which increments after each time it is called. Count takes an argument which is an identifier for the counter, allowing you to specify multiple unique counters in your configuration.","title":"count"},{"location":"config_interpolation/#hostname","text":"Resolves to the hostname of the machine running Benthos. E.g. foo ${!hostname} bar might resolve to foo glados bar .","title":"hostname"},{"location":"configuration/","text":"Configuration A Benthos stream is configured either in a YAML or JSON file using a hierarchical format. For a basic stream pipeline that means the section for, say, an input is very simple: input: type: kafka kafka: topic: foo partition: 0 addresses: - localhost:9092 However, as configurations become more complex this format can sometimes be difficult to read and manage: input: type: kafka kafka: ... processors: - type: conditional conditional: condition: type: jmespath jmespath: query: contains(foo.bar, compress me ) processors: - type: compress compress: algorithm: gzip The above example reads messages from Kafka and, if the JSON path foo.bar contains the phrase \"compress me\" the entire message will be compressed with gzip , otherwise it passes unchanged. Allowing arbitrary hierarchies of processors and conditions like this is powerful, but increases the likelihood of issues being introduced by typos. This document outlines tooling provided by Benthos to help with writing and managing these more complex configuration files. Contents Enabling Discovery Help With Debugging Enabling Discovery The discoverability of configuration fields is a common headache with any configuration driven application. The classic solution is to provide curated documentation that is often hosted on a dedicated site. Benthos does this by generating a markdown document per configuration section . However, a user often only needs to get their hands on a short, runnable example config file for their use case. They just need to see the format and field names as the fields themselves are usually self explanatory. Forcing such a user to navigate a website, scrolling through paragraphs of text, seems inefficient when all they actually needed to see was something like: input: type: amqp amqp: url: amqp://guest:guest@localhost:5672/ consumer_tag: benthos-consumer exchange: benthos-exchange exchange_type: direct key: benthos-key prefetch_count: 10 prefetch_size: 0 queue: benthos-queue output: type: stdout In order to make this process easier Benthos is able to generate usable configuration examples for any types, and you can do this from the binary using the --example flag in combination with --print-yaml or --print-json . If, for example, we wanted to generate a config with a websocket input, a Kafka output and a JMESPath processor in the middle, we could do it with the following command: benthos --print-yaml --example websocket,kafka,jmespath There are also examples within the config directory , where there is a config file for each input and output type, and inside the processors subdirectory there is a file showing each processor type, and so on. All of these generated configuration examples also include other useful config sections such as metrics , logging , etc with sensible defaults. Printing Every Field The format of a Benthos config file naturally exposes all of the options for a section when it's printed with all default values. For example, in a fictional section foo , which has type options bar , baz and qux , if you were to print the entire default foo section of a config it would look something like this: foo: type: bar bar: field1: default_value field2: 2 baz: field3: another_default_value qux: field4: false Which tells you that section foo supports the three object types bar , baz and qux , and defaults to type bar . It also shows you the fields that each section has, and their default values. The Benthos binary is able to print a JSON or YAML config file containing every section in this format with the commands benthos --print-yaml --all and benthos --print-json --all . This can be extremely useful for quick and dirty config discovery when the full repo isn't at hand. As a user you could create a new config file with: benthos --print-yaml --all conf.yaml And simply delete all lines for sections you aren't interested in, then you are left with the full set of fields you want. Alternatively, using tools such as jq you can extract specific type fields: # Get a list of all input types: benthos --print-json --all | jq '.input | keys' # Get all Kafka input fields: benthos --print-json --all | jq '.input.kafka' # Get all AMQP output fields: benthos --print-json --all | jq '.output.amqp' # Get a list of all processor types: benthos --print-json --all | jq '.pipeline.processors[0] | keys' # Get all JSON processor fields: benthos --print-json --all | jq '.pipeline.processors[0].json' Help With Debugging Once you have a config written you now move onto the next headache of proving that it works, and understanding why it doesn't. Benthos, like most good config driven services, performs validation on configs and tries to provide sensible error messages. However, with validation it can be hard to capture all problems, and the user usually understands their intentions better than the service. In order to help expose and diagnose config errors Benthos provides two mechanisms, linting and echoing. Linting Benthos has a lint command ( --lint ) that, after parsing a config file, will print any errors it detects. The main goal of the linter is to expose instances where fields within a provided config are valid JSON or YAML but don't actually affect the behaviour of Benthos. These are useful for pointing out typos in object keys or the use of deprecated fields. For example, imagine we have a config foo.yaml , where we intend to read from AMQP, but there is a typo in our config struct: input: type: amqp amqq: url: amqp://guest:guest@rabbitmqserver:5672/ This config is parse successfully, and Benthos will simply ignore the amqq key and run using default values for the amqp input. This is therefore an easy error to miss, but if we use the linter it will immediately report the problem: $ benthos -c ./foo.yaml --lint input: Key 'amqq' found but is ignored Which points us to exactly where the problem is. Echoing Echoing is where Benthos can print back your configuration after it has been parsed. It is done with the --print-yaml and --print-json commands, which print the Benthos configuration in YAML and JSON format respectively. Since this is done after parsing and applying your config it is able to show you exactly how your config was interpretted: benthos -c ./your-config.yaml --print-yaml You can check the output of the above command to see if certain sections are missing or fields are incorrect, which allows you to pinpoint typos in the config. If your configuration is complex, and the behaviour that you notice implies a certain section is at fault, then you can drill down into that section by using tools such as jq : # Check the second processor config benthos -c ./your-config.yaml --print-json | jq '.pipeline.processors[1]' # Check the condition of a filter processor benthos -c ./your-config.yaml --print-json | jq '.pipeline.processors[0].filter'","title":"Config Cheats"},{"location":"configuration/#configuration","text":"A Benthos stream is configured either in a YAML or JSON file using a hierarchical format. For a basic stream pipeline that means the section for, say, an input is very simple: input: type: kafka kafka: topic: foo partition: 0 addresses: - localhost:9092 However, as configurations become more complex this format can sometimes be difficult to read and manage: input: type: kafka kafka: ... processors: - type: conditional conditional: condition: type: jmespath jmespath: query: contains(foo.bar, compress me ) processors: - type: compress compress: algorithm: gzip The above example reads messages from Kafka and, if the JSON path foo.bar contains the phrase \"compress me\" the entire message will be compressed with gzip , otherwise it passes unchanged. Allowing arbitrary hierarchies of processors and conditions like this is powerful, but increases the likelihood of issues being introduced by typos. This document outlines tooling provided by Benthos to help with writing and managing these more complex configuration files.","title":"Configuration"},{"location":"configuration/#contents","text":"Enabling Discovery Help With Debugging","title":"Contents"},{"location":"configuration/#enabling-discovery","text":"The discoverability of configuration fields is a common headache with any configuration driven application. The classic solution is to provide curated documentation that is often hosted on a dedicated site. Benthos does this by generating a markdown document per configuration section . However, a user often only needs to get their hands on a short, runnable example config file for their use case. They just need to see the format and field names as the fields themselves are usually self explanatory. Forcing such a user to navigate a website, scrolling through paragraphs of text, seems inefficient when all they actually needed to see was something like: input: type: amqp amqp: url: amqp://guest:guest@localhost:5672/ consumer_tag: benthos-consumer exchange: benthos-exchange exchange_type: direct key: benthos-key prefetch_count: 10 prefetch_size: 0 queue: benthos-queue output: type: stdout In order to make this process easier Benthos is able to generate usable configuration examples for any types, and you can do this from the binary using the --example flag in combination with --print-yaml or --print-json . If, for example, we wanted to generate a config with a websocket input, a Kafka output and a JMESPath processor in the middle, we could do it with the following command: benthos --print-yaml --example websocket,kafka,jmespath There are also examples within the config directory , where there is a config file for each input and output type, and inside the processors subdirectory there is a file showing each processor type, and so on. All of these generated configuration examples also include other useful config sections such as metrics , logging , etc with sensible defaults.","title":"Enabling Discovery"},{"location":"configuration/#printing-every-field","text":"The format of a Benthos config file naturally exposes all of the options for a section when it's printed with all default values. For example, in a fictional section foo , which has type options bar , baz and qux , if you were to print the entire default foo section of a config it would look something like this: foo: type: bar bar: field1: default_value field2: 2 baz: field3: another_default_value qux: field4: false Which tells you that section foo supports the three object types bar , baz and qux , and defaults to type bar . It also shows you the fields that each section has, and their default values. The Benthos binary is able to print a JSON or YAML config file containing every section in this format with the commands benthos --print-yaml --all and benthos --print-json --all . This can be extremely useful for quick and dirty config discovery when the full repo isn't at hand. As a user you could create a new config file with: benthos --print-yaml --all conf.yaml And simply delete all lines for sections you aren't interested in, then you are left with the full set of fields you want. Alternatively, using tools such as jq you can extract specific type fields: # Get a list of all input types: benthos --print-json --all | jq '.input | keys' # Get all Kafka input fields: benthos --print-json --all | jq '.input.kafka' # Get all AMQP output fields: benthos --print-json --all | jq '.output.amqp' # Get a list of all processor types: benthos --print-json --all | jq '.pipeline.processors[0] | keys' # Get all JSON processor fields: benthos --print-json --all | jq '.pipeline.processors[0].json'","title":"Printing Every Field"},{"location":"configuration/#help-with-debugging","text":"Once you have a config written you now move onto the next headache of proving that it works, and understanding why it doesn't. Benthos, like most good config driven services, performs validation on configs and tries to provide sensible error messages. However, with validation it can be hard to capture all problems, and the user usually understands their intentions better than the service. In order to help expose and diagnose config errors Benthos provides two mechanisms, linting and echoing.","title":"Help With Debugging"},{"location":"configuration/#linting","text":"Benthos has a lint command ( --lint ) that, after parsing a config file, will print any errors it detects. The main goal of the linter is to expose instances where fields within a provided config are valid JSON or YAML but don't actually affect the behaviour of Benthos. These are useful for pointing out typos in object keys or the use of deprecated fields. For example, imagine we have a config foo.yaml , where we intend to read from AMQP, but there is a typo in our config struct: input: type: amqp amqq: url: amqp://guest:guest@rabbitmqserver:5672/ This config is parse successfully, and Benthos will simply ignore the amqq key and run using default values for the amqp input. This is therefore an easy error to miss, but if we use the linter it will immediately report the problem: $ benthos -c ./foo.yaml --lint input: Key 'amqq' found but is ignored Which points us to exactly where the problem is.","title":"Linting"},{"location":"configuration/#echoing","text":"Echoing is where Benthos can print back your configuration after it has been parsed. It is done with the --print-yaml and --print-json commands, which print the Benthos configuration in YAML and JSON format respectively. Since this is done after parsing and applying your config it is able to show you exactly how your config was interpretted: benthos -c ./your-config.yaml --print-yaml You can check the output of the above command to see if certain sections are missing or fields are incorrect, which allows you to pinpoint typos in the config. If your configuration is complex, and the behaviour that you notice implies a certain section is at fault, then you can drill down into that section by using tools such as jq : # Check the second processor config benthos -c ./your-config.yaml --print-json | jq '.pipeline.processors[1]' # Check the condition of a filter processor benthos -c ./your-config.yaml --print-json | jq '.pipeline.processors[0].filter'","title":"Echoing"},{"location":"dynamic_inputs_and_outputs/","text":"Dynamic Inputs and Outputs It is possible to have sets of inputs and outputs in Benthos that can be added, updated and removed during runtime with the dynamic fan in and dynamic fan out types. Dynamic inputs and outputs are each identified by unique string labels, which are specified when adding them either in configuration or via the HTTP API. The labels are useful when querying which types are active. API The API for dynamic types (both inputs and outputs) is a collection of HTTP REST endpoints: /inputs Returns a JSON object that maps input labels to an object containing details about the input, including uptime and configuration. If the input has terminated naturally the uptime will be set to stopped . { string, input_label : { uptime : string , config : object }, ... } /input/{input_label} GET returns the configuration of the input idenfified by input_label . POST sets the input input_label to the body of the request parsed as a JSON configuration. If the input label already exists the previous input is first stopped and removed. DELETE stops and removes the input identified by input_label . /outputs Returns a JSON object that maps output labels to an object containing details about the output, including uptime and configuration. If the output has terminated naturally the uptime will be set to stopped . { string, output_label : { uptime : string , config : object }, ... } /output/{output_label} GET returns the configuration of the output idenfified by output_label . POST sets the output output_label to the body of the request parsed as a JSON configuration. If the output label already exists the previous output is first stopped and removed. DELETE stops and removes the output identified by output_label . A custom prefix can be set for these endpoints in configuration. Applications Dynamic types are useful when a platforms data streams might need to change regularly and automatically. It is also useful for triggering batches of platform data, e.g. a cron job can be created to send hourly curl requests that adds a dynamic input to read a file of sample data: curl http://localhost:4195/input/read_sample -d @- EOF { type : file , file : { path : /tmp/line_delim_sample_data.txt } } EOF Some inputs have a finite lifetime, e.g. s3 without an SQS queue configured will close once the whole bucket has been read. When a dynamic types lifetime ends the uptime field of an input listing will be set to stopped . You can use this to write tools that trigger new inputs (to move onto the next bucket, for example).","title":"Dynamic inputs and outputs"},{"location":"dynamic_inputs_and_outputs/#dynamic-inputs-and-outputs","text":"It is possible to have sets of inputs and outputs in Benthos that can be added, updated and removed during runtime with the dynamic fan in and dynamic fan out types. Dynamic inputs and outputs are each identified by unique string labels, which are specified when adding them either in configuration or via the HTTP API. The labels are useful when querying which types are active.","title":"Dynamic Inputs and Outputs"},{"location":"dynamic_inputs_and_outputs/#api","text":"The API for dynamic types (both inputs and outputs) is a collection of HTTP REST endpoints:","title":"API"},{"location":"dynamic_inputs_and_outputs/#inputs","text":"Returns a JSON object that maps input labels to an object containing details about the input, including uptime and configuration. If the input has terminated naturally the uptime will be set to stopped . { string, input_label : { uptime : string , config : object }, ... }","title":"/inputs"},{"location":"dynamic_inputs_and_outputs/#inputinput_label","text":"GET returns the configuration of the input idenfified by input_label . POST sets the input input_label to the body of the request parsed as a JSON configuration. If the input label already exists the previous input is first stopped and removed. DELETE stops and removes the input identified by input_label .","title":"/input/{input_label}"},{"location":"dynamic_inputs_and_outputs/#outputs","text":"Returns a JSON object that maps output labels to an object containing details about the output, including uptime and configuration. If the output has terminated naturally the uptime will be set to stopped . { string, output_label : { uptime : string , config : object }, ... }","title":"/outputs"},{"location":"dynamic_inputs_and_outputs/#outputoutput_label","text":"GET returns the configuration of the output idenfified by output_label . POST sets the output output_label to the body of the request parsed as a JSON configuration. If the output label already exists the previous output is first stopped and removed. DELETE stops and removes the output identified by output_label . A custom prefix can be set for these endpoints in configuration.","title":"/output/{output_label}"},{"location":"dynamic_inputs_and_outputs/#applications","text":"Dynamic types are useful when a platforms data streams might need to change regularly and automatically. It is also useful for triggering batches of platform data, e.g. a cron job can be created to send hourly curl requests that adds a dynamic input to read a file of sample data: curl http://localhost:4195/input/read_sample -d @- EOF { type : file , file : { path : /tmp/line_delim_sample_data.txt } } EOF Some inputs have a finite lifetime, e.g. s3 without an SQS queue configured will close once the whole bucket has been read. When a dynamic types lifetime ends the uptime field of an input listing will be set to stopped . You can use this to write tools that trigger new inputs (to move onto the next bucket, for example).","title":"Applications"},{"location":"error_handling/","text":"Error Handling Processor Errors Sometimes things can go wrong. Benthos supports a range of processors such as http and lambda that have the potential to fail if their retry attempts are exhausted. When this happens the data is not dropped but instead continues through the pipeline mostly unchanged. The content remains the same but a metadata flag is added to the message that can be referred to later in the pipeline using the processor_failed condition. This behaviour allows you to define in your config whether you would like the failed messages to be dropped, recovered with more processing, or routed to a dead-letter queue, or any combination thereof. Abandon on Failure It's possible to define a list of processors which should be skipped for messages that failed a previous stage using the try processor: - type: try try: - type: foo - type: bar # Skipped if foo failed - type: baz # Skipped if foo or bar failed Recover Failed Messages Failed messages can be fed into their own processor steps with a catch processor: - type: catch catch: - type: foo # Recover here Once messages finish the catch block they will have their failure flags removed and are treated like regular messages. If this behaviour is not desired then it is possible to simulate a catch block with a conditional processor: - type: process_batch process_batch: - type: conditional conditional: condition: type: processor_failed processors: - type: foo # Recover here Attempt Until Success It's possible to reattempt a processor for a particular message until it is successful with a while processor: - type: process_batch process_batch: - type: while while: at_least_once: true max_loops: 0 # Set this greater than zero to cap the number of attempts condition: type: processor_failed processors: - type: foo # Attempt this processor until success This loop will block the pipeline and prevent the blocking message from being acknowledged. It is therefore usually a good idea in practice to build your condition with an exit strategy after N failed attempts so that the pipeline can unblock itself without intervention. Drop Failed Messages In order to filter out any failed messages from your pipeline you can simply use a filter_parts processor: - type: filter_parts filter_parts: type: processor_failed This will remove any failed messages from a batch. Route to a Dead-Letter Queue It is possible to send failed messages to different destinations using either a group_by processor with a switch output, or a broker output with filter_parts processors. pipeline: processors: - type: group_by group_by: - condition: type: processor_failed output: type: switch switch: outputs: - output: type: foo # Dead letter queue condition: type: processor_failed - output: type: bar # Everything else Note that the group_by processor is only necessary when messages are batched. Alternatively, using a broker output looks like this: output: type: broker broker: pattern: fan_out outputs: - type: foo # Dead letter queue processors: - type: filter_parts filter_parts: type: processor_failed - type: bar # Everything else processors: - type: filter_parts filter_parts: type: not not: type: processor_failed","title":"Error Handling"},{"location":"error_handling/#error-handling","text":"","title":"Error Handling"},{"location":"error_handling/#processor-errors","text":"Sometimes things can go wrong. Benthos supports a range of processors such as http and lambda that have the potential to fail if their retry attempts are exhausted. When this happens the data is not dropped but instead continues through the pipeline mostly unchanged. The content remains the same but a metadata flag is added to the message that can be referred to later in the pipeline using the processor_failed condition. This behaviour allows you to define in your config whether you would like the failed messages to be dropped, recovered with more processing, or routed to a dead-letter queue, or any combination thereof.","title":"Processor Errors"},{"location":"error_handling/#abandon-on-failure","text":"It's possible to define a list of processors which should be skipped for messages that failed a previous stage using the try processor: - type: try try: - type: foo - type: bar # Skipped if foo failed - type: baz # Skipped if foo or bar failed","title":"Abandon on Failure"},{"location":"error_handling/#recover-failed-messages","text":"Failed messages can be fed into their own processor steps with a catch processor: - type: catch catch: - type: foo # Recover here Once messages finish the catch block they will have their failure flags removed and are treated like regular messages. If this behaviour is not desired then it is possible to simulate a catch block with a conditional processor: - type: process_batch process_batch: - type: conditional conditional: condition: type: processor_failed processors: - type: foo # Recover here","title":"Recover Failed Messages"},{"location":"error_handling/#attempt-until-success","text":"It's possible to reattempt a processor for a particular message until it is successful with a while processor: - type: process_batch process_batch: - type: while while: at_least_once: true max_loops: 0 # Set this greater than zero to cap the number of attempts condition: type: processor_failed processors: - type: foo # Attempt this processor until success This loop will block the pipeline and prevent the blocking message from being acknowledged. It is therefore usually a good idea in practice to build your condition with an exit strategy after N failed attempts so that the pipeline can unblock itself without intervention.","title":"Attempt Until Success"},{"location":"error_handling/#drop-failed-messages","text":"In order to filter out any failed messages from your pipeline you can simply use a filter_parts processor: - type: filter_parts filter_parts: type: processor_failed This will remove any failed messages from a batch.","title":"Drop Failed Messages"},{"location":"error_handling/#route-to-a-dead-letter-queue","text":"It is possible to send failed messages to different destinations using either a group_by processor with a switch output, or a broker output with filter_parts processors. pipeline: processors: - type: group_by group_by: - condition: type: processor_failed output: type: switch switch: outputs: - output: type: foo # Dead letter queue condition: type: processor_failed - output: type: bar # Everything else Note that the group_by processor is only necessary when messages are batched. Alternatively, using a broker output looks like this: output: type: broker broker: pattern: fan_out outputs: - type: foo # Dead letter queue processors: - type: filter_parts filter_parts: type: processor_failed - type: bar # Everything else processors: - type: filter_parts filter_parts: type: not not: type: processor_failed","title":"Route to a Dead-Letter Queue"},{"location":"getting_started/","text":"Getting Started Install Build With Go If you have Go version 1.11 or above you can build and install Benthos with: go install github.com/Jeffail/benthos/cmd/benthos benthos -c ./yourconfig.yaml Pull With Docker If you have docker installed you can pull the latest official Benthos image with: docker pull jeffail/benthos docker run --rm -v /path/to/your/config.yaml:/benthos.yaml jeffail/benthos Download an Archive Otherwise you can pull an archive containing Benthos from the releases page . curl -L https://github.com/Jeffail/benthos/releases/download/${VERSION}/benthos_${VERSION}_linux_amd64.tar.gz | tar xz ./benthos -c ./yourconfig.yaml Run A Benthos stream pipeline is configured with a single config file, you can generate a fresh one with benthos --print-yaml or benthos --print-json . The main sections that make up a pipeline are input , buffer , pipeline and output . If we were to pipe stdin directly to Kafka those sections might look like this: input: type: stdin buffer: type: none pipeline: threads: 1 processors: [] output: type: kafka kafka: addresses: - localhost:9092 topic: benthos_stream You can execute this config with: benthos -c ./yourconfig.yaml With this config you can write messages in the terminal and they will be sent to your chosen Kafka topic, how exciting! Next, we might want to add some processing steps in order to mutate the messages. For example, we could add a text processor that converts all text into upper case: input: type: stdin buffer: type: none pipeline: threads: 1 processors: - type: text text: operator: to_upper output: type: kafka kafka: addresses: - localhost:9092 topic: benthos_stream Very useful! You can add as many processing steps as you like. Now that you are a Benthos expert check out all the other cool stuff you can do .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#install","text":"","title":"Install"},{"location":"getting_started/#build-with-go","text":"If you have Go version 1.11 or above you can build and install Benthos with: go install github.com/Jeffail/benthos/cmd/benthos benthos -c ./yourconfig.yaml","title":"Build With Go"},{"location":"getting_started/#pull-with-docker","text":"If you have docker installed you can pull the latest official Benthos image with: docker pull jeffail/benthos docker run --rm -v /path/to/your/config.yaml:/benthos.yaml jeffail/benthos","title":"Pull With Docker"},{"location":"getting_started/#download-an-archive","text":"Otherwise you can pull an archive containing Benthos from the releases page . curl -L https://github.com/Jeffail/benthos/releases/download/${VERSION}/benthos_${VERSION}_linux_amd64.tar.gz | tar xz ./benthos -c ./yourconfig.yaml","title":"Download an Archive"},{"location":"getting_started/#run","text":"A Benthos stream pipeline is configured with a single config file, you can generate a fresh one with benthos --print-yaml or benthos --print-json . The main sections that make up a pipeline are input , buffer , pipeline and output . If we were to pipe stdin directly to Kafka those sections might look like this: input: type: stdin buffer: type: none pipeline: threads: 1 processors: [] output: type: kafka kafka: addresses: - localhost:9092 topic: benthos_stream You can execute this config with: benthos -c ./yourconfig.yaml With this config you can write messages in the terminal and they will be sent to your chosen Kafka topic, how exciting! Next, we might want to add some processing steps in order to mutate the messages. For example, we could add a text processor that converts all text into upper case: input: type: stdin buffer: type: none pipeline: threads: 1 processors: - type: text text: operator: to_upper output: type: kafka kafka: addresses: - localhost:9092 topic: benthos_stream Very useful! You can add as many processing steps as you like. Now that you are a Benthos expert check out all the other cool stuff you can do .","title":"Run"},{"location":"pipeline/","text":"Pipeline Within a Benthos configuration, in between input and output , is a pipeline section. This section describes an array of processors that are to be applied to all messages, and are not bound to any particular input or output. If you have processors that are heavy on CPU and aren't specific to a certain input or output they are best suited for the pipeline section. It is advantageous to use the pipeline section as it allows you to set an explicit number of parallel threads of execution which should ideally match the number of available logical CPU cores. The following patterns allow you to achieve a distribution of work across these processing threads for different input arrangments. Multiple Consumers Sometimes our source of data can have many multiple connected clients and will distribute a stream of messages amongst them. In these circumstances it is possible to fully utilise a set of parallel processing threads without a buffer, provided that the number of consumers is greater than the number of threads. Ideally the number of consumers would be significantly higher than the number of threads in order to compensate for occasional IO stalls. For example, imagine we are consuming from a source baz , which is At-Least-Once and supports multiple connected clients. Our goal is to read the stream as fast as possible, perform mutations on the JSON payload using the jmespath processor , and write the resulting stream to bar . We also wish to take advantage of the delivery guarantees of the source and therefore want acknowledgements to flow directly from our output sink to the input source, and therefore need to avoid using a buffer. For this purpose we would be able to utilise our processing threads without the need for a buffer. We choose four processing threads to match our 4 CPU cores, and choose to use eight parallel consumers of the input baz . input: type: broker broker: copies: 8 inputs: - type: baz buffer: type: none pipeline: threads: 4 processors: - type: jmespath jmespath: query: reservations[].instances[].[tags[?Key=='Name'].Values[] | [0], type, state.name] output: type: bar With this config the pipeline within our Benthos instance would look something like the following: baz -\\ baz -\\ baz --- processor --- bar baz --- processor -/ baz --- processor -/ baz --- processor -/ baz -/ baz -/ Single Consumer Sometimes a source of data can only have a single consuming client. In these circumstances it is still possible to have the single stream of data processed on parallel processing threads by using a buffer . For example, say we have an input stream foo with only a single connected client. Our goal is to read the stream as fast as possible, perform mutations on the JSON payload using the jmespath processor , and write the resulting stream to bar . The messages from foo are At-Most-Once , and so we are not concerned with delivery guarantees and want to focus on performance instead. We have four logical CPU cores on our server and wish to dedicate them all to processing the data. We believe that the bar output will be fast enough to keep up with the stream with a single connection. We set our number of processing threads to four in order to match the CPU cores available. We also chose a memory buffer since it is the fastest buffer option, with a size of 5MB which we have determined to be more than enough to fit four messages of the stream at any given time. input: type: foo buffer: type: memory memory: limit: 5000000 pipeline: threads: 4 processors: - type: jmespath jmespath: query: reservations[].instances[].[tags[?Key=='Name'].Values[] | [0], type, state.name] output: type: bar With this config the pipeline within our Benthos instance would look something like the following: foo - memory buffer --- processor --- bar ( 5MB ) \\-- processor -/ \\-- processor -/ \\-- processor -/","title":"Pipeline"},{"location":"pipeline/#pipeline","text":"Within a Benthos configuration, in between input and output , is a pipeline section. This section describes an array of processors that are to be applied to all messages, and are not bound to any particular input or output. If you have processors that are heavy on CPU and aren't specific to a certain input or output they are best suited for the pipeline section. It is advantageous to use the pipeline section as it allows you to set an explicit number of parallel threads of execution which should ideally match the number of available logical CPU cores. The following patterns allow you to achieve a distribution of work across these processing threads for different input arrangments.","title":"Pipeline"},{"location":"pipeline/#multiple-consumers","text":"Sometimes our source of data can have many multiple connected clients and will distribute a stream of messages amongst them. In these circumstances it is possible to fully utilise a set of parallel processing threads without a buffer, provided that the number of consumers is greater than the number of threads. Ideally the number of consumers would be significantly higher than the number of threads in order to compensate for occasional IO stalls. For example, imagine we are consuming from a source baz , which is At-Least-Once and supports multiple connected clients. Our goal is to read the stream as fast as possible, perform mutations on the JSON payload using the jmespath processor , and write the resulting stream to bar . We also wish to take advantage of the delivery guarantees of the source and therefore want acknowledgements to flow directly from our output sink to the input source, and therefore need to avoid using a buffer. For this purpose we would be able to utilise our processing threads without the need for a buffer. We choose four processing threads to match our 4 CPU cores, and choose to use eight parallel consumers of the input baz . input: type: broker broker: copies: 8 inputs: - type: baz buffer: type: none pipeline: threads: 4 processors: - type: jmespath jmespath: query: reservations[].instances[].[tags[?Key=='Name'].Values[] | [0], type, state.name] output: type: bar With this config the pipeline within our Benthos instance would look something like the following: baz -\\ baz -\\ baz --- processor --- bar baz --- processor -/ baz --- processor -/ baz --- processor -/ baz -/ baz -/","title":"Multiple Consumers"},{"location":"pipeline/#single-consumer","text":"Sometimes a source of data can only have a single consuming client. In these circumstances it is still possible to have the single stream of data processed on parallel processing threads by using a buffer . For example, say we have an input stream foo with only a single connected client. Our goal is to read the stream as fast as possible, perform mutations on the JSON payload using the jmespath processor , and write the resulting stream to bar . The messages from foo are At-Most-Once , and so we are not concerned with delivery guarantees and want to focus on performance instead. We have four logical CPU cores on our server and wish to dedicate them all to processing the data. We believe that the bar output will be fast enough to keep up with the stream with a single connection. We set our number of processing threads to four in order to match the CPU cores available. We also chose a memory buffer since it is the fastest buffer option, with a size of 5MB which we have determined to be more than enough to fit four messages of the stream at any given time. input: type: foo buffer: type: memory memory: limit: 5000000 pipeline: threads: 4 processors: - type: jmespath jmespath: query: reservations[].instances[].[tags[?Key=='Name'].Values[] | [0], type, state.name] output: type: bar With this config the pipeline within our Benthos instance would look something like the following: foo - memory buffer --- processor --- bar ( 5MB ) \\-- processor -/ \\-- processor -/ \\-- processor -/","title":"Single Consumer"},{"location":"workflows/","text":"Workflows in Benthos The Problem A workflow is often expressed as a DAG of processing stages, where each stage can result in N possible next stages, until finally the flow ends at an exit node. For example, if we had processing stages A, B, C and D, where stage A could result in either stage B or C being next, always followed by D, it might look something like this: /-- B --\\ A --| |-- D \\-- C --/ This flow would be easy to express in a standard Benthos config, we could simply use a conditional processor to route to either B or C depending on a condition on the result of A. However, this method of flow control quickly becomes unfeasible as the DAG gets more complicated, imagine expressing this flow using conditional or switch processors: /-- B -------------|-- D / / A --| /-- E --| \\-- C --| \\ \\----------|-- F And imagine doing so knowing that the diagram is subject to change over time. Yikes! Automatic DAG Resolution Benthos comes with a workflow solution called the process_dag processor, which takes a map of process_map processors and calculates the DAG by creating a dependency tree based on their mappings. This method allows you to build your workflow without having to explicitly define (or even know) the final order of stages. Instead, you define how each individual stage mutates the original document and Benthos automatically resolves the necessary execution order, including parallel execution of stages within the same dependency tier. For more information regarding these processors please check their respective documentation. Example Concepts like these are much easier to explain with a simple example, so let's define our workflow stages. We are starting off with arbitrary JSON documents and want to make some HTTP requests to a series of enrichment services where the end result will be placed within the original JSON document at the path tmp.enrichments . The enrichment services, depending on the contents of the original payload, will return content that might need to be fed into a subsequent enrichment service in order to obtain the final result we are looking for. Some of these services might fail due to the contents of the original message, in which case we want to send the full contents to a recovery service that will attempt to recover a fallback version of the goal enrichment. The enrichment stages can be described as: A) Perform an HTTP request to fooserve with the full message payload as the contents. The response can be one of the following: 200: {\"type\":\"bar\",\"bar\":{\"some\":\"fields\"}} 200: {\"type\":\"baz\",\"baz\":{\"some\":\"fields\"}} 400: Bad request B) Perform an HTTP request to barserve with the object bar taken from the output of stage A response 1. The response can be one of the following: 200: {\"type\":\"baz\",\"baz\":{\"some\":\"fields\"}} 400: Bad request C) Perform an HTTP request to bazserve with the object baz taken either from the output of stage A response 2 or stage B response 1. The response can be one of the following: 200: {\"type\":\"qux\",\"qux\":{\"some\":\"fields\"}} 400: Bad request D) If any previous stage returns a 400 then we perform an HTTP request to recoverserve with the full message payload as the contents. The response will always be the following: 200: {\"type\":\"qux\",\"qux\":{\"default\":\"fields\"}} E) Place the final contents (the object at qux ) in the document at tmp.enrichments.qux . A diagram for this flow might look like this: /--------|-- C --|----------------|- E / / \\ / A --|-- B --| \\ / \\ \\ \\ / \\--------|------------|-- D --| For simplicity we will attempt each HTTP request three times and interpret any failure after those attempts as the equivalent of a 400 status code. Stage A There's no need for a premap since we are sending the entire original payload. The postmap targets are either the object bar or baz , and both are optional since we aren't sure which will be present. processors: - type: http http: parallel: true request: url: http://fooserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap_optional: tmp.enrichments.bar: bar tmp.enrichments.baz: baz Stage B The premap for stage B is tmp.enrichments.bar which if not present in the payload results in this stage being skipped. Stage A is the only stage that provides tmp.enrichments.bar and Benthos will make sure stage A is run before stage B. If we were to add more stages later on that might provide tmp.enrichments.bar then they will also automatically be run before this stage. premap: .: tmp.enrichments.bar processors: - type: http http: parallel: true request: url: http://barserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.baz: baz Stage C The premap for stage C is tmp.enrichments.baz which if not present in the payload results in this stage being skipped. Since either stage A or A might provide this target they will both be run before this stage. premap: .: tmp.enrichments.baz processors: - type: http http: parallel: true request: url: http://bazserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux Stage D Stage D is unique since for this stage we need to send the entire contents of the payload (which means no premap), but we still only wish to trigger this stage once any other stage capable of providing tmp.enrichments.qux has already been executed. Therefore, we add the field dependencies with tmp.enrichments.qux as an explicit dependency. We also only wish to execute this stage when tmp.enrichments.qux is missing from the payload, therefore we add a condition that performs this check. dependencies: - tmp.enrichments.qux conditions: - type: jmespath jmespath: query: 'tmp.enrichments.qux == `null`' processors: - type: http http: parallel: true request: url: http://recoverserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux Stage E Stage E doesn't need writing explicitly since by the end of Stage D we should already have our enrichment at tmp.enrichments.qux . The final Benthos configuration might look something like this: input: type: stdin # TODO pipeline: processors: - type: process_dag process_dag: A: processors: - type: http http: parallel: true request: url: http://fooserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap_optional: tmp.enrichments.bar: bar tmp.enrichments.baz: baz B: premap: .: tmp.enrichments.bar processors: - type: http http: parallel: true request: url: http://barserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.baz: baz C: premap: .: tmp.enrichments.baz processors: - type: http http: parallel: true request: url: http://bazserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux D: dependencies: - tmp.enrichments.qux conditions: - type: jmespath jmespath: query: 'tmp.enrichments.qux == `null`' processors: - type: http http: parallel: true request: url: http://recoverserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux output: type: stdout # TODO","title":"Workflows"},{"location":"workflows/#workflows-in-benthos","text":"","title":"Workflows in Benthos"},{"location":"workflows/#the-problem","text":"A workflow is often expressed as a DAG of processing stages, where each stage can result in N possible next stages, until finally the flow ends at an exit node. For example, if we had processing stages A, B, C and D, where stage A could result in either stage B or C being next, always followed by D, it might look something like this: /-- B --\\ A --| |-- D \\-- C --/ This flow would be easy to express in a standard Benthos config, we could simply use a conditional processor to route to either B or C depending on a condition on the result of A. However, this method of flow control quickly becomes unfeasible as the DAG gets more complicated, imagine expressing this flow using conditional or switch processors: /-- B -------------|-- D / / A --| /-- E --| \\-- C --| \\ \\----------|-- F And imagine doing so knowing that the diagram is subject to change over time. Yikes!","title":"The Problem"},{"location":"workflows/#automatic-dag-resolution","text":"Benthos comes with a workflow solution called the process_dag processor, which takes a map of process_map processors and calculates the DAG by creating a dependency tree based on their mappings. This method allows you to build your workflow without having to explicitly define (or even know) the final order of stages. Instead, you define how each individual stage mutates the original document and Benthos automatically resolves the necessary execution order, including parallel execution of stages within the same dependency tier. For more information regarding these processors please check their respective documentation.","title":"Automatic DAG Resolution"},{"location":"workflows/#example","text":"Concepts like these are much easier to explain with a simple example, so let's define our workflow stages. We are starting off with arbitrary JSON documents and want to make some HTTP requests to a series of enrichment services where the end result will be placed within the original JSON document at the path tmp.enrichments . The enrichment services, depending on the contents of the original payload, will return content that might need to be fed into a subsequent enrichment service in order to obtain the final result we are looking for. Some of these services might fail due to the contents of the original message, in which case we want to send the full contents to a recovery service that will attempt to recover a fallback version of the goal enrichment. The enrichment stages can be described as: A) Perform an HTTP request to fooserve with the full message payload as the contents. The response can be one of the following: 200: {\"type\":\"bar\",\"bar\":{\"some\":\"fields\"}} 200: {\"type\":\"baz\",\"baz\":{\"some\":\"fields\"}} 400: Bad request B) Perform an HTTP request to barserve with the object bar taken from the output of stage A response 1. The response can be one of the following: 200: {\"type\":\"baz\",\"baz\":{\"some\":\"fields\"}} 400: Bad request C) Perform an HTTP request to bazserve with the object baz taken either from the output of stage A response 2 or stage B response 1. The response can be one of the following: 200: {\"type\":\"qux\",\"qux\":{\"some\":\"fields\"}} 400: Bad request D) If any previous stage returns a 400 then we perform an HTTP request to recoverserve with the full message payload as the contents. The response will always be the following: 200: {\"type\":\"qux\",\"qux\":{\"default\":\"fields\"}} E) Place the final contents (the object at qux ) in the document at tmp.enrichments.qux . A diagram for this flow might look like this: /--------|-- C --|----------------|- E / / \\ / A --|-- B --| \\ / \\ \\ \\ / \\--------|------------|-- D --| For simplicity we will attempt each HTTP request three times and interpret any failure after those attempts as the equivalent of a 400 status code.","title":"Example"},{"location":"workflows/#stage-a","text":"There's no need for a premap since we are sending the entire original payload. The postmap targets are either the object bar or baz , and both are optional since we aren't sure which will be present. processors: - type: http http: parallel: true request: url: http://fooserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap_optional: tmp.enrichments.bar: bar tmp.enrichments.baz: baz","title":"Stage A"},{"location":"workflows/#stage-b","text":"The premap for stage B is tmp.enrichments.bar which if not present in the payload results in this stage being skipped. Stage A is the only stage that provides tmp.enrichments.bar and Benthos will make sure stage A is run before stage B. If we were to add more stages later on that might provide tmp.enrichments.bar then they will also automatically be run before this stage. premap: .: tmp.enrichments.bar processors: - type: http http: parallel: true request: url: http://barserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.baz: baz","title":"Stage B"},{"location":"workflows/#stage-c","text":"The premap for stage C is tmp.enrichments.baz which if not present in the payload results in this stage being skipped. Since either stage A or A might provide this target they will both be run before this stage. premap: .: tmp.enrichments.baz processors: - type: http http: parallel: true request: url: http://bazserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux","title":"Stage C"},{"location":"workflows/#stage-d","text":"Stage D is unique since for this stage we need to send the entire contents of the payload (which means no premap), but we still only wish to trigger this stage once any other stage capable of providing tmp.enrichments.qux has already been executed. Therefore, we add the field dependencies with tmp.enrichments.qux as an explicit dependency. We also only wish to execute this stage when tmp.enrichments.qux is missing from the payload, therefore we add a condition that performs this check. dependencies: - tmp.enrichments.qux conditions: - type: jmespath jmespath: query: 'tmp.enrichments.qux == `null`' processors: - type: http http: parallel: true request: url: http://recoverserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux","title":"Stage D"},{"location":"workflows/#stage-e","text":"Stage E doesn't need writing explicitly since by the end of Stage D we should already have our enrichment at tmp.enrichments.qux . The final Benthos configuration might look something like this: input: type: stdin # TODO pipeline: processors: - type: process_dag process_dag: A: processors: - type: http http: parallel: true request: url: http://fooserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap_optional: tmp.enrichments.bar: bar tmp.enrichments.baz: baz B: premap: .: tmp.enrichments.bar processors: - type: http http: parallel: true request: url: http://barserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.baz: baz C: premap: .: tmp.enrichments.baz processors: - type: http http: parallel: true request: url: http://bazserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux D: dependencies: - tmp.enrichments.qux conditions: - type: jmespath jmespath: query: 'tmp.enrichments.qux == `null`' processors: - type: http http: parallel: true request: url: http://recoverserve/enrich verb: POST headers: Content-Type: application/json backoff_on: [ 429 ] drop_on: [ 400 ] retries: 3 postmap: tmp.enrichments.qux: qux output: type: stdout # TODO","title":"Stage E"},{"location":"api/streams/","text":"Streams API When Benthos is run in --streams mode it will open up an HTTP REST API for creating and managing independent streams of data instead of creating a single stream. Each stream has its own input, buffer, pipeline and output sections which contains an isolated stream of data with its own lifetime. A walkthrough on using this API can be found here . API GET /streams Returns a map of existing streams by their unique identifiers to an object showing their status and uptime. Response 200 { string, stream id : { active : bool, whether the stream is running , uptime : float, uptime in seconds , uptime_str : string, human readable string of uptime } } POST /streams Sets the entire collection of streams to the body of the request. Streams that exist but aren't within the request body are removed , streams that exist already and are in the request body are updated, other streams within the request body are created. { string, stream id : object, a standard Benthos stream configuration } Response 200 The streams were updated successfully. POST /streams/{id} Create a new stream identified by id by posting a body containing the stream configuration in either JSON or YAML format. The configuration should be a standard Benthos configuration containing the sections input , buffer , pipeline and output . Response 200 The stream was created successfully. GET /streams/{id} Read the details of an existing stream identified by id . Response 200 { active : bool, whether the stream is running , uptime : float, uptime in seconds , uptime_str : string, human readable string of uptime , config : object, the configuration of the stream } PUT /streams/{id} Update an existing stream identified by id by posting a body containing the new stream configuration in either JSON or YAML format. The configuration should be a standard Benthos configuration containing the sections input , buffer , pipeline and output . The previous stream will be shut down before and a new stream will take its place. Response 200 The stream was updated successfully. PATCH /streams/{id} Update an existing stream identified by id by posting a body containing only changes to be made to the existing configuration. The existing configuration will be patched with the new fields and the stream restarted with the result. Response 200 The stream was patched successfully. DELETE /streams/{id} Attempt to shut down and remove a stream identified by id . Response 200 The stream was found, shut down and removed successfully. GET /streams/{id}/stats Read the metrics of an existing stream as a hierarchical JSON object. Response 200 The stream was found.","title":"Streams"},{"location":"api/streams/#streams-api","text":"When Benthos is run in --streams mode it will open up an HTTP REST API for creating and managing independent streams of data instead of creating a single stream. Each stream has its own input, buffer, pipeline and output sections which contains an isolated stream of data with its own lifetime. A walkthrough on using this API can be found here .","title":"Streams API"},{"location":"api/streams/#api","text":"","title":"API"},{"location":"api/streams/#get-streams","text":"Returns a map of existing streams by their unique identifiers to an object showing their status and uptime.","title":"GET /streams"},{"location":"api/streams/#response-200","text":"{ string, stream id : { active : bool, whether the stream is running , uptime : float, uptime in seconds , uptime_str : string, human readable string of uptime } }","title":"Response 200"},{"location":"api/streams/#post-streams","text":"Sets the entire collection of streams to the body of the request. Streams that exist but aren't within the request body are removed , streams that exist already and are in the request body are updated, other streams within the request body are created. { string, stream id : object, a standard Benthos stream configuration }","title":"POST /streams"},{"location":"api/streams/#response-200_1","text":"The streams were updated successfully.","title":"Response 200"},{"location":"api/streams/#post-streamsid","text":"Create a new stream identified by id by posting a body containing the stream configuration in either JSON or YAML format. The configuration should be a standard Benthos configuration containing the sections input , buffer , pipeline and output .","title":"POST /streams/{id}"},{"location":"api/streams/#response-200_2","text":"The stream was created successfully.","title":"Response 200"},{"location":"api/streams/#get-streamsid","text":"Read the details of an existing stream identified by id .","title":"GET /streams/{id}"},{"location":"api/streams/#response-200_3","text":"{ active : bool, whether the stream is running , uptime : float, uptime in seconds , uptime_str : string, human readable string of uptime , config : object, the configuration of the stream }","title":"Response 200"},{"location":"api/streams/#put-streamsid","text":"Update an existing stream identified by id by posting a body containing the new stream configuration in either JSON or YAML format. The configuration should be a standard Benthos configuration containing the sections input , buffer , pipeline and output . The previous stream will be shut down before and a new stream will take its place.","title":"PUT /streams/{id}"},{"location":"api/streams/#response-200_4","text":"The stream was updated successfully.","title":"Response 200"},{"location":"api/streams/#patch-streamsid","text":"Update an existing stream identified by id by posting a body containing only changes to be made to the existing configuration. The existing configuration will be patched with the new fields and the stream restarted with the result.","title":"PATCH /streams/{id}"},{"location":"api/streams/#response-200_5","text":"The stream was patched successfully.","title":"Response 200"},{"location":"api/streams/#delete-streamsid","text":"Attempt to shut down and remove a stream identified by id .","title":"DELETE /streams/{id}"},{"location":"api/streams/#response-200_6","text":"The stream was found, shut down and removed successfully.","title":"Response 200"},{"location":"api/streams/#get-streamsidstats","text":"Read the metrics of an existing stream as a hierarchical JSON object.","title":"GET /streams/{id}/stats"},{"location":"api/streams/#response-200_7","text":"The stream was found.","title":"Response 200"},{"location":"buffers/","text":"Buffers This document was generated with benthos --list-buffers Buffers can solve a number of typical streaming problems and are worth considering if you face circumstances similar to the following: Input sources can periodically spike beyond the capacity of your output sinks. You want to use parallel processing pipelines . You have more outputs than inputs and wish to distribute messages across them in order to maximize overall throughput. Your input source needs occasional protection against back pressure from your sink, e.g. during restarts. Please keep in mind that all buffers have an eventual limit. If you believe that a problem you have would be solved by a buffer the next step is to choose an implementation based on the throughput and delivery guarantees you need. In order to help here are some simplified tables outlining the different options and their qualities: Performance Type Throughput Consumers Capacity Memory Highest Parallel RAM Mmap File High Single Disk Delivery Guarantees Type On Restart On Crash On Disk Corruption Memory Lost Lost Lost Mmap File Persisted Lost Lost Contents memory mmap_file none memory type: memory memory: limit: 5.24288e+08 The memory buffer type simply allocates a set amount of RAM for buffering messages. This can be useful when reading from sources that produce large bursts of data. Messages inside the buffer are lost if the service is stopped. mmap_file type: mmap_file mmap_file: clean_up: true directory: file_size: 2.62144e+08 reserved_disk_space: 1.048576e+08 retry_period: 1s The mmap file buffer type uses memory mapped files to perform low-latency, file-persisted buffering of messages. To configure the mmap file buffer you need to designate a writeable directory for storing the mapped files. Benthos will create multiple files in this directory as it fills them. When files are fully read from they will be deleted. You can disable this feature if you wish to preserve the data indefinitely, but the directory will fill up as fast as data passes through. WARNING: This buffer currently wipes all metadata from message payloads. If you are using metadata in your pipeline you should avoid using this buffer, or preferably all buffers altogether. none type: none none: {} Selecting no buffer (default) means the output layer is directly coupled with the input layer. This is the safest and lowest latency option since acknowledgements from at-least-once protocols can be propagated all the way from the output protocol to the input protocol. If the output layer is hit with back pressure it will propagate all the way to the input layer, and further up the data stream. If you need to relieve your pipeline of this back pressure consider using a more robust buffering solution such as Kafka before resorting to alternatives.","title":"Buffers"},{"location":"buffers/#buffers","text":"This document was generated with benthos --list-buffers Buffers can solve a number of typical streaming problems and are worth considering if you face circumstances similar to the following: Input sources can periodically spike beyond the capacity of your output sinks. You want to use parallel processing pipelines . You have more outputs than inputs and wish to distribute messages across them in order to maximize overall throughput. Your input source needs occasional protection against back pressure from your sink, e.g. during restarts. Please keep in mind that all buffers have an eventual limit. If you believe that a problem you have would be solved by a buffer the next step is to choose an implementation based on the throughput and delivery guarantees you need. In order to help here are some simplified tables outlining the different options and their qualities:","title":"Buffers"},{"location":"buffers/#performance","text":"Type Throughput Consumers Capacity Memory Highest Parallel RAM Mmap File High Single Disk","title":"Performance"},{"location":"buffers/#delivery-guarantees","text":"Type On Restart On Crash On Disk Corruption Memory Lost Lost Lost Mmap File Persisted Lost Lost","title":"Delivery Guarantees"},{"location":"buffers/#contents","text":"memory mmap_file none","title":"Contents"},{"location":"buffers/#memory","text":"type: memory memory: limit: 5.24288e+08 The memory buffer type simply allocates a set amount of RAM for buffering messages. This can be useful when reading from sources that produce large bursts of data. Messages inside the buffer are lost if the service is stopped.","title":"memory"},{"location":"buffers/#mmap_file","text":"type: mmap_file mmap_file: clean_up: true directory: file_size: 2.62144e+08 reserved_disk_space: 1.048576e+08 retry_period: 1s The mmap file buffer type uses memory mapped files to perform low-latency, file-persisted buffering of messages. To configure the mmap file buffer you need to designate a writeable directory for storing the mapped files. Benthos will create multiple files in this directory as it fills them. When files are fully read from they will be deleted. You can disable this feature if you wish to preserve the data indefinitely, but the directory will fill up as fast as data passes through. WARNING: This buffer currently wipes all metadata from message payloads. If you are using metadata in your pipeline you should avoid using this buffer, or preferably all buffers altogether.","title":"mmap_file"},{"location":"buffers/#none","text":"type: none none: {} Selecting no buffer (default) means the output layer is directly coupled with the input layer. This is the safest and lowest latency option since acknowledgements from at-least-once protocols can be propagated all the way from the output protocol to the input protocol. If the output layer is hit with back pressure it will propagate all the way to the input layer, and further up the data stream. If you need to relieve your pipeline of this back pressure consider using a more robust buffering solution such as Kafka before resorting to alternatives.","title":"none"},{"location":"caches/","text":"Caches This document was generated with benthos --list-caches A cache is a key/value store which can be used by certain processors for applications such as deduplication. Caches are listed with unique labels which are referred to by processors that may share them. For example, if we were to deduplicate hypothetical 'foo' and 'bar' inputs, but not 'baz', we could arrange our config as follows: input: type: broker broker: inputs: - type: foo processors: - type: dedupe dedupe: cache: foobar hash: xxhash - type: bar processors: - type: dedupe dedupe: cache: foobar hash: xxhash - type: baz resources: caches: foobar: type: memcached memcached: addresses: - localhost:11211 ttl: 60 In that example we have a single memcached based cache 'foobar', which is used by the dedupe processors of both the 'foo' and 'bar' inputs. A message received from both 'foo' and 'bar' would therefore be detected and removed since the cache is the same for both inputs. Contents dynamodb memcached memory redis dynamodb type: dynamodb dynamodb: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s consistent_read: false credentials: id: role: role_external_id: secret: token: data_key: endpoint: hash_key: max_retries: 3 region: eu-west-1 table: ttl: ttl_key: The dynamodb cache stores key/value pairs as a single document in a DynamoDB table. The key is stored as a string value and used as the table hash key. The value is stored as a binary value using the data_key field name. A prefix can be specified to allow multiple cache types to share a single DynamoDB table. An optional TTL duration ( ttl ) and field ( ttl_key ) can be specified if the backing table has TTL enabled. Strong read consistency can be enabled using the consistent_read configuration field. memcached type: memcached memcached: addresses: - localhost:11211 prefix: retries: 3 retry_period: 500ms ttl: 300 Connects to a cluster of memcached services, a prefix can be specified to allow multiple cache types to share a memcached cluster under different namespaces. memory type: memory memory: compaction_interval: 60s ttl: 300 The memory cache simply stores key/value pairs in a map held in memory. This cache is therefore reset every time the service restarts. Each item in the cache has a TTL set from the moment it was last edited, after which it will be removed during the next compaction. A compaction only occurs during a write where the time since the last compaction is above the compaction interval. It is therefore possible to obtain values of keys that have expired between compactions. redis type: redis redis: expiration: 24h prefix: retries: 3 retry_period: 500ms url: tcp://localhost:6379 Use a Redis instance as a cache. The expiration can be set to zero or an empty string in order to set no expiration.","title":"Caches"},{"location":"caches/#caches","text":"This document was generated with benthos --list-caches A cache is a key/value store which can be used by certain processors for applications such as deduplication. Caches are listed with unique labels which are referred to by processors that may share them. For example, if we were to deduplicate hypothetical 'foo' and 'bar' inputs, but not 'baz', we could arrange our config as follows: input: type: broker broker: inputs: - type: foo processors: - type: dedupe dedupe: cache: foobar hash: xxhash - type: bar processors: - type: dedupe dedupe: cache: foobar hash: xxhash - type: baz resources: caches: foobar: type: memcached memcached: addresses: - localhost:11211 ttl: 60 In that example we have a single memcached based cache 'foobar', which is used by the dedupe processors of both the 'foo' and 'bar' inputs. A message received from both 'foo' and 'bar' would therefore be detected and removed since the cache is the same for both inputs.","title":"Caches"},{"location":"caches/#contents","text":"dynamodb memcached memory redis","title":"Contents"},{"location":"caches/#dynamodb","text":"type: dynamodb dynamodb: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s consistent_read: false credentials: id: role: role_external_id: secret: token: data_key: endpoint: hash_key: max_retries: 3 region: eu-west-1 table: ttl: ttl_key: The dynamodb cache stores key/value pairs as a single document in a DynamoDB table. The key is stored as a string value and used as the table hash key. The value is stored as a binary value using the data_key field name. A prefix can be specified to allow multiple cache types to share a single DynamoDB table. An optional TTL duration ( ttl ) and field ( ttl_key ) can be specified if the backing table has TTL enabled. Strong read consistency can be enabled using the consistent_read configuration field.","title":"dynamodb"},{"location":"caches/#memcached","text":"type: memcached memcached: addresses: - localhost:11211 prefix: retries: 3 retry_period: 500ms ttl: 300 Connects to a cluster of memcached services, a prefix can be specified to allow multiple cache types to share a memcached cluster under different namespaces.","title":"memcached"},{"location":"caches/#memory","text":"type: memory memory: compaction_interval: 60s ttl: 300 The memory cache simply stores key/value pairs in a map held in memory. This cache is therefore reset every time the service restarts. Each item in the cache has a TTL set from the moment it was last edited, after which it will be removed during the next compaction. A compaction only occurs during a write where the time since the last compaction is above the compaction interval. It is therefore possible to obtain values of keys that have expired between compactions.","title":"memory"},{"location":"caches/#redis","text":"type: redis redis: expiration: 24h prefix: retries: 3 retry_period: 500ms url: tcp://localhost:6379 Use a Redis instance as a cache. The expiration can be set to zero or an empty string in order to set no expiration.","title":"redis"},{"location":"conditions/","text":"Conditions This document was generated with benthos --list-conditions Conditions are boolean queries that can be executed based on the contents of a message. Some processors such as filter use conditions for expressing their logic. Conditions themselves can modify ( not ) and combine ( and , or ) other conditions, and can therefore be used to create complex boolean expressions. The format of a condition is similar to other Benthos types: condition: type: text text: operator: equals part: 0 arg: hello world And using boolean condition types we can combine multiple conditions together: condition: type: and and: - type: text text: operator: contains arg: hello world - type: or or: - type: text text: operator: contains arg: foo - type: not not: type: text text: operator: contains arg: bar The above example could be summarised as 'text contains \"hello world\" and also either contains \"foo\" or does not contain \"bar\"'. Conditions can be extremely useful for creating filters on an output. By using a fan out output broker with 'filter' processors on the brokered outputs it is possible to build curated data streams that filter on the content of each message. Batching and Multipart Messages All conditions can be applied to a multipart message, which is synonymous with a batch. Some conditions target a specific part of a message batch, and require you specify the target index with the field part . Some processors such as filter apply its conditions across the whole batch. Whereas other processors such as filter_parts will apply its conditions on each part of a batch individually, in which case the condition acts as if it were referencing a single message batch. Part indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if part = -1 then the selected part will be the last part of the message, if part = -2 then the part before the last element with be selected, and so on. Reusing Conditions Sometimes large chunks of logic are reused across processors, or nested multiple times as branches of a larger condition. It is possible to avoid writing duplicate condition configs by using the resource condition . Contents all and any bounds_check check_field count jmespath metadata not or processor_failed resource static text xor all type: all all: {} All is a condition that tests a child condition against each message of a batch individually. If all messages pass the child condition then this condition also passes. For example, if we wanted to check that all messages of a batch contain the word 'foo' we could use this config: type: all all: type: text text: operator: contains arg: foo and type: and and: [] And is a condition that returns the logical AND of its children conditions. any type: any any: {} Any is a condition that tests a child condition against each message of a batch individually. If any message passes the child condition then this condition also passes. For example, if we wanted to check that at least one message of a batch contains the word 'foo' we could use this config: type: any any: type: text text: operator: contains arg: foo bounds_check type: bounds_check bounds_check: max_part_size: 1.073741824e+09 max_parts: 100 min_part_size: 1 min_parts: 1 Checks a message against a set of bounds. check_field type: check_field check_field: condition: {} parts: [] path: Extracts the value of a field within messages (currently only JSON format is supported) and then tests the extracted value against a child condition. count type: count count: arg: 100 Counts messages starting from one, returning true until the counter reaches its target, at which point it will return false and reset the counter. This condition is useful when paired with the read_until input, as it can be used to cut the input stream off once a certain number of messages have been read. It is worth noting that each discrete count condition will have its own counter. Parallel processors containing a count condition will therefore count independently. It is, however, possible to share the counter across processor pipelines by defining the count condition as a resource. jmespath type: jmespath jmespath: part: 0 query: Parses a message part as a JSON blob and attempts to apply a JMESPath expression to it, expecting a boolean response. If the response is true the condition passes, otherwise it does not. Please refer to the JMESPath website for information and tutorials regarding the syntax of expressions. For example, with the following config: jmespath: part: 0 query: a == 'foo' If the initial jmespaths of part 0 were: { a : foo } Then the condition would pass. JMESPath is traditionally used for mutating JSON, in order to do this please instead use the jmespath processor. metadata type: metadata metadata: arg: key: operator: equals_cs part: 0 Metadata is a condition that checks metadata keys of a message part against an operator from the following list: enum Checks whether the contents of a metadata key matches one of the defined enum values. type: metadata metadata: operator: enum part: 0 key: foo arg: - bar - baz - qux - quux equals Checks whether the contents of a metadata key matches an argument. This operator is case insensitive. type: metadata metadata: operator: equals part: 0 key: foo arg: bar equals_cs Checks whether the contents of a metadata key matches an argument. This operator is case sensitive. type: metadata metadata: operator: equals_cs part: 0 key: foo arg: BAR exists Checks whether a metadata key exists. type: metadata metadata: operator: exists part: 0 key: foo greater_than Checks whether the contents of a metadata key, parsed as a floating point number, is greater than an argument. Returns false if the metadata value cannot be parsed into a number. type: metadata metadata: operator: greater_than part: 0 key: foo arg: 3 has_prefix Checks whether the contents of a metadata key match one of the provided prefixes. The arg field can either be a singular prefix string or a list of prefixes. type: metadata metadata: operator: has_prefix part: 0 key: foo arg: - foo - bar - baz less_than Checks whether the contents of a metadata key, parsed as a floating point number, is less than an argument. Returns false if the metadata value cannot be parsed into a number. type: metadata metadata: operator: less_than part: 0 key: foo arg: 3 regexp_partial Checks whether any section of the contents of a metadata key matches a regular expression (RE2 syntax). type: metadata metadata: operator: regexp_partial part: 0 key: foo arg: 1[a-z]2 regexp_exact Checks whether the contents of a metadata key exactly matches a regular expression (RE2 syntax). type: metadata metadata: operator: regexp_partial part: 0 key: foo arg: 1[a-z]2 not type: not not: {} Not is a condition that returns the opposite (NOT) of its child condition. The body of a not object is the child condition, i.e. in order to express 'part 0 NOT equal to \"foo\"' you could have the following YAML config: type: not not: type: text text: operator: equal part: 0 arg: foo Or, the same example as JSON: { type : not , not : { type : text , text : { operator : equal , part : 0, arg : foo } } } or type: or or: [] Or is a condition that returns the logical OR of its children conditions. processor_failed type: processor_failed processor_failed: part: 0 Returns true if a processing stage of a message has failed. This condition is useful for dropping failed messages or creating dead letter queues, you can read more about these patterns here . resource type: resource resource: Resource is a condition type that runs a condition resource by its name. This condition allows you to run the same configured condition resource in multiple processors, or as a branch of another condition. For example, let's imagine we have two outputs, one of which only receives messages that satisfy a condition and the other receives the logical NOT of that same condition. In this example we can save ourselves the trouble of configuring the same condition twice by referring to it as a resource, like this: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: processors: - type: filter filter: type: resource resource: foobar - type: bar bar: processors: - type: filter filter: type: not not: type: resource resource: foobar resources: conditions: foobar: type: text text: operator: equals_cs part: 1 arg: filter me please static type: static static: true Static is a condition that always resolves to the same static boolean value. text type: text text: arg: operator: equals_cs part: 0 Text is a condition that checks the contents of a message part as plain text against a logical operator and an argument. Available logical operators are: equals_cs Checks whether the part equals the argument (case sensitive.) equals Checks whether the part equals the argument under unicode case-folding (case insensitive.) contains_cs Checks whether the part contains the argument (case sensitive.) contains Checks whether the part contains the argument under unicode case-folding (case insensitive.) prefix_cs Checks whether the part begins with the argument (case sensitive.) prefix Checks whether the part begins with the argument under unicode case-folding (case insensitive.) suffix_cs Checks whether the part ends with the argument (case sensitive.) suffix Checks whether the part ends with the argument under unicode case-folding (case insensitive.) regexp_partial Checks whether any section of the message part matches a regular expression (RE2 syntax). regexp_exact Checks whether the message part exactly matches a regular expression (RE2 syntax). xor type: xor xor: [] Xor is a condition that returns the logical XOR of its children conditions, meaning it only resolves to true if exactly one of its children conditions resolves to true.","title":"Conditions"},{"location":"conditions/#conditions","text":"This document was generated with benthos --list-conditions Conditions are boolean queries that can be executed based on the contents of a message. Some processors such as filter use conditions for expressing their logic. Conditions themselves can modify ( not ) and combine ( and , or ) other conditions, and can therefore be used to create complex boolean expressions. The format of a condition is similar to other Benthos types: condition: type: text text: operator: equals part: 0 arg: hello world And using boolean condition types we can combine multiple conditions together: condition: type: and and: - type: text text: operator: contains arg: hello world - type: or or: - type: text text: operator: contains arg: foo - type: not not: type: text text: operator: contains arg: bar The above example could be summarised as 'text contains \"hello world\" and also either contains \"foo\" or does not contain \"bar\"'. Conditions can be extremely useful for creating filters on an output. By using a fan out output broker with 'filter' processors on the brokered outputs it is possible to build curated data streams that filter on the content of each message.","title":"Conditions"},{"location":"conditions/#batching-and-multipart-messages","text":"All conditions can be applied to a multipart message, which is synonymous with a batch. Some conditions target a specific part of a message batch, and require you specify the target index with the field part . Some processors such as filter apply its conditions across the whole batch. Whereas other processors such as filter_parts will apply its conditions on each part of a batch individually, in which case the condition acts as if it were referencing a single message batch. Part indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if part = -1 then the selected part will be the last part of the message, if part = -2 then the part before the last element with be selected, and so on.","title":"Batching and Multipart Messages"},{"location":"conditions/#reusing-conditions","text":"Sometimes large chunks of logic are reused across processors, or nested multiple times as branches of a larger condition. It is possible to avoid writing duplicate condition configs by using the resource condition .","title":"Reusing Conditions"},{"location":"conditions/#contents","text":"all and any bounds_check check_field count jmespath metadata not or processor_failed resource static text xor","title":"Contents"},{"location":"conditions/#all","text":"type: all all: {} All is a condition that tests a child condition against each message of a batch individually. If all messages pass the child condition then this condition also passes. For example, if we wanted to check that all messages of a batch contain the word 'foo' we could use this config: type: all all: type: text text: operator: contains arg: foo","title":"all"},{"location":"conditions/#and","text":"type: and and: [] And is a condition that returns the logical AND of its children conditions.","title":"and"},{"location":"conditions/#any","text":"type: any any: {} Any is a condition that tests a child condition against each message of a batch individually. If any message passes the child condition then this condition also passes. For example, if we wanted to check that at least one message of a batch contains the word 'foo' we could use this config: type: any any: type: text text: operator: contains arg: foo","title":"any"},{"location":"conditions/#bounds_check","text":"type: bounds_check bounds_check: max_part_size: 1.073741824e+09 max_parts: 100 min_part_size: 1 min_parts: 1 Checks a message against a set of bounds.","title":"bounds_check"},{"location":"conditions/#check_field","text":"type: check_field check_field: condition: {} parts: [] path: Extracts the value of a field within messages (currently only JSON format is supported) and then tests the extracted value against a child condition.","title":"check_field"},{"location":"conditions/#count","text":"type: count count: arg: 100 Counts messages starting from one, returning true until the counter reaches its target, at which point it will return false and reset the counter. This condition is useful when paired with the read_until input, as it can be used to cut the input stream off once a certain number of messages have been read. It is worth noting that each discrete count condition will have its own counter. Parallel processors containing a count condition will therefore count independently. It is, however, possible to share the counter across processor pipelines by defining the count condition as a resource.","title":"count"},{"location":"conditions/#jmespath","text":"type: jmespath jmespath: part: 0 query: Parses a message part as a JSON blob and attempts to apply a JMESPath expression to it, expecting a boolean response. If the response is true the condition passes, otherwise it does not. Please refer to the JMESPath website for information and tutorials regarding the syntax of expressions. For example, with the following config: jmespath: part: 0 query: a == 'foo' If the initial jmespaths of part 0 were: { a : foo } Then the condition would pass. JMESPath is traditionally used for mutating JSON, in order to do this please instead use the jmespath processor.","title":"jmespath"},{"location":"conditions/#metadata","text":"type: metadata metadata: arg: key: operator: equals_cs part: 0 Metadata is a condition that checks metadata keys of a message part against an operator from the following list:","title":"metadata"},{"location":"conditions/#enum","text":"Checks whether the contents of a metadata key matches one of the defined enum values. type: metadata metadata: operator: enum part: 0 key: foo arg: - bar - baz - qux - quux","title":"enum"},{"location":"conditions/#equals","text":"Checks whether the contents of a metadata key matches an argument. This operator is case insensitive. type: metadata metadata: operator: equals part: 0 key: foo arg: bar","title":"equals"},{"location":"conditions/#equals_cs","text":"Checks whether the contents of a metadata key matches an argument. This operator is case sensitive. type: metadata metadata: operator: equals_cs part: 0 key: foo arg: BAR","title":"equals_cs"},{"location":"conditions/#exists","text":"Checks whether a metadata key exists. type: metadata metadata: operator: exists part: 0 key: foo","title":"exists"},{"location":"conditions/#greater_than","text":"Checks whether the contents of a metadata key, parsed as a floating point number, is greater than an argument. Returns false if the metadata value cannot be parsed into a number. type: metadata metadata: operator: greater_than part: 0 key: foo arg: 3","title":"greater_than"},{"location":"conditions/#has_prefix","text":"Checks whether the contents of a metadata key match one of the provided prefixes. The arg field can either be a singular prefix string or a list of prefixes. type: metadata metadata: operator: has_prefix part: 0 key: foo arg: - foo - bar - baz","title":"has_prefix"},{"location":"conditions/#less_than","text":"Checks whether the contents of a metadata key, parsed as a floating point number, is less than an argument. Returns false if the metadata value cannot be parsed into a number. type: metadata metadata: operator: less_than part: 0 key: foo arg: 3","title":"less_than"},{"location":"conditions/#regexp_partial","text":"Checks whether any section of the contents of a metadata key matches a regular expression (RE2 syntax). type: metadata metadata: operator: regexp_partial part: 0 key: foo arg: 1[a-z]2","title":"regexp_partial"},{"location":"conditions/#regexp_exact","text":"Checks whether the contents of a metadata key exactly matches a regular expression (RE2 syntax). type: metadata metadata: operator: regexp_partial part: 0 key: foo arg: 1[a-z]2","title":"regexp_exact"},{"location":"conditions/#not","text":"type: not not: {} Not is a condition that returns the opposite (NOT) of its child condition. The body of a not object is the child condition, i.e. in order to express 'part 0 NOT equal to \"foo\"' you could have the following YAML config: type: not not: type: text text: operator: equal part: 0 arg: foo Or, the same example as JSON: { type : not , not : { type : text , text : { operator : equal , part : 0, arg : foo } } }","title":"not"},{"location":"conditions/#or","text":"type: or or: [] Or is a condition that returns the logical OR of its children conditions.","title":"or"},{"location":"conditions/#processor_failed","text":"type: processor_failed processor_failed: part: 0 Returns true if a processing stage of a message has failed. This condition is useful for dropping failed messages or creating dead letter queues, you can read more about these patterns here .","title":"processor_failed"},{"location":"conditions/#resource","text":"type: resource resource: Resource is a condition type that runs a condition resource by its name. This condition allows you to run the same configured condition resource in multiple processors, or as a branch of another condition. For example, let's imagine we have two outputs, one of which only receives messages that satisfy a condition and the other receives the logical NOT of that same condition. In this example we can save ourselves the trouble of configuring the same condition twice by referring to it as a resource, like this: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: processors: - type: filter filter: type: resource resource: foobar - type: bar bar: processors: - type: filter filter: type: not not: type: resource resource: foobar resources: conditions: foobar: type: text text: operator: equals_cs part: 1 arg: filter me please","title":"resource"},{"location":"conditions/#static","text":"type: static static: true Static is a condition that always resolves to the same static boolean value.","title":"static"},{"location":"conditions/#text","text":"type: text text: arg: operator: equals_cs part: 0 Text is a condition that checks the contents of a message part as plain text against a logical operator and an argument. Available logical operators are:","title":"text"},{"location":"conditions/#equals_cs_1","text":"Checks whether the part equals the argument (case sensitive.)","title":"equals_cs"},{"location":"conditions/#equals_1","text":"Checks whether the part equals the argument under unicode case-folding (case insensitive.)","title":"equals"},{"location":"conditions/#contains_cs","text":"Checks whether the part contains the argument (case sensitive.)","title":"contains_cs"},{"location":"conditions/#contains","text":"Checks whether the part contains the argument under unicode case-folding (case insensitive.)","title":"contains"},{"location":"conditions/#prefix_cs","text":"Checks whether the part begins with the argument (case sensitive.)","title":"prefix_cs"},{"location":"conditions/#prefix","text":"Checks whether the part begins with the argument under unicode case-folding (case insensitive.)","title":"prefix"},{"location":"conditions/#suffix_cs","text":"Checks whether the part ends with the argument (case sensitive.)","title":"suffix_cs"},{"location":"conditions/#suffix","text":"Checks whether the part ends with the argument under unicode case-folding (case insensitive.)","title":"suffix"},{"location":"conditions/#regexp_partial_1","text":"Checks whether any section of the message part matches a regular expression (RE2 syntax).","title":"regexp_partial"},{"location":"conditions/#regexp_exact_1","text":"Checks whether the message part exactly matches a regular expression (RE2 syntax).","title":"regexp_exact"},{"location":"conditions/#xor","text":"type: xor xor: [] Xor is a condition that returns the logical XOR of its children conditions, meaning it only resolves to true if exactly one of its children conditions resolves to true.","title":"xor"},{"location":"cookbooks/","text":"Cookbooks This is a collection of Benthos snippets demonstrating common but more advanced patterns in stream processing. Duplicate Monitoring Benthos has a deduplication processor which makes removing duplicates from a stream easy. However, sometimes we might only be interested in detecting duplicates without mutating the stream. This cookbook demonstrates how to passively count duplicates and expose them via metrics aggregators. TODO: Joining streams of JSON documents TODO: Parallel enrichment of JSON documents","title":"Cookbooks"},{"location":"cookbooks/#cookbooks","text":"This is a collection of Benthos snippets demonstrating common but more advanced patterns in stream processing.","title":"Cookbooks"},{"location":"cookbooks/#duplicate-monitoring","text":"Benthos has a deduplication processor which makes removing duplicates from a stream easy. However, sometimes we might only be interested in detecting duplicates without mutating the stream. This cookbook demonstrates how to passively count duplicates and expose them via metrics aggregators. TODO: Joining streams of JSON documents TODO: Parallel enrichment of JSON documents","title":"Duplicate Monitoring"},{"location":"cookbooks/duplicate-monitoring/","text":"Duplicate Monitoring Benthos has a deduplication processor which makes removing duplicates from a stream easy. However, sometimes we might only be interested in detecting duplicates without mutating the stream. This cookbook demonstrates how to passively count duplicates and expose them via metrics aggregators. We can do this by creating a stream passthrough that increments a metric counter each time a duplicate is found in a stream using the cache and metric processors. This method is extremely flexible and therefore it's possible to detect duplicates from a combination of any number of uniquely identifying fields within the documents, or from a hash of the entire document contents. We'll cover both. By ID Field Here our chosen metrics aggregator is Prometheus, but this works with any available Benthos metrics target . We're also going to assume that the messages are JSON documents, and that we're detecting duplicates with a combination of string fields at the paths document.id and document.user.id . Here's the config (omitting our input and output sections for brevity): pipeline: processors: - type: process_batch process_batch: - type: cache cache: cache: dupes key: ${!json_field:document.id}_${!json_field:document.user.id} operator: add value: x - type: catch catch: - type: metric metric: type: counter path: duplicate_id resources: caches: dupes: type: memory memory: ttl: 300 metrics: type: prometheus For each message in a stream this pipeline begins by attempting to add a new item to a cache, where the key is an interpolated combination of our identifying fields. The cache action fails if the key already exists in the cache, and therefore only messages that are duplicates will be caught within the following catch block. Within the catch block we then increment a counter which tracks the number of duplicates found. The cache processor requires a target cache, which in this case we've labelled dupes , and the configuration for that can be found within the following resources.caches section. We've chosen a memory based cache here with a TTL of 5 minutes for simplicity, but there are many options which would allow us to share the cache across multiple instances of this pipeline. But there's more! Remember that within the metric processor it's also possible to label these counts with extra information. For example, we could label our counters with the source Kafka topic and partition: - type: metric metric: type: counter path: duplicate_id labels: topic: ${!metadata:kafka_topic} partition: ${!metadata:kafka_partition} This would let us expose duplicate levels per topic per partition in our dashboards! By Document Hash In order to detect duplicates by a hash of the entire document we can modify the previous configuration by adding a hash processor to convert documents into their hashes. We then dedupe by the new full contents of the message (which is now a hash). However, this would mutate the contents of the stream, which we need to avoid. Therefore, we wrap this action within a process_map processor and set a postmap_optional target to a path that will never be found, which prevents the hash result from being added into the original message contents. We then catch any cache errors like in the previous example. The config (omitting sections shared with the previous example for brevity) would look like this: pipeline: processors: - type: process_batch process_batch: - type: process_map process_map: processors: - type: hash hash: algorithm: xxhash64 - type: cache cache: cache: dupes key: ${!content} operator: add value: x postmap_optional: will: never.exist - type: catch catch: - type: metric metric: type: counter path: duplicate_hash","title":"Duplicate monitoring"},{"location":"cookbooks/duplicate-monitoring/#duplicate-monitoring","text":"Benthos has a deduplication processor which makes removing duplicates from a stream easy. However, sometimes we might only be interested in detecting duplicates without mutating the stream. This cookbook demonstrates how to passively count duplicates and expose them via metrics aggregators. We can do this by creating a stream passthrough that increments a metric counter each time a duplicate is found in a stream using the cache and metric processors. This method is extremely flexible and therefore it's possible to detect duplicates from a combination of any number of uniquely identifying fields within the documents, or from a hash of the entire document contents. We'll cover both.","title":"Duplicate Monitoring"},{"location":"cookbooks/duplicate-monitoring/#by-id-field","text":"Here our chosen metrics aggregator is Prometheus, but this works with any available Benthos metrics target . We're also going to assume that the messages are JSON documents, and that we're detecting duplicates with a combination of string fields at the paths document.id and document.user.id . Here's the config (omitting our input and output sections for brevity): pipeline: processors: - type: process_batch process_batch: - type: cache cache: cache: dupes key: ${!json_field:document.id}_${!json_field:document.user.id} operator: add value: x - type: catch catch: - type: metric metric: type: counter path: duplicate_id resources: caches: dupes: type: memory memory: ttl: 300 metrics: type: prometheus For each message in a stream this pipeline begins by attempting to add a new item to a cache, where the key is an interpolated combination of our identifying fields. The cache action fails if the key already exists in the cache, and therefore only messages that are duplicates will be caught within the following catch block. Within the catch block we then increment a counter which tracks the number of duplicates found. The cache processor requires a target cache, which in this case we've labelled dupes , and the configuration for that can be found within the following resources.caches section. We've chosen a memory based cache here with a TTL of 5 minutes for simplicity, but there are many options which would allow us to share the cache across multiple instances of this pipeline.","title":"By ID Field"},{"location":"cookbooks/duplicate-monitoring/#but-theres-more","text":"Remember that within the metric processor it's also possible to label these counts with extra information. For example, we could label our counters with the source Kafka topic and partition: - type: metric metric: type: counter path: duplicate_id labels: topic: ${!metadata:kafka_topic} partition: ${!metadata:kafka_partition} This would let us expose duplicate levels per topic per partition in our dashboards!","title":"But there's more!"},{"location":"cookbooks/duplicate-monitoring/#by-document-hash","text":"In order to detect duplicates by a hash of the entire document we can modify the previous configuration by adding a hash processor to convert documents into their hashes. We then dedupe by the new full contents of the message (which is now a hash). However, this would mutate the contents of the stream, which we need to avoid. Therefore, we wrap this action within a process_map processor and set a postmap_optional target to a path that will never be found, which prevents the hash result from being added into the original message contents. We then catch any cache errors like in the previous example. The config (omitting sections shared with the previous example for brevity) would look like this: pipeline: processors: - type: process_batch process_batch: - type: process_map process_map: processors: - type: hash hash: algorithm: xxhash64 - type: cache cache: cache: dupes key: ${!content} operator: add value: x postmap_optional: will: never.exist - type: catch catch: - type: metric metric: type: counter path: duplicate_hash","title":"By Document Hash"},{"location":"examples/","text":"Examples Here is a collection of Benthos configuration examples showing a range of common or interesting use cases. Kafka JSON Mutating Create a pipeline that mutates JSON documents from a Kafka stream. Kafka Delayed Retry Create a processing pipeline where if the processor fails the data is redirected to a second pipeline for retries. The second pipeline delays the next attempt based on a timestamp within the document. Deduplicated Twitter Firehose Create a pipeline that consumes a continuous HTTP stream (in this case the Twitter firehose) and writes the data to a Kafka topic. Streaming AWS S3 Archives Create a pipeline that continuously consumes .tar.gz archives from an S3 bucket, decompresses and unarchives the data, and then writes the data to a Kafka topic.","title":"Applied Examples"},{"location":"examples/#examples","text":"Here is a collection of Benthos configuration examples showing a range of common or interesting use cases.","title":"Examples"},{"location":"examples/#kafka-json-mutating","text":"Create a pipeline that mutates JSON documents from a Kafka stream.","title":"Kafka JSON Mutating"},{"location":"examples/#kafka-delayed-retry","text":"Create a processing pipeline where if the processor fails the data is redirected to a second pipeline for retries. The second pipeline delays the next attempt based on a timestamp within the document.","title":"Kafka Delayed Retry"},{"location":"examples/#deduplicated-twitter-firehose","text":"Create a pipeline that consumes a continuous HTTP stream (in this case the Twitter firehose) and writes the data to a Kafka topic.","title":"Deduplicated Twitter Firehose"},{"location":"examples/#streaming-aws-s3-archives","text":"Create a pipeline that continuously consumes .tar.gz archives from an S3 bucket, decompresses and unarchives the data, and then writes the data to a Kafka topic.","title":"Streaming AWS S3 Archives"},{"location":"examples/kafka-delayed-retry/","text":"Kafka Delayed Retry The Enrichment Stream We are consuming a Kafka stream of documents and wish to apply an enrichment to each item of the stream via an HTTP service. If the enrichment fails for a document then we wish to send the data to a different destination than successful documents such that the enrichment can be retried on a delay by a component down stream. In order to do this we are going to use the catch processor as our error recovery handling mechanism. In order to dynamically route documents we will use function interpolation to base the output topic on metadata, which we can set dynamically via our recovery mechanism. input: type: kafka_balanced kafka_balanced: addresses: - TODO topics: - source-queue consumer_group: enrichment-consumer max_batch_count: 20 pipeline: processors: - type: metadata metadata: operator: set set: output_topic value: enriched-queue - type: http http: parallel: true request: url: TODO verb: POST retries: 3 - type: catch catch: - type: metadata metadata: operator: set set: output_topic value: retry-queue output: type: kafka kafka: addresses: - TODO topic: ${!metadata:output_topic} We start our processing steps by setting all documents to have a metadata key output_topic set to enriched-queue , which is where successfully enriched documents should go. We then do an HTTP request with the http processor which performs our enrichment. In reality it would likely be more useful to wrap this step in a process_map processor but the error handling mechanism would be the same. After our enrichment the documents will either be enriched or will be flagged as having failed a processing step , which means we can perform processors specifically only on failed documents with the catch processor. We use this to set the metadata field output_topic to retry-queue only for failed documents. Finally, our output topic is a function interpolation string ${!metadata:output_topic} which resolves dynamically to the contents of the metadata key output_topic for each document. Most output types have a similar way of dynamically routing documents, otherwise you could use the switch or broker outputs to multiplex the documents. The Retry Stream We now wish to reconsume and reprocess the failed documents from the above pipeline, but only after 3600 seconds since the data was first consumed. This time period can be calculated by referring to a timestamp within the JSON document at the path meta.created_at . We can do this by combining the awk processor with the sleep processor, using awk to calculate our target sleep period: input: type: kafka_balanced kafka_balanced: addresses: - TODO topics: - retry-queue consumer_group: retry-consumer max_batch_count: 20 pipeline: processors: - type: awk awk: codec: json program: | { delay_for = 3600 - (timestamp_unix() - timestamp_unix(meta_created_at)) if ( delay_for 0 ) delay_for = 0 metadata_set( delay_for_s , delay_for) } - type: sleep sleep: duration: ${!metadata:delay_for_s}s # TODO: Reprocess output: type: TODO This works because the awk processor codec is set to json , meaning the document is parsed as a JSON object, walked, and all fields found are set as variables, allowing them to be referred to within the AWK program. The awk processor also has functions for setting metadata, which is used for writing our calculated sleep period. We do not print anything with our AWK program as we do not wish to modify the contents of the document. The sleep processor then simply halts the pipeline for a duration determined through function interpolation, allowing us to specify it via the metadata key we set. After reprocessing we can multiplex the documents that still failed the retry stage to a dead-letter queue similar to the first pipeline.","title":"Kafka delayed retry"},{"location":"examples/kafka-delayed-retry/#kafka-delayed-retry","text":"","title":"Kafka Delayed Retry"},{"location":"examples/kafka-delayed-retry/#the-enrichment-stream","text":"We are consuming a Kafka stream of documents and wish to apply an enrichment to each item of the stream via an HTTP service. If the enrichment fails for a document then we wish to send the data to a different destination than successful documents such that the enrichment can be retried on a delay by a component down stream. In order to do this we are going to use the catch processor as our error recovery handling mechanism. In order to dynamically route documents we will use function interpolation to base the output topic on metadata, which we can set dynamically via our recovery mechanism. input: type: kafka_balanced kafka_balanced: addresses: - TODO topics: - source-queue consumer_group: enrichment-consumer max_batch_count: 20 pipeline: processors: - type: metadata metadata: operator: set set: output_topic value: enriched-queue - type: http http: parallel: true request: url: TODO verb: POST retries: 3 - type: catch catch: - type: metadata metadata: operator: set set: output_topic value: retry-queue output: type: kafka kafka: addresses: - TODO topic: ${!metadata:output_topic} We start our processing steps by setting all documents to have a metadata key output_topic set to enriched-queue , which is where successfully enriched documents should go. We then do an HTTP request with the http processor which performs our enrichment. In reality it would likely be more useful to wrap this step in a process_map processor but the error handling mechanism would be the same. After our enrichment the documents will either be enriched or will be flagged as having failed a processing step , which means we can perform processors specifically only on failed documents with the catch processor. We use this to set the metadata field output_topic to retry-queue only for failed documents. Finally, our output topic is a function interpolation string ${!metadata:output_topic} which resolves dynamically to the contents of the metadata key output_topic for each document. Most output types have a similar way of dynamically routing documents, otherwise you could use the switch or broker outputs to multiplex the documents.","title":"The Enrichment Stream"},{"location":"examples/kafka-delayed-retry/#the-retry-stream","text":"We now wish to reconsume and reprocess the failed documents from the above pipeline, but only after 3600 seconds since the data was first consumed. This time period can be calculated by referring to a timestamp within the JSON document at the path meta.created_at . We can do this by combining the awk processor with the sleep processor, using awk to calculate our target sleep period: input: type: kafka_balanced kafka_balanced: addresses: - TODO topics: - retry-queue consumer_group: retry-consumer max_batch_count: 20 pipeline: processors: - type: awk awk: codec: json program: | { delay_for = 3600 - (timestamp_unix() - timestamp_unix(meta_created_at)) if ( delay_for 0 ) delay_for = 0 metadata_set( delay_for_s , delay_for) } - type: sleep sleep: duration: ${!metadata:delay_for_s}s # TODO: Reprocess output: type: TODO This works because the awk processor codec is set to json , meaning the document is parsed as a JSON object, walked, and all fields found are set as variables, allowing them to be referred to within the AWK program. The awk processor also has functions for setting metadata, which is used for writing our calculated sleep period. We do not print anything with our AWK program as we do not wish to modify the contents of the document. The sleep processor then simply halts the pipeline for a duration determined through function interpolation, allowing us to specify it via the metadata key we set. After reprocessing we can multiplex the documents that still failed the retry stage to a dead-letter queue similar to the first pipeline.","title":"The Retry Stream"},{"location":"examples/kafka-json-mutating/","text":"Kafka JSON Mutating In this example we start off with a stream of JSON data being written to a Kafka topic. Our task is to consume the stream, mutate the data into a new format, filter certain items, and write the stream to a new topic. For this example the data will be social media interactions with NLP enrichments such as sentiment and language detection, it looks something like this: { content : { text : This ist a public service announcement: I hate Jerry, Kate is okay. , created_at : 1524599100 }, language :[ { confidence : 0.07, code : de }, { confidence : 0.93, code : en } ], sentiment : [ { entity : jerry , level : negative , confidence : 0.45 }, { entity : kate , level : neutral , confidence : 0.08 } ] } And we wish to mutate the data such that irrelevant data is removed. Specifically, we want to reduce the language detection candidates down to a single language code, and we want to remove sentiment entities with a confidence below a certain level (0.3). The desired output from the above sample would be: { content : { created_at : 1524599100, text : This ist a public service announcement: I hate Jerry, Kate is okay. }, entities : [ { name : jerry , sentiment : negative } ], language : en } Which we can accomplish using the JMESPath processor . The main factor of concern in this example is throughput, it needs to be as high as possible, but we also need to preserve at-least-once delivery guarantees. The full config for this example can be found here . Since the pipeline is mostly going to be throttled by CPU the mutations execute on processing threads matching the number of logical CPUs. In order to keep the processing threads busy with traffic there are parameters in the config for tuning the number of concurrent readers to reduce input IO stalling. There is also a parameter for the message batch size, which can tuned in order to increase the throughput of the output in case it becomes the bottleneck. Input input: type: broker broker: count: 8 # Try cranking this value up if your CPUs aren't maxed out inputs: - type: kafka_balanced kafka_balanced: addresses: - localhost:9092 # TODO client_id: benthos_mutator_1 consumer_group: benthos_mutator_group topics: - data_stream max_batch_count: 8 # Batch size: Tune this to increase output throughput The kafka_balanced input is used here, which automatically distributes the partitions of a topic across all consumers of the same consumer group. This allows us to have as many consumers both inside the process and as separate services as we need. It should be noted that the number of Kafka partitions for this topic should be significantly higher than the total number of consumers in order to get a good distribution of messages. Using a broker allows us to tune the number of parallel consumers inside the process in order to reach our maximum CPU utilisation. We also specify a max_batch_count , which is the maximum number of messages to batch together when we have a backlog of prefetched messages. The advantage of this is that a batch will be sent as a single request on the Kafka producer output. By tuning the batch size we should be able to increase our output throughput to prevent it from bottlenecking the pipeline. Pipeline pipeline: threads: 4 # This should match the number of logical CPUs processors: - type: jmespath jmespath: query: | { content: content, entities: sentiment[?confidence `0.3`].{ name: entity, sentiment: level }, language: max_by(language, confidence).code } The pipeline is where we construct our processing pipelines. We set the number of threads to match our available logical CPUs in order to avoid contention. You might find slightly increasing or decreasing this number can improve throughput due to competing services on the machine. The number of input consumers needs to be significantly higher than the number of threads here in order to prevent IO stalling. The only processor defined here is our JMESPath query, which is where our JSON mutation comes from. You can read more about JMESPath here . Output Our output is a standard Kafka producer output.","title":"Kafka json mutating"},{"location":"examples/kafka-json-mutating/#kafka-json-mutating","text":"In this example we start off with a stream of JSON data being written to a Kafka topic. Our task is to consume the stream, mutate the data into a new format, filter certain items, and write the stream to a new topic. For this example the data will be social media interactions with NLP enrichments such as sentiment and language detection, it looks something like this: { content : { text : This ist a public service announcement: I hate Jerry, Kate is okay. , created_at : 1524599100 }, language :[ { confidence : 0.07, code : de }, { confidence : 0.93, code : en } ], sentiment : [ { entity : jerry , level : negative , confidence : 0.45 }, { entity : kate , level : neutral , confidence : 0.08 } ] } And we wish to mutate the data such that irrelevant data is removed. Specifically, we want to reduce the language detection candidates down to a single language code, and we want to remove sentiment entities with a confidence below a certain level (0.3). The desired output from the above sample would be: { content : { created_at : 1524599100, text : This ist a public service announcement: I hate Jerry, Kate is okay. }, entities : [ { name : jerry , sentiment : negative } ], language : en } Which we can accomplish using the JMESPath processor . The main factor of concern in this example is throughput, it needs to be as high as possible, but we also need to preserve at-least-once delivery guarantees. The full config for this example can be found here . Since the pipeline is mostly going to be throttled by CPU the mutations execute on processing threads matching the number of logical CPUs. In order to keep the processing threads busy with traffic there are parameters in the config for tuning the number of concurrent readers to reduce input IO stalling. There is also a parameter for the message batch size, which can tuned in order to increase the throughput of the output in case it becomes the bottleneck.","title":"Kafka JSON Mutating"},{"location":"examples/kafka-json-mutating/#input","text":"input: type: broker broker: count: 8 # Try cranking this value up if your CPUs aren't maxed out inputs: - type: kafka_balanced kafka_balanced: addresses: - localhost:9092 # TODO client_id: benthos_mutator_1 consumer_group: benthos_mutator_group topics: - data_stream max_batch_count: 8 # Batch size: Tune this to increase output throughput The kafka_balanced input is used here, which automatically distributes the partitions of a topic across all consumers of the same consumer group. This allows us to have as many consumers both inside the process and as separate services as we need. It should be noted that the number of Kafka partitions for this topic should be significantly higher than the total number of consumers in order to get a good distribution of messages. Using a broker allows us to tune the number of parallel consumers inside the process in order to reach our maximum CPU utilisation. We also specify a max_batch_count , which is the maximum number of messages to batch together when we have a backlog of prefetched messages. The advantage of this is that a batch will be sent as a single request on the Kafka producer output. By tuning the batch size we should be able to increase our output throughput to prevent it from bottlenecking the pipeline.","title":"Input"},{"location":"examples/kafka-json-mutating/#pipeline","text":"pipeline: threads: 4 # This should match the number of logical CPUs processors: - type: jmespath jmespath: query: | { content: content, entities: sentiment[?confidence `0.3`].{ name: entity, sentiment: level }, language: max_by(language, confidence).code } The pipeline is where we construct our processing pipelines. We set the number of threads to match our available logical CPUs in order to avoid contention. You might find slightly increasing or decreasing this number can improve throughput due to competing services on the machine. The number of input consumers needs to be significantly higher than the number of threads here in order to prevent IO stalling. The only processor defined here is our JMESPath query, which is where our JSON mutation comes from. You can read more about JMESPath here .","title":"Pipeline"},{"location":"examples/kafka-json-mutating/#output","text":"Our output is a standard Kafka producer output.","title":"Output"},{"location":"examples/streaming-aws-s3-archives/","text":"Streaming AWS S3 Archives This example demonstrates how Benthos can be used to stream an S3 bucket of .tar.gz archives containing JSON documents into any output target. This example is able to listen for newly added archives and then downloads, decompresses, unarchives and streams the JSON documents found within to a Kafka topic. The Kafka output in this example can be replaced with any Benthos output target . The method used to stream archives is via an SQS queue , which is a common pattern. Benthos can work either with S3 events sent via SQS directly, or by S3 events broadcast via SNS to SQS, there is a small adjustment to the config which is explained in the input section. The full config for this example can be found here . Input input: type: s3 s3: region: eu-west-1 # TODO bucket: TODO delete_objects: false sqs_url: TODO sqs_body_path: Records.s3.object.key sqs_envelope_path: sqs_max_messages: 10 credentials: id: TODO secret: TODO token: TODO role: TODO This input section contains lots of fields to be completed which are self explanatory, such as bucket , sqs_url and the credentials section. The sqs_body_path field is the JSON path within an SQS message that contains the name of new S3 files, which should be left as Records.s3.object.key unless you have built a custom solution. If SNS is being used to broadcast S3 events instead of connecting SQS directly you will need to fill in the sqs_envelope_path , which is the JSON path inside an SNS message that contains the enveloped S3 event. The value of sqs_envelope_path should be Message when using the standard AWS set up. This example uses a single consumer, but if the throughput isn't high enough to keep up with the bucket it is possible to use a broker type to have multiple parallel consumers: input: type: broker broker: copies: 8 # Increase this to gain more parallel consumers inputs: - type: s3 s3: ... etc You can have any number of consumers of a bucket and messages (archives) will automatically be distributed amongst them via the SQS queue. Pipeline pipeline: threads: 4 # Try to match the number of available logical CPU cores processors: - type: decompress decompress: algorithm: gzip - type: unarchive unarchive: format: tar - type: split - type: batch batch: count: 10 # The size of message batches to send to Kafka The processors in this example start off with a simple decompress and unarchive of the payload. This results in a single payload of multiple documents. The split processor turns this payload into individual messages. The final processor is optional. It is a batch stage that bundles the individual messages back into batches to be sent to the Kafka topic, increasing the throughput. If the batch processor is used it should be a factor of the number of messages inside the S3 archives. The size also needs to be low enough so that the overall size of the batch doesn't exceed the maximum bytes of a Kafka request. These processors are heavy on CPU, which is why they are configured inside the pipeline section. This allows you to explicitly set the number of parallel threads to exactly match the number of logical CPU cores available. Output The output config is a standard Kafka output.","title":"Streaming aws s3 archives"},{"location":"examples/streaming-aws-s3-archives/#streaming-aws-s3-archives","text":"This example demonstrates how Benthos can be used to stream an S3 bucket of .tar.gz archives containing JSON documents into any output target. This example is able to listen for newly added archives and then downloads, decompresses, unarchives and streams the JSON documents found within to a Kafka topic. The Kafka output in this example can be replaced with any Benthos output target . The method used to stream archives is via an SQS queue , which is a common pattern. Benthos can work either with S3 events sent via SQS directly, or by S3 events broadcast via SNS to SQS, there is a small adjustment to the config which is explained in the input section. The full config for this example can be found here .","title":"Streaming AWS S3 Archives"},{"location":"examples/streaming-aws-s3-archives/#input","text":"input: type: s3 s3: region: eu-west-1 # TODO bucket: TODO delete_objects: false sqs_url: TODO sqs_body_path: Records.s3.object.key sqs_envelope_path: sqs_max_messages: 10 credentials: id: TODO secret: TODO token: TODO role: TODO This input section contains lots of fields to be completed which are self explanatory, such as bucket , sqs_url and the credentials section. The sqs_body_path field is the JSON path within an SQS message that contains the name of new S3 files, which should be left as Records.s3.object.key unless you have built a custom solution. If SNS is being used to broadcast S3 events instead of connecting SQS directly you will need to fill in the sqs_envelope_path , which is the JSON path inside an SNS message that contains the enveloped S3 event. The value of sqs_envelope_path should be Message when using the standard AWS set up. This example uses a single consumer, but if the throughput isn't high enough to keep up with the bucket it is possible to use a broker type to have multiple parallel consumers: input: type: broker broker: copies: 8 # Increase this to gain more parallel consumers inputs: - type: s3 s3: ... etc You can have any number of consumers of a bucket and messages (archives) will automatically be distributed amongst them via the SQS queue.","title":"Input"},{"location":"examples/streaming-aws-s3-archives/#pipeline","text":"pipeline: threads: 4 # Try to match the number of available logical CPU cores processors: - type: decompress decompress: algorithm: gzip - type: unarchive unarchive: format: tar - type: split - type: batch batch: count: 10 # The size of message batches to send to Kafka The processors in this example start off with a simple decompress and unarchive of the payload. This results in a single payload of multiple documents. The split processor turns this payload into individual messages. The final processor is optional. It is a batch stage that bundles the individual messages back into batches to be sent to the Kafka topic, increasing the throughput. If the batch processor is used it should be a factor of the number of messages inside the S3 archives. The size also needs to be low enough so that the overall size of the batch doesn't exceed the maximum bytes of a Kafka request. These processors are heavy on CPU, which is why they are configured inside the pipeline section. This allows you to explicitly set the number of parallel threads to exactly match the number of logical CPU cores available.","title":"Pipeline"},{"location":"examples/streaming-aws-s3-archives/#output","text":"The output config is a standard Kafka output.","title":"Output"},{"location":"examples/twitter-firehose/","text":"Twitter Firehose This example demonstrates how Benthos can be used to stream the Twitter firehose into a Kafka topic. The output section could be changed to target any of the supported output types . This example includes deduplication, which means multiple instances can be run for redundancy without swamping the data sink with duplicates. Deduplication is performed via a shared Memcached cluster. As of the time of writing this example there are three streaming APIs for Twitter: PowerTrack, Firehose and Replay. All three provide an HTTP stream connection, where tweets are delivered as line-delimited ( \\r\\n ) JSON blobs. Occasionally the stream will deliver blank lines in order to keep the connection alive. The stream is never ending, and therefore if the connection closes it should be reopened. The example provided could be used to consume any of the stream types. The full config for this example can be found here . Input The input of this example is fairly standard. We initiate an HTTP stream which is automatically recovered if a disconnection occurs. The only processor attached to the input is a bounds_check filter that removes any empty lines. input: type: http_client http_client: url: https://gnip-stream.twitter.com/stream/firehose/accounts/foo/publishers/twitter/prod.json?partition=1 verb: GET content_type: application/json basic_auth: enabled: true password: # TODO username: # TODO stream: enabled: true max_buffer: 10_000_000 # 10MB - The max supported length of a single line processors: - type: bounds_check # Filter out keep alives (empty message) bounds_check: min_part_size: 2 It's worth noting that you can add the backfillMinutes URL parameter if you have the feature enabled. This means any connection recovery will always gain a small window of automatic backfill. Buffer buffer: type: memory memory: limit: 500_000_000 We add a memory based buffer in this config which will help us keep up with the stream during sudden traffic spikes. It also allows us to parallelise the next layer of deduplication processors. Pipeline pipeline: threads: 16 # Determines the max number of concurrent calls to dedupe cache processors: - type: filter # Filter out non-json objects and error messages filter: type: jmespath jmespath: query: keys(@) | length(@) `0` !contains(@, 'error') - type: dedupe dedupe: cache: dedupe drop_on_err: false # Prefer occasional duplicates over lost messages key: ${!json_field:id_str} # Dedupe based on 'id_str' field of tweets hash: none The pipeline section contains two processors. The first processor is a JMESPath query which checks whether the message object is an invalid JSON object or system error message from Twitter. We chose to remove these messages since client disconnects are handled automatically and it's possible to observe the reasons for a disconnection from the API dashboard. The second processor is a deduplication step which checks the id_str field of the tweet against a shared Memcached cluster (the cache details are configured later on in the resources section). This is likely to be the bottleneck of the system (mostly idle on network IO), therefore the threads field should be tweaked in order to tune the optimum number of concurrent Memcached requests. Output The output section is a standard Kafka connection. output: type: kafka kafka: addresses: - localhost:9092 # TODO client_id: benthos_firehose_bridge topic: twitter_firehose max_msg_bytes: 10_000_000 # 10MB - The max supported message size This can be changed to any other output type without impacting the rest of the pipeline. Resources resources: caches: dedupe: type: memcached memcached: addresses: - localhost:11211 # TODO ttl: 604_800 # Keep Twitter IDs cached for a week The resources section contains the configuration of our deduplication cache. We are using Memcached which allows us share the dedupe cache across multiple redundant Benthos instances. If you aren't using redundant instances or wish to deduplicate elsewhere then you can simply remove this section as well as the dedupe processor in the pipeline section, this should also improve throughput.","title":"Twitter firehose"},{"location":"examples/twitter-firehose/#twitter-firehose","text":"This example demonstrates how Benthos can be used to stream the Twitter firehose into a Kafka topic. The output section could be changed to target any of the supported output types . This example includes deduplication, which means multiple instances can be run for redundancy without swamping the data sink with duplicates. Deduplication is performed via a shared Memcached cluster. As of the time of writing this example there are three streaming APIs for Twitter: PowerTrack, Firehose and Replay. All three provide an HTTP stream connection, where tweets are delivered as line-delimited ( \\r\\n ) JSON blobs. Occasionally the stream will deliver blank lines in order to keep the connection alive. The stream is never ending, and therefore if the connection closes it should be reopened. The example provided could be used to consume any of the stream types. The full config for this example can be found here .","title":"Twitter Firehose"},{"location":"examples/twitter-firehose/#input","text":"The input of this example is fairly standard. We initiate an HTTP stream which is automatically recovered if a disconnection occurs. The only processor attached to the input is a bounds_check filter that removes any empty lines. input: type: http_client http_client: url: https://gnip-stream.twitter.com/stream/firehose/accounts/foo/publishers/twitter/prod.json?partition=1 verb: GET content_type: application/json basic_auth: enabled: true password: # TODO username: # TODO stream: enabled: true max_buffer: 10_000_000 # 10MB - The max supported length of a single line processors: - type: bounds_check # Filter out keep alives (empty message) bounds_check: min_part_size: 2 It's worth noting that you can add the backfillMinutes URL parameter if you have the feature enabled. This means any connection recovery will always gain a small window of automatic backfill.","title":"Input"},{"location":"examples/twitter-firehose/#buffer","text":"buffer: type: memory memory: limit: 500_000_000 We add a memory based buffer in this config which will help us keep up with the stream during sudden traffic spikes. It also allows us to parallelise the next layer of deduplication processors.","title":"Buffer"},{"location":"examples/twitter-firehose/#pipeline","text":"pipeline: threads: 16 # Determines the max number of concurrent calls to dedupe cache processors: - type: filter # Filter out non-json objects and error messages filter: type: jmespath jmespath: query: keys(@) | length(@) `0` !contains(@, 'error') - type: dedupe dedupe: cache: dedupe drop_on_err: false # Prefer occasional duplicates over lost messages key: ${!json_field:id_str} # Dedupe based on 'id_str' field of tweets hash: none The pipeline section contains two processors. The first processor is a JMESPath query which checks whether the message object is an invalid JSON object or system error message from Twitter. We chose to remove these messages since client disconnects are handled automatically and it's possible to observe the reasons for a disconnection from the API dashboard. The second processor is a deduplication step which checks the id_str field of the tweet against a shared Memcached cluster (the cache details are configured later on in the resources section). This is likely to be the bottleneck of the system (mostly idle on network IO), therefore the threads field should be tweaked in order to tune the optimum number of concurrent Memcached requests.","title":"Pipeline"},{"location":"examples/twitter-firehose/#output","text":"The output section is a standard Kafka connection. output: type: kafka kafka: addresses: - localhost:9092 # TODO client_id: benthos_firehose_bridge topic: twitter_firehose max_msg_bytes: 10_000_000 # 10MB - The max supported message size This can be changed to any other output type without impacting the rest of the pipeline.","title":"Output"},{"location":"examples/twitter-firehose/#resources","text":"resources: caches: dedupe: type: memcached memcached: addresses: - localhost:11211 # TODO ttl: 604_800 # Keep Twitter IDs cached for a week The resources section contains the configuration of our deduplication cache. We are using Memcached which allows us share the dedupe cache across multiple redundant Benthos instances. If you aren't using redundant instances or wish to deduplicate elsewhere then you can simply remove this section as well as the dedupe processor in the pipeline section, this should also improve throughput.","title":"Resources"},{"location":"examples/webhook_with_fallback/","text":"Webhook With Fallback It's possible to create fallback options for output types by using a try broker . In this guide we'll explore this pattern by creating stream bridges that accept messages via an HTTP server endpoint and forwards the messages to another HTTP endpoint, and if it fails to do so will print the message to stdout as a warning. In practice our input might be something like a RabbitMQ exchange, and our fallback output might be a queue used for scheduling action by a third party. We will also be using Benthos in --streams mode in this example so that we can create these bridges dynamically via REST endpoints . The first thing to do is to run Benthos: benthos --streams And we can set up and start our stream under the id foo by sending it to the /streams/foo endpoint. For this example we will use curl, but this would normally be done via another service. curl http://localhost:4195/streams/foo --data-binary @- EOF input: type: http_server output: type: broker broker: pattern: try outputs: - type: http_client http_client: url: http://localhost/post verb: POST backoff_on: [ 429 ] drop_on: [ 409 ] retries: 3 - type: stdout processors: - type: insert_part insert_part: content: MESSAGE FAILED: index: 0 EOF It's worth noting that we have added a processor on the fallback output which adds prefix content to the logged message. We could apply any processors here to make arbitrary changes to the payload. We can check whether the stream is ready and active with: curl http://localhost:4195/streams/foo | jq '.active' And send some content with: curl http://localhost:4195/foo/post -d hello world Benthos will now attempt to send the payload to the configured http_client endpoint. If the endpoint isn't reachable then after a few seconds you should see something like this on stdout : MESSAGE FAILED: hello world Try playing around with inputs, fallback outputs and fallback processors.","title":"Webhook with fallback"},{"location":"examples/webhook_with_fallback/#webhook-with-fallback","text":"It's possible to create fallback options for output types by using a try broker . In this guide we'll explore this pattern by creating stream bridges that accept messages via an HTTP server endpoint and forwards the messages to another HTTP endpoint, and if it fails to do so will print the message to stdout as a warning. In practice our input might be something like a RabbitMQ exchange, and our fallback output might be a queue used for scheduling action by a third party. We will also be using Benthos in --streams mode in this example so that we can create these bridges dynamically via REST endpoints . The first thing to do is to run Benthos: benthos --streams And we can set up and start our stream under the id foo by sending it to the /streams/foo endpoint. For this example we will use curl, but this would normally be done via another service. curl http://localhost:4195/streams/foo --data-binary @- EOF input: type: http_server output: type: broker broker: pattern: try outputs: - type: http_client http_client: url: http://localhost/post verb: POST backoff_on: [ 429 ] drop_on: [ 409 ] retries: 3 - type: stdout processors: - type: insert_part insert_part: content: MESSAGE FAILED: index: 0 EOF It's worth noting that we have added a processor on the fallback output which adds prefix content to the logged message. We could apply any processors here to make arbitrary changes to the payload. We can check whether the stream is ready and active with: curl http://localhost:4195/streams/foo | jq '.active' And send some content with: curl http://localhost:4195/foo/post -d hello world Benthos will now attempt to send the payload to the configured http_client endpoint. If the endpoint isn't reachable then after a few seconds you should see something like this on stdout : MESSAGE FAILED: hello world Try playing around with inputs, fallback outputs and fallback processors.","title":"Webhook With Fallback"},{"location":"inputs/","text":"Inputs This document was generated with benthos --list-inputs An input is a source of data piped through an array of optional processors . Only one input is configured at the root of a Benthos config. However, the root input can be a broker which combines multiple inputs. An input config section looks like this: input: type: foo foo: bar: baz processors: - type: qux Contents amqp broker dynamic file files gcp_pubsub hdfs http_client http_server inproc kafka kafka_balanced kinesis mqtt nanomsg nats nats_stream nsq read_until redis_list redis_pubsub redis_streams s3 sqs stdin websocket amqp type: amqp amqp: bindings_declare: [] consumer_tag: benthos-consumer max_batch_count: 1 prefetch_count: 10 prefetch_size: 0 queue: benthos-queue queue_declare: durable: true enabled: false tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: amqp://guest:guest@localhost:5672/ Connects to an AMQP (0.91) queue. AMQP is a messaging protocol used by various message brokers, including RabbitMQ. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. It's possible for this input type to declare the target queue by setting queue_declare.enabled to true , if the queue already exists then the declaration passively verifies that they match the target fields. Similarly, it is possible to declare queue bindings by adding objects to the bindings_declare array. Binding declare objects take the form of: { exchange : benthos-exchange , key : benthos-key } TLS is automatic when connecting to an amqps URL, but custom settings can be enabled in the tls section. Metadata This input adds the following metadata fields to each message: - amqp_content_type - amqp_content_encoding - amqp_delivery_mode - amqp_priority - amqp_correlation_id - amqp_reply_to - amqp_expiration - amqp_message_id - amqp_timestamp - amqp_type - amqp_user_id - amqp_app_id - amqp_consumer_tag - amqp_delivery_tag - amqp_redelivered - amqp_exchange - amqp_routing_key - All existing message headers, including nested headers prefixed with the key of their respective parent. You can access these metadata fields using function interpolation . broker type: broker broker: copies: 1 inputs: [] The broker type allows you to combine multiple inputs, where each input will be read in parallel. A broker type is configured with its own list of input configurations and a field to specify how many copies of the list of inputs should be created. Adding more input types allows you to merge streams from multiple sources into one. For example, reading from both RabbitMQ and Kafka: type: broker broker: copies: 1 inputs: - type: amqp amqp: url: amqp://guest:guest@localhost:5672/ consumer_tag: benthos-consumer exchange: benthos-exchange exchange_type: direct key: benthos-key queue: benthos-queue - type: kafka kafka: addresses: - localhost:9092 client_id: benthos_kafka_input consumer_group: benthos_consumer_group partition: 0 topic: benthos_stream If the number of copies is greater than zero the list will be copied that number of times. For example, if your inputs were of type foo and bar, with 'copies' set to '2', you would end up with two 'foo' inputs and two 'bar' inputs. Processors It is possible to configure processors at the broker level, where they will be applied to all child inputs, as well as on the individual child inputs. If you have processors at both the broker level and on child inputs then the broker processors will be applied after the child nodes processors. dynamic type: dynamic dynamic: inputs: {} prefix: timeout: 5s The dynamic type is a special broker type where the inputs are identified by unique labels and can be created, changed and removed during runtime via a REST HTTP interface. To GET a JSON map of input identifiers with their current uptimes use the /inputs endpoint. To perform CRUD actions on the inputs themselves use POST, DELETE, and GET methods on the /inputs/{input_id} endpoint. When using POST the body of the request should be a JSON configuration for the input, if the input already exists it will be changed. file type: file file: delimiter: max_buffer: 1e+06 multipart: false path: The file type reads input from a file. If multipart is set to false each line is read as a separate message. If multipart is set to true each line is read as a message part, and an empty line indicates the end of a message. If the delimiter field is left empty then line feed (\\n) is used. files type: files files: path: Reads files from a path, where each discrete file will be consumed as a single message payload. The path can either point to a single file (resulting in only a single message) or a directory, in which case the directory will be walked and each file found will become a message. Metadata This input adds the following metadata fields to each message: - path You can access these metadata fields using function interpolation . gcp_pubsub type: gcp_pubsub gcp_pubsub: max_outstanding_bytes: 1e+09 max_outstanding_messages: 1000 project: subscription: Consumes messages from a GCP Cloud Pub/Sub subscription. Attributes from each message are added as metadata, which can be accessed using function interpolation . hdfs type: hdfs hdfs: directory: hosts: - localhost:9000 user: benthos_hdfs Reads files from a HDFS directory, where each discrete file will be consumed as a single message payload. Metadata This input adds the following metadata fields to each message: - hdfs_name - hdfs_path You can access these metadata fields using function interpolation . http_client type: http_client http_client: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: payload: rate_limit: retries: 3 retry_period: 1s stream: delimiter: enabled: false max_buffer: 1e+06 multipart: false reconnect: true timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/get verb: GET The HTTP client input type connects to a server and continuously performs requests for a single message. You should set a sensible retry period and max backoff so as to not flood your target server. The URL and header values of this type can be dynamically set using function interpolations described here . Streaming If you enable streaming then Benthos will consume the body of the response as a line delimited list of message parts. Each part is read as an individual message unless multipart is set to true, in which case an empty line indicates the end of a message. http_server type: http_server http_server: address: cert_file: key_file: path: /post timeout: 5s ws_path: /post/ws Receive messages POSTed over HTTP(S). HTTP 2.0 is supported when using TLS, which is enabled when key and cert files are specified. You can leave the 'address' config field blank in order to use the instance wide HTTP server. Metadata This input adds the following metadata fields to each message: - http_server_user_agent - All headers (only first values are taken) - All cookies You can access these metadata fields using function interpolation . inproc type: inproc inproc: Directly connect to an output within a Benthos process by referencing it by a chosen ID. This allows you to hook up isolated streams whilst running Benthos in --streams mode mode, it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow. It is possible to connect multiple inputs to the same inproc ID, but only one output can connect to an inproc ID, and will replace existing outputs if a collision occurs. kafka type: kafka kafka: addresses: - localhost:9092 client_id: benthos_kafka_input commit_period: 1s consumer_group: benthos_consumer_group max_batch_count: 1 max_processing_period: 100ms partition: 0 start_from_oldest: true target_version: 1.0.0 tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topic: benthos_stream Connects to a kafka (0.8+) server. Offsets are managed within kafka as per the consumer group (set via config). Only one partition per input is supported, if you wish to balance partitions across a consumer group look at the kafka_balanced input type instead. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. The field max_processing_period should be set above the maximum estimated time taken to process a message. The target version by default will be the oldest supported, as it is expected that the server will be backwards compatible. In order to support newer client features you should increase this version up to the known version of the target server. TLS Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar Metadata This input adds the following metadata fields to each message: - kafka_key - kafka_topic - kafka_partition - kafka_offset - kafka_timestamp_unix - All existing message headers (version 0.11+) You can access these metadata fields using function interpolation . kafka_balanced type: kafka_balanced kafka_balanced: addresses: - localhost:9092 client_id: benthos_kafka_input commit_period: 1s consumer_group: benthos_consumer_group group: heartbeat_interval: 3s rebalance_timeout: 60s session_timeout: 10s max_batch_count: 1 max_processing_period: 100ms start_from_oldest: true target_version: 1.0.0 tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topics: - benthos_stream Connects to a kafka (0.9+) server. Offsets are managed within kafka as per the consumer group (set via config), and partitions are automatically balanced across any members of the consumer group. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. The field max_processing_period should be set above the maximum estimated time taken to process a message. TLS Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar Metadata This input adds the following metadata fields to each message: - kafka_key - kafka_topic - kafka_partition - kafka_offset - kafka_timestamp_unix - All existing message headers (version 0.11+) You can access these metadata fields using function interpolation . kinesis type: kinesis kinesis: client_id: benthos_consumer commit_period: 1s credentials: id: role: role_external_id: secret: token: dynamodb_table: endpoint: limit: 100 region: eu-west-1 shard: 0 start_from_oldest: true stream: timeout: 5s Receive messages from a Kinesis stream. It's possible to use DynamoDB for persisting shard iterators by setting the table name. Offsets will then be tracked per client_id per shard_id . When using this mode you should create a table with namespace as the primary key and shard_id as a sort key. mqtt type: mqtt mqtt: client_id: benthos_input qos: 1 topics: - benthos_topic urls: - tcp://localhost:1883 Subscribe to topics on MQTT brokers. Metadata This input adds the following metadata fields to each message: - mqtt_duplicate - mqtt_qos - mqtt_retained - mqtt_topic - mqtt_message_id You can access these metadata fields using function interpolation . nanomsg type: nanomsg nanomsg: bind: true poll_timeout: 5s reply_timeout: 5s socket_type: PULL sub_filters: [] urls: - tcp://*:5555 The scalability protocols are common communication patterns. This input should be compatible with any implementation, but specifically targets Nanomsg. Currently only PULL and SUB sockets are supported. nats type: nats nats: prefetch_count: 32 queue: benthos_queue subject: benthos_messages urls: - nats://localhost:4222 Subscribe to a NATS subject. NATS is at-most-once, if you need at-least-once behaviour then look at NATS Stream. The urls can contain username/password semantics. e.g. nats://derek:pass@localhost:4222 Metadata This input adds the following metadata fields to each message: - nats_subject You can access these metadata fields using function interpolation . nats_stream type: nats_stream nats_stream: client_id: benthos_client cluster_id: test-cluster durable_name: benthos_offset max_inflight: 1024 queue: benthos_queue start_from_oldest: true subject: benthos_messages unsubscribe_on_close: true urls: - nats://localhost:4222 Subscribe to a NATS Stream subject, which is at-least-once. Joining a queue is optional and allows multiple clients of a subject to consume using queue semantics. Tracking and persisting offsets through a durable name is also optional and works with or without a queue. If a durable name is not provided then subjects are consumed from the most recently published message. When a consumer closes its connection it unsubscribes, when all consumers of a durable queue do this the offsets are deleted. In order to avoid this you can stop the consumers from unsubscribing by setting the field unsubscribe_on_close to false . Metadata This input adds the following metadata fields to each message: - nats_stream_subject - nats_stream_sequence You can access these metadata fields using function interpolation . nsq type: nsq nsq: channel: benthos_stream lookupd_http_addresses: - localhost:4161 max_in_flight: 100 nsqd_tcp_addresses: - localhost:4150 topic: benthos_messages user_agent: benthos_consumer Subscribe to an NSQ instance topic and channel. read_until type: read_until read_until: condition: type: text text: arg: operator: equals_cs part: 0 input: {} restart_input: false Reads from an input and tests a condition on each message. Messages are read continuously while the condition returns false, when the condition returns true the message that triggered the condition is sent out and the input is closed. Use this type to define inputs where the stream should end once a certain message appears. Sometimes inputs close themselves. For example, when the file input type reaches the end of a file it will shut down. By default this type will also shut down. If you wish for the input type to be restarted every time it shuts down until the condition is met then set restart_input to true . Metadata A metadata key benthos_read_until containing the value final is added to the first part of the message that triggers to input to stop. redis_list type: redis_list redis_list: key: benthos_list timeout: 5s url: tcp://localhost:6379 Pops messages from the beginning of a Redis list using the BLPop command. redis_pubsub type: redis_pubsub redis_pubsub: channels: - benthos_chan url: tcp://localhost:6379 Redis supports a publish/subscribe model, it's possible to subscribe to multiple channels using this input. redis_streams type: redis_streams redis_streams: body_key: body client_id: benthos_consumer commit_period: 1s consumer_group: benthos_group limit: 10 start_from_oldest: true streams: - benthos_stream timeout: 5s url: tcp://localhost:6379 Pulls messages from Redis (v5.0+) streams with the XREADGROUP command. The client_id should be unique for each consumer of a group. The field limit specifies the maximum number of records to be received per request. When more than one record is returned they are batched and can be split into individual messages with the split processor. Redis stream entries are key/value pairs, as such it is necessary to specify the key that contains the body of the message. All other keys/value pairs are saved as metadata fields. s3 type: s3 s3: bucket: credentials: id: role: role_external_id: secret: token: delete_objects: false download_manager: enabled: true endpoint: max_batch_count: 1 prefix: region: eu-west-1 retries: 3 sqs_body_path: Records.s3.object.key sqs_envelope_path: sqs_max_messages: 10 sqs_url: timeout: 5s Downloads objects in an Amazon S3 bucket, optionally filtered by a prefix. If an SQS queue has been configured then only object keys read from the queue will be downloaded. Otherwise, the entire list of objects found when this input is created will be downloaded. Note that the prefix configuration is only used when downloading objects without SQS configured. If the download manager is enabled this can help speed up file downloads but results in file metadata not being copied. If your bucket is configured to send events directly to an SQS queue then you need to set the sqs_body_path field to where the object key is found in the payload. However, it is also common practice to send bucket events to an SNS topic which sends enveloped events to SQS, in which case you must also set the sqs_envelope_path field to where the payload can be found. Here is a guide for setting up an SQS queue that receives events for new S3 bucket objects: https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html WARNING: When using SQS please make sure you have sensible values for sqs_max_messages and also the visibility timeout of the queue itself. When Benthos consumes an S3 item as a result of receiving an SQS message the message is not deleted until the S3 item has been sent onwards. This ensures at-least-once crash resiliency, but also means that if the S3 item takes longer to process than the visibility timeout of your queue then the same items might be processed multiple times. Metadata This input adds the following metadata fields to each message: - s3_key - s3_bucket - s3_last_modified_unix* - s3_last_modified (RFC3339)* - s3_content_type* - All user defined metadata* * Only added when NOT using download manager You can access these metadata fields using function interpolation . sqs type: sqs sqs: credentials: id: role: role_external_id: secret: token: endpoint: max_number_of_messages: 1 region: eu-west-1 timeout: 5s url: Receive messages from an Amazon SQS URL, only the body is extracted into messages. stdin type: stdin stdin: delimiter: max_buffer: 1e+06 multipart: false The stdin input simply reads any data piped to stdin as messages. By default the messages are assumed single part and are line delimited. If the multipart option is set to true then lines are interpretted as message parts, and an empty line indicates the end of the message. If the delimiter field is left empty then line feed (\\n) is used. websocket type: websocket websocket: basic_auth: enabled: false password: username: oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: open_message: url: ws://localhost:4195/get/ws Connects to a websocket server and continuously receives messages. It is possible to configure an open_message , which when set to a non-empty string will be sent to the websocket server each time a connection is first established.","title":"Inputs"},{"location":"inputs/#inputs","text":"This document was generated with benthos --list-inputs An input is a source of data piped through an array of optional processors . Only one input is configured at the root of a Benthos config. However, the root input can be a broker which combines multiple inputs. An input config section looks like this: input: type: foo foo: bar: baz processors: - type: qux","title":"Inputs"},{"location":"inputs/#contents","text":"amqp broker dynamic file files gcp_pubsub hdfs http_client http_server inproc kafka kafka_balanced kinesis mqtt nanomsg nats nats_stream nsq read_until redis_list redis_pubsub redis_streams s3 sqs stdin websocket","title":"Contents"},{"location":"inputs/#amqp","text":"type: amqp amqp: bindings_declare: [] consumer_tag: benthos-consumer max_batch_count: 1 prefetch_count: 10 prefetch_size: 0 queue: benthos-queue queue_declare: durable: true enabled: false tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: amqp://guest:guest@localhost:5672/ Connects to an AMQP (0.91) queue. AMQP is a messaging protocol used by various message brokers, including RabbitMQ. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. It's possible for this input type to declare the target queue by setting queue_declare.enabled to true , if the queue already exists then the declaration passively verifies that they match the target fields. Similarly, it is possible to declare queue bindings by adding objects to the bindings_declare array. Binding declare objects take the form of: { exchange : benthos-exchange , key : benthos-key } TLS is automatic when connecting to an amqps URL, but custom settings can be enabled in the tls section.","title":"amqp"},{"location":"inputs/#metadata","text":"This input adds the following metadata fields to each message: - amqp_content_type - amqp_content_encoding - amqp_delivery_mode - amqp_priority - amqp_correlation_id - amqp_reply_to - amqp_expiration - amqp_message_id - amqp_timestamp - amqp_type - amqp_user_id - amqp_app_id - amqp_consumer_tag - amqp_delivery_tag - amqp_redelivered - amqp_exchange - amqp_routing_key - All existing message headers, including nested headers prefixed with the key of their respective parent. You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#broker","text":"type: broker broker: copies: 1 inputs: [] The broker type allows you to combine multiple inputs, where each input will be read in parallel. A broker type is configured with its own list of input configurations and a field to specify how many copies of the list of inputs should be created. Adding more input types allows you to merge streams from multiple sources into one. For example, reading from both RabbitMQ and Kafka: type: broker broker: copies: 1 inputs: - type: amqp amqp: url: amqp://guest:guest@localhost:5672/ consumer_tag: benthos-consumer exchange: benthos-exchange exchange_type: direct key: benthos-key queue: benthos-queue - type: kafka kafka: addresses: - localhost:9092 client_id: benthos_kafka_input consumer_group: benthos_consumer_group partition: 0 topic: benthos_stream If the number of copies is greater than zero the list will be copied that number of times. For example, if your inputs were of type foo and bar, with 'copies' set to '2', you would end up with two 'foo' inputs and two 'bar' inputs.","title":"broker"},{"location":"inputs/#processors","text":"It is possible to configure processors at the broker level, where they will be applied to all child inputs, as well as on the individual child inputs. If you have processors at both the broker level and on child inputs then the broker processors will be applied after the child nodes processors.","title":"Processors"},{"location":"inputs/#dynamic","text":"type: dynamic dynamic: inputs: {} prefix: timeout: 5s The dynamic type is a special broker type where the inputs are identified by unique labels and can be created, changed and removed during runtime via a REST HTTP interface. To GET a JSON map of input identifiers with their current uptimes use the /inputs endpoint. To perform CRUD actions on the inputs themselves use POST, DELETE, and GET methods on the /inputs/{input_id} endpoint. When using POST the body of the request should be a JSON configuration for the input, if the input already exists it will be changed.","title":"dynamic"},{"location":"inputs/#file","text":"type: file file: delimiter: max_buffer: 1e+06 multipart: false path: The file type reads input from a file. If multipart is set to false each line is read as a separate message. If multipart is set to true each line is read as a message part, and an empty line indicates the end of a message. If the delimiter field is left empty then line feed (\\n) is used.","title":"file"},{"location":"inputs/#files","text":"type: files files: path: Reads files from a path, where each discrete file will be consumed as a single message payload. The path can either point to a single file (resulting in only a single message) or a directory, in which case the directory will be walked and each file found will become a message.","title":"files"},{"location":"inputs/#metadata_1","text":"This input adds the following metadata fields to each message: - path You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#gcp_pubsub","text":"type: gcp_pubsub gcp_pubsub: max_outstanding_bytes: 1e+09 max_outstanding_messages: 1000 project: subscription: Consumes messages from a GCP Cloud Pub/Sub subscription. Attributes from each message are added as metadata, which can be accessed using function interpolation .","title":"gcp_pubsub"},{"location":"inputs/#hdfs","text":"type: hdfs hdfs: directory: hosts: - localhost:9000 user: benthos_hdfs Reads files from a HDFS directory, where each discrete file will be consumed as a single message payload.","title":"hdfs"},{"location":"inputs/#metadata_2","text":"This input adds the following metadata fields to each message: - hdfs_name - hdfs_path You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#http_client","text":"type: http_client http_client: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: payload: rate_limit: retries: 3 retry_period: 1s stream: delimiter: enabled: false max_buffer: 1e+06 multipart: false reconnect: true timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/get verb: GET The HTTP client input type connects to a server and continuously performs requests for a single message. You should set a sensible retry period and max backoff so as to not flood your target server. The URL and header values of this type can be dynamically set using function interpolations described here .","title":"http_client"},{"location":"inputs/#streaming","text":"If you enable streaming then Benthos will consume the body of the response as a line delimited list of message parts. Each part is read as an individual message unless multipart is set to true, in which case an empty line indicates the end of a message.","title":"Streaming"},{"location":"inputs/#http_server","text":"type: http_server http_server: address: cert_file: key_file: path: /post timeout: 5s ws_path: /post/ws Receive messages POSTed over HTTP(S). HTTP 2.0 is supported when using TLS, which is enabled when key and cert files are specified. You can leave the 'address' config field blank in order to use the instance wide HTTP server.","title":"http_server"},{"location":"inputs/#metadata_3","text":"This input adds the following metadata fields to each message: - http_server_user_agent - All headers (only first values are taken) - All cookies You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#inproc","text":"type: inproc inproc: Directly connect to an output within a Benthos process by referencing it by a chosen ID. This allows you to hook up isolated streams whilst running Benthos in --streams mode mode, it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow. It is possible to connect multiple inputs to the same inproc ID, but only one output can connect to an inproc ID, and will replace existing outputs if a collision occurs.","title":"inproc"},{"location":"inputs/#kafka","text":"type: kafka kafka: addresses: - localhost:9092 client_id: benthos_kafka_input commit_period: 1s consumer_group: benthos_consumer_group max_batch_count: 1 max_processing_period: 100ms partition: 0 start_from_oldest: true target_version: 1.0.0 tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topic: benthos_stream Connects to a kafka (0.8+) server. Offsets are managed within kafka as per the consumer group (set via config). Only one partition per input is supported, if you wish to balance partitions across a consumer group look at the kafka_balanced input type instead. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. The field max_processing_period should be set above the maximum estimated time taken to process a message. The target version by default will be the oldest supported, as it is expected that the server will be backwards compatible. In order to support newer client features you should increase this version up to the known version of the target server.","title":"kafka"},{"location":"inputs/#tls","text":"Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar","title":"TLS"},{"location":"inputs/#metadata_4","text":"This input adds the following metadata fields to each message: - kafka_key - kafka_topic - kafka_partition - kafka_offset - kafka_timestamp_unix - All existing message headers (version 0.11+) You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#kafka_balanced","text":"type: kafka_balanced kafka_balanced: addresses: - localhost:9092 client_id: benthos_kafka_input commit_period: 1s consumer_group: benthos_consumer_group group: heartbeat_interval: 3s rebalance_timeout: 60s session_timeout: 10s max_batch_count: 1 max_processing_period: 100ms start_from_oldest: true target_version: 1.0.0 tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topics: - benthos_stream Connects to a kafka (0.9+) server. Offsets are managed within kafka as per the consumer group (set via config), and partitions are automatically balanced across any members of the consumer group. The field max_batch_count specifies the maximum number of prefetched messages to be batched together. When more than one message is batched they can be split into individual messages with the split processor. The field max_processing_period should be set above the maximum estimated time taken to process a message.","title":"kafka_balanced"},{"location":"inputs/#tls_1","text":"Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar","title":"TLS"},{"location":"inputs/#metadata_5","text":"This input adds the following metadata fields to each message: - kafka_key - kafka_topic - kafka_partition - kafka_offset - kafka_timestamp_unix - All existing message headers (version 0.11+) You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#kinesis","text":"type: kinesis kinesis: client_id: benthos_consumer commit_period: 1s credentials: id: role: role_external_id: secret: token: dynamodb_table: endpoint: limit: 100 region: eu-west-1 shard: 0 start_from_oldest: true stream: timeout: 5s Receive messages from a Kinesis stream. It's possible to use DynamoDB for persisting shard iterators by setting the table name. Offsets will then be tracked per client_id per shard_id . When using this mode you should create a table with namespace as the primary key and shard_id as a sort key.","title":"kinesis"},{"location":"inputs/#mqtt","text":"type: mqtt mqtt: client_id: benthos_input qos: 1 topics: - benthos_topic urls: - tcp://localhost:1883 Subscribe to topics on MQTT brokers.","title":"mqtt"},{"location":"inputs/#metadata_6","text":"This input adds the following metadata fields to each message: - mqtt_duplicate - mqtt_qos - mqtt_retained - mqtt_topic - mqtt_message_id You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#nanomsg","text":"type: nanomsg nanomsg: bind: true poll_timeout: 5s reply_timeout: 5s socket_type: PULL sub_filters: [] urls: - tcp://*:5555 The scalability protocols are common communication patterns. This input should be compatible with any implementation, but specifically targets Nanomsg. Currently only PULL and SUB sockets are supported.","title":"nanomsg"},{"location":"inputs/#nats","text":"type: nats nats: prefetch_count: 32 queue: benthos_queue subject: benthos_messages urls: - nats://localhost:4222 Subscribe to a NATS subject. NATS is at-most-once, if you need at-least-once behaviour then look at NATS Stream. The urls can contain username/password semantics. e.g. nats://derek:pass@localhost:4222","title":"nats"},{"location":"inputs/#metadata_7","text":"This input adds the following metadata fields to each message: - nats_subject You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#nats_stream","text":"type: nats_stream nats_stream: client_id: benthos_client cluster_id: test-cluster durable_name: benthos_offset max_inflight: 1024 queue: benthos_queue start_from_oldest: true subject: benthos_messages unsubscribe_on_close: true urls: - nats://localhost:4222 Subscribe to a NATS Stream subject, which is at-least-once. Joining a queue is optional and allows multiple clients of a subject to consume using queue semantics. Tracking and persisting offsets through a durable name is also optional and works with or without a queue. If a durable name is not provided then subjects are consumed from the most recently published message. When a consumer closes its connection it unsubscribes, when all consumers of a durable queue do this the offsets are deleted. In order to avoid this you can stop the consumers from unsubscribing by setting the field unsubscribe_on_close to false .","title":"nats_stream"},{"location":"inputs/#metadata_8","text":"This input adds the following metadata fields to each message: - nats_stream_subject - nats_stream_sequence You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#nsq","text":"type: nsq nsq: channel: benthos_stream lookupd_http_addresses: - localhost:4161 max_in_flight: 100 nsqd_tcp_addresses: - localhost:4150 topic: benthos_messages user_agent: benthos_consumer Subscribe to an NSQ instance topic and channel.","title":"nsq"},{"location":"inputs/#read_until","text":"type: read_until read_until: condition: type: text text: arg: operator: equals_cs part: 0 input: {} restart_input: false Reads from an input and tests a condition on each message. Messages are read continuously while the condition returns false, when the condition returns true the message that triggered the condition is sent out and the input is closed. Use this type to define inputs where the stream should end once a certain message appears. Sometimes inputs close themselves. For example, when the file input type reaches the end of a file it will shut down. By default this type will also shut down. If you wish for the input type to be restarted every time it shuts down until the condition is met then set restart_input to true .","title":"read_until"},{"location":"inputs/#metadata_9","text":"A metadata key benthos_read_until containing the value final is added to the first part of the message that triggers to input to stop.","title":"Metadata"},{"location":"inputs/#redis_list","text":"type: redis_list redis_list: key: benthos_list timeout: 5s url: tcp://localhost:6379 Pops messages from the beginning of a Redis list using the BLPop command.","title":"redis_list"},{"location":"inputs/#redis_pubsub","text":"type: redis_pubsub redis_pubsub: channels: - benthos_chan url: tcp://localhost:6379 Redis supports a publish/subscribe model, it's possible to subscribe to multiple channels using this input.","title":"redis_pubsub"},{"location":"inputs/#redis_streams","text":"type: redis_streams redis_streams: body_key: body client_id: benthos_consumer commit_period: 1s consumer_group: benthos_group limit: 10 start_from_oldest: true streams: - benthos_stream timeout: 5s url: tcp://localhost:6379 Pulls messages from Redis (v5.0+) streams with the XREADGROUP command. The client_id should be unique for each consumer of a group. The field limit specifies the maximum number of records to be received per request. When more than one record is returned they are batched and can be split into individual messages with the split processor. Redis stream entries are key/value pairs, as such it is necessary to specify the key that contains the body of the message. All other keys/value pairs are saved as metadata fields.","title":"redis_streams"},{"location":"inputs/#s3","text":"type: s3 s3: bucket: credentials: id: role: role_external_id: secret: token: delete_objects: false download_manager: enabled: true endpoint: max_batch_count: 1 prefix: region: eu-west-1 retries: 3 sqs_body_path: Records.s3.object.key sqs_envelope_path: sqs_max_messages: 10 sqs_url: timeout: 5s Downloads objects in an Amazon S3 bucket, optionally filtered by a prefix. If an SQS queue has been configured then only object keys read from the queue will be downloaded. Otherwise, the entire list of objects found when this input is created will be downloaded. Note that the prefix configuration is only used when downloading objects without SQS configured. If the download manager is enabled this can help speed up file downloads but results in file metadata not being copied. If your bucket is configured to send events directly to an SQS queue then you need to set the sqs_body_path field to where the object key is found in the payload. However, it is also common practice to send bucket events to an SNS topic which sends enveloped events to SQS, in which case you must also set the sqs_envelope_path field to where the payload can be found. Here is a guide for setting up an SQS queue that receives events for new S3 bucket objects: https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html WARNING: When using SQS please make sure you have sensible values for sqs_max_messages and also the visibility timeout of the queue itself. When Benthos consumes an S3 item as a result of receiving an SQS message the message is not deleted until the S3 item has been sent onwards. This ensures at-least-once crash resiliency, but also means that if the S3 item takes longer to process than the visibility timeout of your queue then the same items might be processed multiple times.","title":"s3"},{"location":"inputs/#metadata_10","text":"This input adds the following metadata fields to each message: - s3_key - s3_bucket - s3_last_modified_unix* - s3_last_modified (RFC3339)* - s3_content_type* - All user defined metadata* * Only added when NOT using download manager You can access these metadata fields using function interpolation .","title":"Metadata"},{"location":"inputs/#sqs","text":"type: sqs sqs: credentials: id: role: role_external_id: secret: token: endpoint: max_number_of_messages: 1 region: eu-west-1 timeout: 5s url: Receive messages from an Amazon SQS URL, only the body is extracted into messages.","title":"sqs"},{"location":"inputs/#stdin","text":"type: stdin stdin: delimiter: max_buffer: 1e+06 multipart: false The stdin input simply reads any data piped to stdin as messages. By default the messages are assumed single part and are line delimited. If the multipart option is set to true then lines are interpretted as message parts, and an empty line indicates the end of the message. If the delimiter field is left empty then line feed (\\n) is used.","title":"stdin"},{"location":"inputs/#websocket","text":"type: websocket websocket: basic_auth: enabled: false password: username: oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: open_message: url: ws://localhost:4195/get/ws Connects to a websocket server and continuously receives messages. It is possible to configure an open_message , which when set to a non-empty string will be sent to the websocket server each time a connection is first established.","title":"websocket"},{"location":"metrics/","text":"Metric Target Types This document was generated with benthos --list-metrics A metrics type represents a destination for Benthos metrics to be aggregated such as Statsd, Prometheus, or for debugging purposes an HTTP endpoint that exposes a JSON object of metrics. A metrics config section looks like this: metrics: type: foo prefix: benthos foo: bar: baz Benthos exposes lots of metrics and their paths will depend on your pipeline configuration. However, there are some critical metrics that will always be present that are outlined in this document . blacklist type: blacklist blacklist: child: {} paths: [] patterns: [] prefix: benthos Blacklist metric paths within Benthos so that they are not aggregated by a child metric target. Blacklists can either be path prefixes or regular expression patterns, if either a path prefix or regular expression matches a metric path it will be excluded. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level. Paths An entry in the paths field will check using prefix matching. This can be used, for example, to allow none of the child specific metrics paths from an output broker with the path output.broker . Patterns An entry in the patterns field will be parsed as an RE2 regular expression and tested against each metric path. This can be used, for example, to allow none of the latency based metrics with the pattern .*\\.latency . http_server type: http_server http_server: {} prefix: benthos It is possible to expose metrics without an aggregator service by having Benthos serve them as a JSON object at the endpoints /stats and /metrics . This is useful for quickly debugging a pipeline. The object takes the form of a hierarchical representation of the dot paths for each metric combined. So, for example, if Benthos exposed two metric counters foo.bar and bar.baz then the resulting object might look like this: { foo : { bar : 9 }, bar : { baz : 3 } } prometheus type: prometheus prefix: benthos prometheus: push_interval: push_job_name: benthos_push push_url: Host endpoints ( /metrics and /stats ) for Prometheus scraping. Metrics paths will differ from the list in that dot separators will instead be underscores. Push Gateway The field push_url is optional and when set will trigger a push of metrics to a Prometheus Push Gateway once Benthos shuts down. It is also possible to specify a push_interval which results in periodic pushes. The Push Gateway This is useful for when Benthos instances are short lived. Do not include the \"/metrics/jobs/...\" path in the push URL. rename type: rename prefix: benthos rename: by_regexp: [] child: {} Rename metric paths as they are registered. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level. by_regexp An array of objects of the form {\"pattern\":\"foo\",\"value\":\"bar\"} where each pattern will be parsed as RE2 regular expressions, these expressions are tested against each metric path, where all occurrences will be replaced with the specified value. Inside the value $ signs are interpreted as submatch expansions, e.g. $1 represents the first submatch. To replace the paths 'foo.bar.zap' and 'foo.baz.zap' with 'zip.bar' and 'zip.baz' respectively we could use this config: type: rename rename: by_regexp: - pattern: foo\\\\.([a-z]*)\\\\.zap value: zip.$1 statsd type: statsd prefix: benthos statsd: address: localhost:4040 flush_period: 100ms network: udp Push metrics over a TCP or UDP connection using the StatsD protocol . whitelist type: whitelist prefix: benthos whitelist: child: {} paths: [] patterns: [] Whitelist metric paths within Benthos so that only matching metric paths are aggregated by a child metric target. Whitelists can either be path prefixes or regular expression patterns, if either a path prefix or regular expression matches a metric path it will be included. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level. Paths An entry in the paths field will check using prefix matching. This can be used, for example, to allow the child specific metrics paths from an output broker with the path output.broker . Patterns An entry in the patterns field will be parsed as an RE2 regular expression and tested against each metric path. This can be used, for example, to allow all latency based metrics with the pattern .*\\.latency .","title":"Metrics"},{"location":"metrics/#metric-target-types","text":"This document was generated with benthos --list-metrics A metrics type represents a destination for Benthos metrics to be aggregated such as Statsd, Prometheus, or for debugging purposes an HTTP endpoint that exposes a JSON object of metrics. A metrics config section looks like this: metrics: type: foo prefix: benthos foo: bar: baz Benthos exposes lots of metrics and their paths will depend on your pipeline configuration. However, there are some critical metrics that will always be present that are outlined in this document .","title":"Metric Target Types"},{"location":"metrics/#blacklist","text":"type: blacklist blacklist: child: {} paths: [] patterns: [] prefix: benthos Blacklist metric paths within Benthos so that they are not aggregated by a child metric target. Blacklists can either be path prefixes or regular expression patterns, if either a path prefix or regular expression matches a metric path it will be excluded. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level.","title":"blacklist"},{"location":"metrics/#paths","text":"An entry in the paths field will check using prefix matching. This can be used, for example, to allow none of the child specific metrics paths from an output broker with the path output.broker .","title":"Paths"},{"location":"metrics/#patterns","text":"An entry in the patterns field will be parsed as an RE2 regular expression and tested against each metric path. This can be used, for example, to allow none of the latency based metrics with the pattern .*\\.latency .","title":"Patterns"},{"location":"metrics/#http_server","text":"type: http_server http_server: {} prefix: benthos It is possible to expose metrics without an aggregator service by having Benthos serve them as a JSON object at the endpoints /stats and /metrics . This is useful for quickly debugging a pipeline. The object takes the form of a hierarchical representation of the dot paths for each metric combined. So, for example, if Benthos exposed two metric counters foo.bar and bar.baz then the resulting object might look like this: { foo : { bar : 9 }, bar : { baz : 3 } }","title":"http_server"},{"location":"metrics/#prometheus","text":"type: prometheus prefix: benthos prometheus: push_interval: push_job_name: benthos_push push_url: Host endpoints ( /metrics and /stats ) for Prometheus scraping. Metrics paths will differ from the list in that dot separators will instead be underscores.","title":"prometheus"},{"location":"metrics/#push-gateway","text":"The field push_url is optional and when set will trigger a push of metrics to a Prometheus Push Gateway once Benthos shuts down. It is also possible to specify a push_interval which results in periodic pushes. The Push Gateway This is useful for when Benthos instances are short lived. Do not include the \"/metrics/jobs/...\" path in the push URL.","title":"Push Gateway"},{"location":"metrics/#rename","text":"type: rename prefix: benthos rename: by_regexp: [] child: {} Rename metric paths as they are registered. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level.","title":"rename"},{"location":"metrics/#by_regexp","text":"An array of objects of the form {\"pattern\":\"foo\",\"value\":\"bar\"} where each pattern will be parsed as RE2 regular expressions, these expressions are tested against each metric path, where all occurrences will be replaced with the specified value. Inside the value $ signs are interpreted as submatch expansions, e.g. $1 represents the first submatch. To replace the paths 'foo.bar.zap' and 'foo.baz.zap' with 'zip.bar' and 'zip.baz' respectively we could use this config: type: rename rename: by_regexp: - pattern: foo\\\\.([a-z]*)\\\\.zap value: zip.$1","title":"by_regexp"},{"location":"metrics/#statsd","text":"type: statsd prefix: benthos statsd: address: localhost:4040 flush_period: 100ms network: udp Push metrics over a TCP or UDP connection using the StatsD protocol .","title":"statsd"},{"location":"metrics/#whitelist","text":"type: whitelist prefix: benthos whitelist: child: {} paths: [] patterns: [] Whitelist metric paths within Benthos so that only matching metric paths are aggregated by a child metric target. Whitelists can either be path prefixes or regular expression patterns, if either a path prefix or regular expression matches a metric path it will be included. Metrics must be matched using dot notation even if the chosen output uses a different form. For example, the path would be 'foo.bar' rather than 'foo_bar' even when sending metrics to Prometheus. The prefix field in a metrics config is ignored by this type. Please configure a prefix at the child level.","title":"whitelist"},{"location":"metrics/#paths_1","text":"An entry in the paths field will check using prefix matching. This can be used, for example, to allow the child specific metrics paths from an output broker with the path output.broker .","title":"Paths"},{"location":"metrics/#patterns_1","text":"An entry in the patterns field will be parsed as an RE2 regular expression and tested against each metric path. This can be used, for example, to allow all latency based metrics with the pattern .*\\.latency .","title":"Patterns"},{"location":"metrics/paths/","text":"Metric Paths This document lists some of the most useful metrics exposed by Benthos, there are lots of more granular metrics available that may not appear here which will depend on your pipeline configuration. Paths are listed here in dot notation, which is how they will appear if aggregated by Statsd. Other metrics destinations such as Prometheus will display these metrics with other notations (underscores instead of dots.) Input input.count : The number of times the input has attempted to read messages. input.received : The number of messages received by the input. input.batch.received : The number of message batches received by the input. input.connection.up input.connection.failed input.connection.lost input.latency : Measures the roundtrip latency from the point at which a message is read up to the moment the message has either been acknowledged by an output or has been stored within an external buffer. Buffer buffer.backlog : The (sometimes estimated) size of the buffer backlog in bytes. buffer.write.count buffer.write.error buffer.read.count buffer.read.error buffer.latency : Measures the roundtrip latency from the point at which a message is read from the buffer up to the moment it has been acknowledged by the output. Processors Processor metrics are prefixed by the area of the Benthos pipeline they reside in and their index. For example, processors in the pipeline section will be prefixed with pipeline.processor.N , where N is the index. pipeline.processor.0.count pipeline.processor.0.sent pipeline.processor.0.batch.sent pipeline.processor.0.error Conditions condition.count condition.true condition.false Output output.count : The number of times the output has attempted to send messages. output.sent : The number of messages sent. output.batch.sent : The number of message batches sent. output.connection.up output.connection.failed output.connection.lost","title":"Paths"},{"location":"metrics/paths/#metric-paths","text":"This document lists some of the most useful metrics exposed by Benthos, there are lots of more granular metrics available that may not appear here which will depend on your pipeline configuration. Paths are listed here in dot notation, which is how they will appear if aggregated by Statsd. Other metrics destinations such as Prometheus will display these metrics with other notations (underscores instead of dots.)","title":"Metric Paths"},{"location":"metrics/paths/#input","text":"input.count : The number of times the input has attempted to read messages. input.received : The number of messages received by the input. input.batch.received : The number of message batches received by the input. input.connection.up input.connection.failed input.connection.lost input.latency : Measures the roundtrip latency from the point at which a message is read up to the moment the message has either been acknowledged by an output or has been stored within an external buffer.","title":"Input"},{"location":"metrics/paths/#buffer","text":"buffer.backlog : The (sometimes estimated) size of the buffer backlog in bytes. buffer.write.count buffer.write.error buffer.read.count buffer.read.error buffer.latency : Measures the roundtrip latency from the point at which a message is read from the buffer up to the moment it has been acknowledged by the output.","title":"Buffer"},{"location":"metrics/paths/#processors","text":"Processor metrics are prefixed by the area of the Benthos pipeline they reside in and their index. For example, processors in the pipeline section will be prefixed with pipeline.processor.N , where N is the index. pipeline.processor.0.count pipeline.processor.0.sent pipeline.processor.0.batch.sent pipeline.processor.0.error","title":"Processors"},{"location":"metrics/paths/#conditions","text":"condition.count condition.true condition.false","title":"Conditions"},{"location":"metrics/paths/#output","text":"output.count : The number of times the output has attempted to send messages. output.sent : The number of messages sent. output.batch.sent : The number of message batches sent. output.connection.up output.connection.failed output.connection.lost","title":"Output"},{"location":"outputs/","text":"Outputs This document was generated with benthos --list-outputs An output is a sink where we wish to send our consumed data after applying an optional array of processors . Only one output is configured at the root of a Benthos config. However, the output can be a broker which combines multiple outputs under a chosen brokering pattern. An output config section looks like this: output: type: foo foo: bar: baz processors: - type: qux Back Pressure Benthos outputs apply back pressure to components upstream. This means if your output target starts blocking traffic Benthos will gracefully stop consuming until the issue is resolved. Retries When a Benthos output fails to send a message the error is propagated back up to the input, where depending on the protocol it will either be pushed back to the source as a Noack (AMQP) or will be reattempted indefinitely with the commit witheld until success (Kafka). It's possible to instead have Benthos indefinitely retry an output until success with a retry output. Some other outputs, such as the broker , might also retry indefinitely depending on their configuration. Multiplexing Outputs It is possible to perform content based multiplexing of messages to specific outputs either by using the switch output, or a broker with the fan_out pattern and a filter processor on each output, which is a processor that drops messages if the condition does not pass. Conditions are content aware logical operators that can be combined using boolean logic. For more information regarding conditions, including a full list of available conditions please read the docs here . Dead Letter Queues It's possible to create fallback outputs for when an output target fails using a broker output with the 'try' pattern. Contents amqp broker cache dynamic dynamodb elasticsearch file files gcp_pubsub hdfs http_client http_server inproc kafka kinesis mqtt nanomsg nats nats_stream nsq redis_list redis_pubsub redis_streams retry s3 sqs stdout switch websocket amqp type: amqp amqp: exchange: benthos-exchange exchange_declare: durable: true enabled: false type: direct immediate: false key: benthos-key mandatory: false persistent: false tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: amqp://guest:guest@localhost:5672/ Sends messages to an AMQP (0.91) exchange. AMQP is a messaging protocol used by various message brokers, including RabbitMQ. The metadata from each message are delivered as headers. It's possible for this output type to create the target exchange by setting exchange_declare.enabled to true , if the exchange already exists then the declaration passively verifies that the settings match. Exchange type options are: direct|fanout|topic|x-custom TLS is automatic when connecting to an amqps URL, but custom settings can be enabled in the tls section. The field 'key' can be dynamically set using function interpolations described here . broker type: broker broker: copies: 1 outputs: [] pattern: fan_out The broker output type allows you to configure multiple output targets by listing them: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: foo_field_1: value1 - type: bar bar: bar_field_1: value2 bar_field_2: value3 - type: baz baz: baz_field_1: value4 processors: - type: baz_processor processors: - type: some_processor The broker pattern determines the way in which messages are allocated to outputs and can be chosen from the following: fan_out With the fan out pattern all outputs will be sent every message that passes through Benthos. If an output applies back pressure it will block all subsequent messages, and if an output fails to send a message it will be retried continuously until completion or service shut down. round_robin With the round robin pattern each message will be assigned a single output following their order. If an output applies back pressure it will block all subsequent messages. If an output fails to send a message then the message will be re-attempted with the next input, and so on. greedy The greedy pattern results in higher output throughput at the cost of potentially disproportionate message allocations to those outputs. Each message is sent to a single output, which is determined by allowing outputs to claim messages as soon as they are able to process them. This results in certain faster outputs potentially processing more messages at the cost of slower outputs. try The try pattern attempts to send each message to only one output, starting from the first output on the list. If an output attempt fails then the broker attempts to send to the next output in the list and so on. This pattern is useful for triggering events in the case where certain output targets have broken. For example, if you had an output type http_client but wished to reroute messages whenever the endpoint becomes unreachable you could use a try broker. Utilising More Outputs When using brokered outputs with patterns such as round robin or greedy it is possible to have multiple messages in-flight at the same time. In order to fully utilise this you either need to have a greater number of input sources than output sources or use a buffer . Processors It is possible to configure processors at the broker level, where they will be applied to all child outputs, as well as on the individual child outputs. If you have processors at both the broker level and on child outputs then the broker processors will be applied before the child nodes processors. cache type: cache cache: key: ${!count:items}-${!timestamp_unix_nano} target: Stores message parts as items in a cache. Caches are configured within the resources section and can target any of the following types: dynamodb memcached memory redis Like follows: output: type: cache cache: target: foo key: ${!json_field:document.id} resources: caches: foo: type: memcached memcached: addresses: - localhost:11211 ttl: 60 In order to create a unique key value per item you should use function interpolations described here . When sending batched messages the interpolations are performed per message part. dynamic type: dynamic dynamic: outputs: {} prefix: timeout: 5s The dynamic type is a special broker type where the outputs are identified by unique labels and can be created, changed and removed during runtime via a REST HTTP interface. The broker pattern used is always fan_out , meaning each message will be delivered to each dynamic output. To GET a JSON map of output identifiers with their current uptimes use the '/outputs' endpoint. To perform CRUD actions on the outputs themselves use POST, DELETE, and GET methods on the /outputs/{output_id} endpoint. When using POST the body of the request should be a JSON configuration for the output, if the output already exists it will be changed. dynamodb type: dynamodb dynamodb: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: max_retries: 3 region: eu-west-1 string_columns: {} table: ttl: ttl_key: Inserts messages into a DynamoDB table. Columns are populated by writing a map of key/value pairs, where the values are function interpolated strings calculated per message of a batch. This allows you to populate columns by extracting fields within the document payload or metadata like follows: type: dynamodb dynamodb: table: foo string_columns: id: ${!json_field:id} title: ${!json_field:body.title} topic: ${!metadata:kafka_topic} full_content: ${!content} elasticsearch type: elasticsearch elasticsearch: aws: credentials: id: role: role_external_id: secret: token: enabled: false endpoint: region: eu-west-1 backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s basic_auth: enabled: false password: username: id: ${!count:elastic_ids}-${!timestamp_unix} index: benthos_index max_retries: 0 pipeline: sniff: true timeout: 5s type: doc urls: - http://localhost:9200 Publishes messages into an Elasticsearch index. This output currently does not support creating the target index. Both the id and index fields can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part. file type: file file: delimiter: path: The file output type simply appends all messages to an output file. Single part messages are printed with a delimiter (defaults to '\\n' if left empty). Multipart messages are written with each part delimited, with the final part followed by two delimiters, e.g. a multipart message [ \"foo\", \"bar\", \"baz\" ] would be written as: foo\\n bar\\n baz\\n\\n files type: files files: path: ${!count:files}-${!timestamp_unix_nano}.txt Writes each individual part of each message to a new file. Message parts only contain raw data, and therefore in order to create a unique file for each part you need to generate unique file names. This can be done by using function interpolations on the path field as described here . When sending batched messages these interpolations are performed per message part. gcp_pubsub type: gcp_pubsub gcp_pubsub: project: topic: Sends messages to a GCP Cloud Pub/Sub topic. Metadata from messages are sent as attributes. hdfs type: hdfs hdfs: directory: hosts: - localhost:9000 path: ${!count:files}-${!timestamp_unix_nano}.txt user: benthos_hdfs Sends message parts as files to a HDFS directory. Each file is written with the path specified with the 'path' field, in order to have a different path for each object you should use function interpolations described here . When sending batched messages the interpolations are performed per message part. http_client type: http_client http_client: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: rate_limit: retries: 3 retry_period: 1s timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/post verb: POST Sends messages to an HTTP server. The request will be retried for each message whenever the response code is outside the range of 200 - 299 inclusive. It is possible to list codes outside of this range in the drop_on field in order to prevent retry attempts. The period of time between retries is linear by default. Response codes that are within the backoff_on list will instead apply exponential backoff between retry attempts. When the number of retries expires the output will reject the message, the behaviour after this will depend on the pipeline but usually this simply means the send is attempted again until successful whilst applying back pressure. The URL and header values of this type can be dynamically set using function interpolations described here . The body of the HTTP request is the raw contents of the message payload. If the message has multiple parts the request will be sent according to RFC1341 http_server type: http_server http_server: address: cert_file: key_file: path: /get stream_path: /get/stream timeout: 5s ws_path: /get/ws Sets up an HTTP server that will send messages over HTTP(S) GET requests. HTTP 2.0 is supported when using TLS, which is enabled when key and cert files are specified. You can leave the 'address' config field blank in order to use the default service, but this will ignore TLS options. You can receive a single, discrete message on the configured 'path' endpoint, or receive a constant stream of line delimited messages on the configured 'stream_path' endpoint. inproc type: inproc inproc: Sends data directly to Benthos inputs by connecting to a unique ID. This allows you to hook up isolated streams whilst running Benthos in --streams mode mode, it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow. It is possible to connect multiple inputs to the same inproc ID, but only one output can connect to an inproc ID, and will replace existing outputs if a collision occurs. kafka type: kafka kafka: ack_replicas: false addresses: - localhost:9092 client_id: benthos_kafka_output compression: none key: max_msg_bytes: 1e+06 round_robin_partitions: false target_version: 1.0.0 timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topic: benthos_stream The kafka output type writes messages to a kafka broker, these messages are acknowledged, which is propagated back to the input. The config field ack_replicas determines whether we wait for acknowledgement from all replicas or just a single broker. It is possible to specify a compression codec to use out of the following options: none, snappy, lz4 and gzip. If the field key is not empty then each message will be given its contents as a key. Both the key and topic fields can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part. By default the paritioner will select partitions based on a hash of the key value. If the key is empty then a partition is chosen at random. You can alternatively force the partitioner to round-robin partitions with the field round_robin_partitions . TLS Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar kinesis type: kinesis kinesis: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: hash_key: max_retries: 0 partition_key: region: eu-west-1 stream: Sends messages to a Kinesis stream. Both the partition_key (required) and hash_key (optional) fields can be dynamically set using function interpolations described here . When sending batched messages the interpolations are performed per message part. mqtt type: mqtt mqtt: client_id: benthos_output qos: 1 topic: benthos_topic urls: - tcp://localhost:1883 Pushes messages to an MQTT broker. nanomsg type: nanomsg nanomsg: bind: false poll_timeout: 5s socket_type: PUSH urls: - tcp://localhost:5556 The scalability protocols are common communication patterns. This output should be compatible with any implementation, but specifically targets Nanomsg. Currently only PUSH and PUB sockets are supported. nats type: nats nats: subject: benthos_messages urls: - nats://localhost:4222 Publish to an NATS subject. NATS is at-most-once, so delivery is not guaranteed. For at-least-once behaviour with NATS look at NATS Stream. This output will interpolate functions within the subject field, you can find a list of functions here . nats_stream type: nats_stream nats_stream: client_id: benthos_client cluster_id: test-cluster subject: benthos_messages urls: - nats://localhost:4222 Publish to a NATS Stream subject. nsq type: nsq nsq: nsqd_tcp_address: localhost:4150 topic: benthos_messages user_agent: benthos_producer Publish to an NSQ topic. The topic field can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part. redis_list type: redis_list redis_list: key: benthos_list url: tcp://localhost:6379 Pushes messages onto the end of a Redis list (which is created if it doesn't already exist) using the RPUSH command. redis_pubsub type: redis_pubsub redis_pubsub: channel: benthos_chan url: tcp://localhost:6379 Publishes messages through the Redis PubSub model. It is not possible to guarantee that messages have been received. This output will interpolate functions within the channel field, you can find a list of functions here . redis_streams type: redis_streams redis_streams: body_key: body max_length: 0 stream: benthos_stream url: tcp://localhost:6379 Pushes messages to a Redis (v5.0+) Stream (which is created if it doesn't already exist) using the XADD command. It's possible to specify a maximum length of the target stream by setting it to a value greater than 0, in which case this cap is applied only when Redis is able to remove a whole macro node, for efficiency. Redis stream entries are key/value pairs, as such it is necessary to specify the key to be set to the body of the message. All metadata fields of the message will also be set as key/value pairs, if there is a key collision between a metadata item and the body then the body takes precedence. retry type: retry retry: backoff: initial_interval: 500ms max_elapsed_time: 0s max_interval: 3s max_retries: 0 output: {} Attempts to write messages to a child output and if the write fails for any reason the message is retried either until success or, if the retries or max elapsed time fields are non-zero, either is reached. All messages in Benthos are always retried on an output error, but this would usually involve propagating the error back to the source of the message, whereby it would be reprocessed before reaching the output layer once again. This output type is useful whenever we wish to avoid reprocessing a message on the event of a failed send. We might, for example, have a dedupe processor that we want to avoid reapplying to the same message more than once in the pipeline. Rather than retrying the same output you may wish to retry the send using a different output target (a dead letter queue). In which case you should instead use the broker output type with the pattern 'try'. s3 type: s3 s3: bucket: content_type: application/octet-stream credentials: id: role: role_external_id: secret: token: endpoint: path: ${!count:files}-${!timestamp_unix_nano}.txt region: eu-west-1 timeout: 5s Sends message parts as objects to an Amazon S3 bucket. Each object is uploaded with the path specified with the path field. In order to have a different path for each object you should use function interpolations described here , which are calculated per message of a batch. sqs type: sqs sqs: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: max_retries: 0 region: eu-west-1 url: Sends messages to an SQS queue. stdout type: stdout stdout: delimiter: The stdout output type prints messages to stdout. Single part messages are printed with a delimiter (defaults to '\\n' if left empty). Multipart messages are written with each part delimited, with the final part followed by two delimiters, e.g. a multipart message [ \"foo\", \"bar\", \"baz\" ] would be written as: foo\\n bar\\n baz\\n\\n switch type: switch switch: outputs: [] The switch output type allows you to configure multiple conditional output targets by listing child outputs paired with conditions. Conditional logic is currently applied per whole message batch. In order to multiplex per message of a batch use the broker output with the pattern fan_out . In the following example, messages containing \"foo\" will be sent to both the foo and baz outputs. Messages containing \"bar\" will be sent to both the bar and baz outputs. Messages containing both \"foo\" and \"bar\" will be sent to all three outputs. And finally, messages that do not contain \"foo\" or \"bar\" will be sent to the baz output only. output: type: switch switch: outputs: - output: type: foo foo: foo_field_1: value1 condition: type: text text: operator: contains arg: foo fallthrough: true - output: type: bar bar: bar_field_1: value2 bar_field_2: value3 condition: type: text text: operator: contains arg: bar fallthrough: true - output: type: baz baz: baz_field_1: value4 processors: - type: baz_processor processors: - type: some_processor The switch output requires a minimum of two outputs. If no condition is defined for an output, it behaves like a static true condition. If fallthrough is set to true , the switch output will continue evaluating additional outputs after finding a match. If an output applies back pressure it will block all subsequent messages, and if an output fails to send a message, it will be retried continuously until completion or service shut down. Messages that do not match any outputs will be dropped. websocket type: websocket websocket: basic_auth: enabled: false password: username: oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: url: ws://localhost:4195/post/ws Sends messages to an HTTP server via a websocket connection.","title":"Outputs"},{"location":"outputs/#outputs","text":"This document was generated with benthos --list-outputs An output is a sink where we wish to send our consumed data after applying an optional array of processors . Only one output is configured at the root of a Benthos config. However, the output can be a broker which combines multiple outputs under a chosen brokering pattern. An output config section looks like this: output: type: foo foo: bar: baz processors: - type: qux","title":"Outputs"},{"location":"outputs/#back-pressure","text":"Benthos outputs apply back pressure to components upstream. This means if your output target starts blocking traffic Benthos will gracefully stop consuming until the issue is resolved.","title":"Back Pressure"},{"location":"outputs/#retries","text":"When a Benthos output fails to send a message the error is propagated back up to the input, where depending on the protocol it will either be pushed back to the source as a Noack (AMQP) or will be reattempted indefinitely with the commit witheld until success (Kafka). It's possible to instead have Benthos indefinitely retry an output until success with a retry output. Some other outputs, such as the broker , might also retry indefinitely depending on their configuration.","title":"Retries"},{"location":"outputs/#multiplexing-outputs","text":"It is possible to perform content based multiplexing of messages to specific outputs either by using the switch output, or a broker with the fan_out pattern and a filter processor on each output, which is a processor that drops messages if the condition does not pass. Conditions are content aware logical operators that can be combined using boolean logic. For more information regarding conditions, including a full list of available conditions please read the docs here .","title":"Multiplexing Outputs"},{"location":"outputs/#dead-letter-queues","text":"It's possible to create fallback outputs for when an output target fails using a broker output with the 'try' pattern.","title":"Dead Letter Queues"},{"location":"outputs/#contents","text":"amqp broker cache dynamic dynamodb elasticsearch file files gcp_pubsub hdfs http_client http_server inproc kafka kinesis mqtt nanomsg nats nats_stream nsq redis_list redis_pubsub redis_streams retry s3 sqs stdout switch websocket","title":"Contents"},{"location":"outputs/#amqp","text":"type: amqp amqp: exchange: benthos-exchange exchange_declare: durable: true enabled: false type: direct immediate: false key: benthos-key mandatory: false persistent: false tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: amqp://guest:guest@localhost:5672/ Sends messages to an AMQP (0.91) exchange. AMQP is a messaging protocol used by various message brokers, including RabbitMQ. The metadata from each message are delivered as headers. It's possible for this output type to create the target exchange by setting exchange_declare.enabled to true , if the exchange already exists then the declaration passively verifies that the settings match. Exchange type options are: direct|fanout|topic|x-custom TLS is automatic when connecting to an amqps URL, but custom settings can be enabled in the tls section. The field 'key' can be dynamically set using function interpolations described here .","title":"amqp"},{"location":"outputs/#broker","text":"type: broker broker: copies: 1 outputs: [] pattern: fan_out The broker output type allows you to configure multiple output targets by listing them: output: type: broker broker: pattern: fan_out outputs: - type: foo foo: foo_field_1: value1 - type: bar bar: bar_field_1: value2 bar_field_2: value3 - type: baz baz: baz_field_1: value4 processors: - type: baz_processor processors: - type: some_processor The broker pattern determines the way in which messages are allocated to outputs and can be chosen from the following:","title":"broker"},{"location":"outputs/#fan_out","text":"With the fan out pattern all outputs will be sent every message that passes through Benthos. If an output applies back pressure it will block all subsequent messages, and if an output fails to send a message it will be retried continuously until completion or service shut down.","title":"fan_out"},{"location":"outputs/#round_robin","text":"With the round robin pattern each message will be assigned a single output following their order. If an output applies back pressure it will block all subsequent messages. If an output fails to send a message then the message will be re-attempted with the next input, and so on.","title":"round_robin"},{"location":"outputs/#greedy","text":"The greedy pattern results in higher output throughput at the cost of potentially disproportionate message allocations to those outputs. Each message is sent to a single output, which is determined by allowing outputs to claim messages as soon as they are able to process them. This results in certain faster outputs potentially processing more messages at the cost of slower outputs.","title":"greedy"},{"location":"outputs/#try","text":"The try pattern attempts to send each message to only one output, starting from the first output on the list. If an output attempt fails then the broker attempts to send to the next output in the list and so on. This pattern is useful for triggering events in the case where certain output targets have broken. For example, if you had an output type http_client but wished to reroute messages whenever the endpoint becomes unreachable you could use a try broker.","title":"try"},{"location":"outputs/#utilising-more-outputs","text":"When using brokered outputs with patterns such as round robin or greedy it is possible to have multiple messages in-flight at the same time. In order to fully utilise this you either need to have a greater number of input sources than output sources or use a buffer .","title":"Utilising More Outputs"},{"location":"outputs/#processors","text":"It is possible to configure processors at the broker level, where they will be applied to all child outputs, as well as on the individual child outputs. If you have processors at both the broker level and on child outputs then the broker processors will be applied before the child nodes processors.","title":"Processors"},{"location":"outputs/#cache","text":"type: cache cache: key: ${!count:items}-${!timestamp_unix_nano} target: Stores message parts as items in a cache. Caches are configured within the resources section and can target any of the following types: dynamodb memcached memory redis Like follows: output: type: cache cache: target: foo key: ${!json_field:document.id} resources: caches: foo: type: memcached memcached: addresses: - localhost:11211 ttl: 60 In order to create a unique key value per item you should use function interpolations described here . When sending batched messages the interpolations are performed per message part.","title":"cache"},{"location":"outputs/#dynamic","text":"type: dynamic dynamic: outputs: {} prefix: timeout: 5s The dynamic type is a special broker type where the outputs are identified by unique labels and can be created, changed and removed during runtime via a REST HTTP interface. The broker pattern used is always fan_out , meaning each message will be delivered to each dynamic output. To GET a JSON map of output identifiers with their current uptimes use the '/outputs' endpoint. To perform CRUD actions on the outputs themselves use POST, DELETE, and GET methods on the /outputs/{output_id} endpoint. When using POST the body of the request should be a JSON configuration for the output, if the output already exists it will be changed.","title":"dynamic"},{"location":"outputs/#dynamodb","text":"type: dynamodb dynamodb: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: max_retries: 3 region: eu-west-1 string_columns: {} table: ttl: ttl_key: Inserts messages into a DynamoDB table. Columns are populated by writing a map of key/value pairs, where the values are function interpolated strings calculated per message of a batch. This allows you to populate columns by extracting fields within the document payload or metadata like follows: type: dynamodb dynamodb: table: foo string_columns: id: ${!json_field:id} title: ${!json_field:body.title} topic: ${!metadata:kafka_topic} full_content: ${!content}","title":"dynamodb"},{"location":"outputs/#elasticsearch","text":"type: elasticsearch elasticsearch: aws: credentials: id: role: role_external_id: secret: token: enabled: false endpoint: region: eu-west-1 backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s basic_auth: enabled: false password: username: id: ${!count:elastic_ids}-${!timestamp_unix} index: benthos_index max_retries: 0 pipeline: sniff: true timeout: 5s type: doc urls: - http://localhost:9200 Publishes messages into an Elasticsearch index. This output currently does not support creating the target index. Both the id and index fields can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part.","title":"elasticsearch"},{"location":"outputs/#file","text":"type: file file: delimiter: path: The file output type simply appends all messages to an output file. Single part messages are printed with a delimiter (defaults to '\\n' if left empty). Multipart messages are written with each part delimited, with the final part followed by two delimiters, e.g. a multipart message [ \"foo\", \"bar\", \"baz\" ] would be written as: foo\\n bar\\n baz\\n\\n","title":"file"},{"location":"outputs/#files","text":"type: files files: path: ${!count:files}-${!timestamp_unix_nano}.txt Writes each individual part of each message to a new file. Message parts only contain raw data, and therefore in order to create a unique file for each part you need to generate unique file names. This can be done by using function interpolations on the path field as described here . When sending batched messages these interpolations are performed per message part.","title":"files"},{"location":"outputs/#gcp_pubsub","text":"type: gcp_pubsub gcp_pubsub: project: topic: Sends messages to a GCP Cloud Pub/Sub topic. Metadata from messages are sent as attributes.","title":"gcp_pubsub"},{"location":"outputs/#hdfs","text":"type: hdfs hdfs: directory: hosts: - localhost:9000 path: ${!count:files}-${!timestamp_unix_nano}.txt user: benthos_hdfs Sends message parts as files to a HDFS directory. Each file is written with the path specified with the 'path' field, in order to have a different path for each object you should use function interpolations described here . When sending batched messages the interpolations are performed per message part.","title":"hdfs"},{"location":"outputs/#http_client","text":"type: http_client http_client: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: rate_limit: retries: 3 retry_period: 1s timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/post verb: POST Sends messages to an HTTP server. The request will be retried for each message whenever the response code is outside the range of 200 - 299 inclusive. It is possible to list codes outside of this range in the drop_on field in order to prevent retry attempts. The period of time between retries is linear by default. Response codes that are within the backoff_on list will instead apply exponential backoff between retry attempts. When the number of retries expires the output will reject the message, the behaviour after this will depend on the pipeline but usually this simply means the send is attempted again until successful whilst applying back pressure. The URL and header values of this type can be dynamically set using function interpolations described here . The body of the HTTP request is the raw contents of the message payload. If the message has multiple parts the request will be sent according to RFC1341","title":"http_client"},{"location":"outputs/#http_server","text":"type: http_server http_server: address: cert_file: key_file: path: /get stream_path: /get/stream timeout: 5s ws_path: /get/ws Sets up an HTTP server that will send messages over HTTP(S) GET requests. HTTP 2.0 is supported when using TLS, which is enabled when key and cert files are specified. You can leave the 'address' config field blank in order to use the default service, but this will ignore TLS options. You can receive a single, discrete message on the configured 'path' endpoint, or receive a constant stream of line delimited messages on the configured 'stream_path' endpoint.","title":"http_server"},{"location":"outputs/#inproc","text":"type: inproc inproc: Sends data directly to Benthos inputs by connecting to a unique ID. This allows you to hook up isolated streams whilst running Benthos in --streams mode mode, it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow. It is possible to connect multiple inputs to the same inproc ID, but only one output can connect to an inproc ID, and will replace existing outputs if a collision occurs.","title":"inproc"},{"location":"outputs/#kafka","text":"type: kafka kafka: ack_replicas: false addresses: - localhost:9092 client_id: benthos_kafka_output compression: none key: max_msg_bytes: 1e+06 round_robin_partitions: false target_version: 1.0.0 timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false topic: benthos_stream The kafka output type writes messages to a kafka broker, these messages are acknowledged, which is propagated back to the input. The config field ack_replicas determines whether we wait for acknowledgement from all replicas or just a single broker. It is possible to specify a compression codec to use out of the following options: none, snappy, lz4 and gzip. If the field key is not empty then each message will be given its contents as a key. Both the key and topic fields can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part. By default the paritioner will select partitions based on a hash of the key value. If the key is empty then a partition is chosen at random. You can alternatively force the partitioner to round-robin partitions with the field round_robin_partitions .","title":"kafka"},{"location":"outputs/#tls","text":"Custom TLS settings can be used to override system defaults. This includes providing a collection of root certificate authorities, providing a list of client certificates to use for client verification and skipping certificate verification. Client certificates can either be added by file or by raw contents: enabled: true client_certs: - cert_file: ./example.pem key_file: ./example.key - cert: foo key: bar","title":"TLS"},{"location":"outputs/#kinesis","text":"type: kinesis kinesis: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: hash_key: max_retries: 0 partition_key: region: eu-west-1 stream: Sends messages to a Kinesis stream. Both the partition_key (required) and hash_key (optional) fields can be dynamically set using function interpolations described here . When sending batched messages the interpolations are performed per message part.","title":"kinesis"},{"location":"outputs/#mqtt","text":"type: mqtt mqtt: client_id: benthos_output qos: 1 topic: benthos_topic urls: - tcp://localhost:1883 Pushes messages to an MQTT broker.","title":"mqtt"},{"location":"outputs/#nanomsg","text":"type: nanomsg nanomsg: bind: false poll_timeout: 5s socket_type: PUSH urls: - tcp://localhost:5556 The scalability protocols are common communication patterns. This output should be compatible with any implementation, but specifically targets Nanomsg. Currently only PUSH and PUB sockets are supported.","title":"nanomsg"},{"location":"outputs/#nats","text":"type: nats nats: subject: benthos_messages urls: - nats://localhost:4222 Publish to an NATS subject. NATS is at-most-once, so delivery is not guaranteed. For at-least-once behaviour with NATS look at NATS Stream. This output will interpolate functions within the subject field, you can find a list of functions here .","title":"nats"},{"location":"outputs/#nats_stream","text":"type: nats_stream nats_stream: client_id: benthos_client cluster_id: test-cluster subject: benthos_messages urls: - nats://localhost:4222 Publish to a NATS Stream subject.","title":"nats_stream"},{"location":"outputs/#nsq","text":"type: nsq nsq: nsqd_tcp_address: localhost:4150 topic: benthos_messages user_agent: benthos_producer Publish to an NSQ topic. The topic field can be dynamically set using function interpolations described here . When sending batched messages these interpolations are performed per message part.","title":"nsq"},{"location":"outputs/#redis_list","text":"type: redis_list redis_list: key: benthos_list url: tcp://localhost:6379 Pushes messages onto the end of a Redis list (which is created if it doesn't already exist) using the RPUSH command.","title":"redis_list"},{"location":"outputs/#redis_pubsub","text":"type: redis_pubsub redis_pubsub: channel: benthos_chan url: tcp://localhost:6379 Publishes messages through the Redis PubSub model. It is not possible to guarantee that messages have been received. This output will interpolate functions within the channel field, you can find a list of functions here .","title":"redis_pubsub"},{"location":"outputs/#redis_streams","text":"type: redis_streams redis_streams: body_key: body max_length: 0 stream: benthos_stream url: tcp://localhost:6379 Pushes messages to a Redis (v5.0+) Stream (which is created if it doesn't already exist) using the XADD command. It's possible to specify a maximum length of the target stream by setting it to a value greater than 0, in which case this cap is applied only when Redis is able to remove a whole macro node, for efficiency. Redis stream entries are key/value pairs, as such it is necessary to specify the key to be set to the body of the message. All metadata fields of the message will also be set as key/value pairs, if there is a key collision between a metadata item and the body then the body takes precedence.","title":"redis_streams"},{"location":"outputs/#retry","text":"type: retry retry: backoff: initial_interval: 500ms max_elapsed_time: 0s max_interval: 3s max_retries: 0 output: {} Attempts to write messages to a child output and if the write fails for any reason the message is retried either until success or, if the retries or max elapsed time fields are non-zero, either is reached. All messages in Benthos are always retried on an output error, but this would usually involve propagating the error back to the source of the message, whereby it would be reprocessed before reaching the output layer once again. This output type is useful whenever we wish to avoid reprocessing a message on the event of a failed send. We might, for example, have a dedupe processor that we want to avoid reapplying to the same message more than once in the pipeline. Rather than retrying the same output you may wish to retry the send using a different output target (a dead letter queue). In which case you should instead use the broker output type with the pattern 'try'.","title":"retry"},{"location":"outputs/#s3","text":"type: s3 s3: bucket: content_type: application/octet-stream credentials: id: role: role_external_id: secret: token: endpoint: path: ${!count:files}-${!timestamp_unix_nano}.txt region: eu-west-1 timeout: 5s Sends message parts as objects to an Amazon S3 bucket. Each object is uploaded with the path specified with the path field. In order to have a different path for each object you should use function interpolations described here , which are calculated per message of a batch.","title":"s3"},{"location":"outputs/#sqs","text":"type: sqs sqs: backoff: initial_interval: 1s max_elapsed_time: 30s max_interval: 5s credentials: id: role: role_external_id: secret: token: endpoint: max_retries: 0 region: eu-west-1 url: Sends messages to an SQS queue.","title":"sqs"},{"location":"outputs/#stdout","text":"type: stdout stdout: delimiter: The stdout output type prints messages to stdout. Single part messages are printed with a delimiter (defaults to '\\n' if left empty). Multipart messages are written with each part delimited, with the final part followed by two delimiters, e.g. a multipart message [ \"foo\", \"bar\", \"baz\" ] would be written as: foo\\n bar\\n baz\\n\\n","title":"stdout"},{"location":"outputs/#switch","text":"type: switch switch: outputs: [] The switch output type allows you to configure multiple conditional output targets by listing child outputs paired with conditions. Conditional logic is currently applied per whole message batch. In order to multiplex per message of a batch use the broker output with the pattern fan_out . In the following example, messages containing \"foo\" will be sent to both the foo and baz outputs. Messages containing \"bar\" will be sent to both the bar and baz outputs. Messages containing both \"foo\" and \"bar\" will be sent to all three outputs. And finally, messages that do not contain \"foo\" or \"bar\" will be sent to the baz output only. output: type: switch switch: outputs: - output: type: foo foo: foo_field_1: value1 condition: type: text text: operator: contains arg: foo fallthrough: true - output: type: bar bar: bar_field_1: value2 bar_field_2: value3 condition: type: text text: operator: contains arg: bar fallthrough: true - output: type: baz baz: baz_field_1: value4 processors: - type: baz_processor processors: - type: some_processor The switch output requires a minimum of two outputs. If no condition is defined for an output, it behaves like a static true condition. If fallthrough is set to true , the switch output will continue evaluating additional outputs after finding a match. If an output applies back pressure it will block all subsequent messages, and if an output fails to send a message, it will be retried continuously until completion or service shut down. Messages that do not match any outputs will be dropped.","title":"switch"},{"location":"outputs/#websocket","text":"type: websocket websocket: basic_auth: enabled: false password: username: oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: url: ws://localhost:4195/post/ws Sends messages to an HTTP server via a websocket connection.","title":"websocket"},{"location":"processors/","text":"Processors This document was generated with benthos --list-processors . Benthos processors are functions applied to messages passing through a pipeline. The function signature allows a processor to mutate or drop messages depending on the content of the message. Processors are set via config, and depending on where in the config they are placed they will be run either immediately after a specific input (set in the input section), on all messages (set in the pipeline section) or before a specific output (set in the output section). Most processors apply to all messages and can be placed in the pipeline section: pipeline: threads: 1 processors: - type: foo foo: bar: baz The threads field in the pipeline section determines how many parallel processing threads are created. You can read more about parallel processing in the pipeline guide . By organising processors you can configure complex behaviours in your pipeline. You can find some examples here . Error Handling Some processors have conditions whereby they might fail. Benthos has mechanisms for detecting and recovering from these failures which can be read about here . Batching and Multiple Part Messages All Benthos processors support multiple part messages, which are synonymous with batches. Some processors such as batch and split are able to create, expand and break down batches. Many processors are able to perform their behaviours on specific parts of a message batch, or on all parts, and have a field parts for specifying an array of part indexes they should apply to. If the list of target parts is empty these processors will be applied to all message parts. Part indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if part = -1 then the selected part will be the last part of the message, if part = -2 then the part before the last element will be selected, and so on. Some processors such as filter and dedupe act across an entire batch, when instead we'd like to perform them on individual messages of a batch. In this case the process_batch processor can be used. Contents archive awk batch bounds_check cache catch compress conditional decode decompress dedupe encode filter filter_parts grok group_by group_by_value hash hash_sample http insert_part jmespath json lambda log merge_json metadata metric noop process_batch process_dag process_field process_map sample select_parts sleep split subprocess switch text throttle try unarchive while archive type: archive archive: format: binary path: ${!count:files}-${!timestamp_unix_nano}.txt Archives all the messages of a batch into a single message according to the selected archive format. Supported archive formats are: tar , zip , binary , lines and json_array . Some archive formats (such as tar, zip) treat each archive item (message part) as a file with a path. Since message parts only contain raw data a unique path must be generated for each part. This can be done by using function interpolations on the 'path' field as described here . For types that aren't file based (such as binary) the file field is ignored. The json_array format attempts to JSON parse each message and append the result to an array, which becomes the contents of the resulting message. The resulting archived message adopts the metadata of the first message part of the batch. awk type: awk awk: codec: text parts: [] program: BEGIN { x = 0 } { print $0, x; x++ } Executes an AWK program on messages by feeding contents as the input based on a codec and replaces the contents with the result. If the result is empty (nothing is printed by the program) then the original message contents remain unchanged. Comes with a wide range of custom functions for accessing message metadata, json fields, printing logs, etc. These functions can be overridden by functions within the program. Codecs A codec can be specified that determines how the contents of the message are fed into the program. This does not change the custom functions. none An empty string is fed into the program. Functions can still be used in order to extract and mutate metadata and message contents. This is useful for when your program only uses functions and doesn't need the full text of the message to be parsed by the program. text The full contents of the message are fed into the program as a string, allowing you to reference tokenised segments of the message with variables ($0, $1, etc). Custom functions can still be used with this codec. This is the default codec as it behaves most similar to typical usage of the awk command line tool. json No contents are fed into the program. Instead, variables are extracted from the message by walking the flattened JSON structure. Each value is converted into a variable by taking its full path, e.g. the object: { foo : { bar : { value : 10 }, created_at : 2018-12-18T11:57:32 } } Would result in the following variable declarations: foo_bar_value = 10 foo_created_at = 2018-12-18T11:57:32 Custom functions can also still be used with this codec. batch type: batch batch: byte_size: 0 condition: type: static static: false count: 0 period: Reads a number of discrete messages, buffering (but not acknowledging) the message parts until either: The byte_size field is non-zero and the total size of the batch in bytes matches or exceeds it. The count field is non-zero and the total number of messages in the batch matches or exceeds it. A message added to the batch causes the condition to resolve true . The period field is non-empty and the time since the last batch exceeds its value. Once one of these events trigger the parts are combined into a single batch of messages and sent through the pipeline. After reaching a destination the acknowledgment is sent out for all messages inside the batch at the same time, preserving at-least-once delivery guarantees. The period field - when non-empty - defines a period of time whereby a batch is sent even if the byte_size has not yet been reached. Batch parameters are only triggered when a message is added, meaning a pending batch can last beyond this period if no messages are added since the period was reached. When a batch is sent to an output the behaviour will differ depending on the protocol. If the output type supports multipart messages then the batch is sent as a single message with multiple parts. If the output only supports single part messages then the parts will be sent as a batch of single part messages. If the output supports neither multipart or batches of messages then Benthos falls back to sending them individually. If a Benthos stream contains multiple brokered inputs or outputs then the batch operator should always be applied directly after an input in order to avoid unexpected behaviour and message ordering. bounds_check type: bounds_check bounds_check: max_part_size: 1.073741824e+09 max_parts: 100 min_part_size: 1 min_parts: 1 Checks whether each message batch fits within certain boundaries, and drops batches that do not. cache type: cache cache: cache: key: operator: set parts: [] value: Performs operations against a cache resource for each message of a batch, allowing you to store or retrieve data within message payloads. This processor will interpolate functions within the key and value fields individually for each message of the batch. This allows you to specify dynamic keys and values based on the contents of the message payloads and metadata. You can find a list of functions here . Operators set Set a key in the cache to a value. If the key already exists the contents are overridden. add Set a key in the cache to a value. If the key already exists the action fails with a 'key already exists' error, which can be detected with processor error handling . get Retrieve the contents of a cached key and replace the original message payload with the result. If the key does not exist the action fails with an error, which can be detected with processor error handling . Examples The cache processor can be used in combination with other processors in order to solve a variety of data stream problems. Deduplication Deduplication can be done using the add operator with a key extracted from the message payload, since it fails when a key already exists we can remove the duplicates using a processor_failed condition: - type: cache cache: cache: TODO operator: add key: ${!json_field:message.id} value: storeme - type: filter_parts filter_parts: type: processor_failed Hydration It's possible to enrich payloads with content previously stored in a cache by using the process_dag processor: - type: process_map process_map: processors: - type: cache cache: cache: TODO operator: get key: ${!json_field:message.document_id} postmap: message.document: . catch type: catch catch: [] Behaves similarly to the process_batch processor, where a list of child processors are applied to individual messages of a batch. However, processors are only applied to messages that failed a processing step prior to the catch. For example, with the following config: - type: foo - type: catch catch: - type: bar - type: baz If the processor foo fails for a particular message, that message will be fed into the processors bar and baz . Messages that do not fail for the processor foo will skip these processors. When messages leave the catch block their fail flags are cleared. This processor is useful for when it's possible to recover failed messages, or when special actions (such as logging/metrics) are required before dropping them. More information about error handing can be found here . compress type: compress compress: algorithm: gzip level: -1 parts: [] Compresses messages according to the selected algorithm. Supported compression algorithms are: gzip, zlib, flate. The 'level' field might not apply to all algorithms. conditional type: conditional conditional: condition: type: text text: arg: operator: equals_cs part: 0 else_processors: [] processors: [] Conditional is a processor that has a list of child processors , else_processors , and a condition. For each message batch, if the condition passes, the child processors will be applied, otherwise the else_processors are applied. This processor is useful for applying processors based on the content of message batches. In order to conditionally process each message of a batch individually use this processor with the process_batch processor. You can find a full list of conditions here . decode type: decode decode: parts: [] scheme: base64 Decodes messages according to the selected scheme. Supported available schemes are: base64. decompress type: decompress decompress: algorithm: gzip parts: [] Decompresses messages according to the selected algorithm. Supported decompression types are: gzip, zlib, bzip2, flate. dedupe type: dedupe dedupe: cache: drop_on_err: true hash: none key: parts: - 0 Dedupes message batches by caching selected (and optionally hashed) messages, dropping batches that are already cached. The hash type can be chosen from: none or xxhash. This processor acts across an entire batch, in order to deduplicate individual messages within a batch use this processor with the process_batch processor. Optionally, the key field can be populated in order to hash on a function interpolated string rather than the full contents of messages. This allows you to deduplicate based on dynamic fields within a message, such as its metadata, JSON fields, etc. A full list of interpolation functions can be found here . For example, the following config would deduplicate based on the concatenated values of the metadata field kafka_key and the value of the JSON path id within the message contents: dedupe: cache: foocache key: ${!metadata:kafka_key}-${!json_field:id} Caches should be configured as a resource, for more information check out the documentation here . When using this processor with an output target that might fail you should always wrap the output within a retry block. This ensures that during outages your messages aren't reprocessed after failures, which would result in messages being dropped. Delivery Guarantees Performing deduplication on a stream using a distributed cache voids any at-least-once guarantees that it previously had. This is because the cache will preserve message signatures even if the message fails to leave the Benthos pipeline, which would cause message loss in the event of an outage at the output sink followed by a restart of the Benthos instance. If you intend to preserve at-least-once delivery guarantees you can avoid this problem by using a memory based cache. This is a compromise that can achieve effective deduplication but parallel deployments of the pipeline as well as service restarts increase the chances of duplicates passing undetected. encode type: encode encode: parts: [] scheme: base64 Encodes messages according to the selected scheme. Supported schemes are: base64. filter type: filter filter: type: text text: arg: operator: equals_cs part: 0 Tests each message batch against a condition, if the condition fails then the batch is dropped. You can find a full list of conditions here . In order to filter individual messages of a batch use the filter_parts processor. filter_parts type: filter_parts filter_parts: type: text text: arg: operator: equals_cs part: 0 Tests each individual message of a batch against a condition, if the condition fails then the message is dropped. If the resulting batch is empty it will be dropped. You can find a full list of conditions here , in this case each condition will be applied to a message as if it were a single message batch. This processor is useful if you are combining messages into batches using the batch processor and wish to remove specific parts. grok type: grok grok: named_captures_only: true output_format: json parts: [] patterns: [] remove_empty_values: true use_default_patterns: true Parses message payloads by attempting to apply a list of Grok patterns, if a pattern returns at least one value a resulting structured object is created according to the chosen output format and will replace the payload. Currently only json is a valid output format. This processor respects type hints in the grok patterns, therefore with the pattern %{WORD:first},%{INT:second:int} and a payload of foo,1 the resulting payload would be {\"first\":\"foo\",\"second\":1} . group_by type: group_by group_by: [] Splits a batch of messages into N batches, where each resulting batch contains a group of messages determined by conditions that are applied per message of the original batch. Once the groups are established a list of processors are applied to their respective grouped batch, which can be used to label the batch as per their grouping. Each group is configured in a list with a condition and a list of processors: type: group_by group_by: - condition: type: static static: true processors: - type: noop Messages are added to the first group that passes and can only belong to a single group. Messages that do not pass the conditions of any group are placed in a final batch with no processors applied. For example, imagine we have a batch of messages that we wish to split into two groups - the foos and the bars - which should be sent to different output destinations based on those groupings. We also need to send the foos as a tar gzip archive. For this purpose we can use the group_by processor with a switch output: pipeline: processors: - type: group_by group_by: - condition: type: text text: operator: contains arg: this is a foo processors: - type: archive archive: format: tar - type: compress compress: algorithm: gzip - type: metadata metadata: operator: set key: grouping value: foo output: type: switch switch: outputs: - output: type: foo_output condition: type: metadata metadata: operator: equals key: grouping arg: foo - output: type: bar_output Since any message that isn't a foo is a bar, and bars do not require their own processing steps, we only need a single grouping configuration. group_by_value type: group_by_value group_by_value: value: ${!metadata:example} Splits a batch of messages into N batches, where each resulting batch contains a group of messages determined by a function interpolated string evaluated per message. This allows you to group messages using arbitrary fields within their content or metadata, process them individually, and send them to unique locations as per their group. For example, if we were consuming Kafka messages and needed to group them by their key, archive the groups, and send them to S3 with the key as part of the path we could achieve that with the following: pipeline: processors: - type: group_by_value group_by_value: value: ${!metadata:kafka_key} - type: archive archive: format: tar - type: compress compress: algorithm: gzip output: type: s3 s3: bucket: TODO path: docs/${!metadata:kafka_key}/${!count:files}-${!timestamp_unix_nano}.tar.gz hash type: hash hash: algorithm: sha256 parts: [] Hashes messages according to the selected algorithm. Supported algorithms are: sha256, sha512, xxhash64. This processor is mostly useful when combined with the process_field processor as it allows you to hash a specific field of a document like this: # Hash the contents of 'foo.bar' process_field: path: foo.bar processors: - type: hash hash: algorithm: sha256 hash_sample type: hash_sample hash_sample: parts: - 0 retain_max: 10 retain_min: 0 Retains a percentage of message batches deterministically by hashing selected messages and checking the hash against a valid range, dropping all others. For example, setting retain_min to 0.0 and remain_max to 50.0 results in dropping half of the input stream, and setting retain_min to 50.0 and retain_max to 100.1 will drop the other half. In order to sample individual messages of a batch use this processor with the process_batch processor. http type: http http: max_parallel: 0 parallel: false request: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: rate_limit: retries: 3 retry_period: 1s timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/post verb: POST Performs an HTTP request using a message batch as the request body, and replaces the original message parts with the body of the response. If the batch contains only a single message part then it will be sent as the body of the request. If the batch contains multiple messages then they will be sent as a multipart HTTP request using a Content-Type: multipart header. If you are sending batches and wish to avoid this behaviour then you can set the parallel flag to true and the messages of a batch will be sent as individual requests in parallel. You can also cap the max number of parallel requests with max_parallel . Alternatively, you can use the archive processor to create a single message from the batch. The rate_limit field can be used to specify a rate limit resource to cap the rate of requests across all parallel components service wide. The URL and header values of this type can be dynamically set using function interpolations described here . In order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the process_map or process_field processors. Error Handling When all retry attempts for a message are exhausted the processor cancels the attempt. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about these patterns here . insert_part type: insert_part insert_part: content: index: -1 Insert a new message into a batch at an index. If the specified index is greater than the length of the existing batch it will be appended to the end. The index can be negative, and if so the message will be inserted from the end counting backwards starting from -1. E.g. if index = -1 then the new message will become the last of the batch, if index = -2 then the new message will be inserted before the last message, and so on. If the negative index is greater than the length of the existing batch it will be inserted at the beginning. This processor will interpolate functions within the 'content' field, you can find a list of functions here . jmespath type: jmespath jmespath: parts: [] query: Parses a message as a JSON document and attempts to apply a JMESPath expression to it, replacing the contents of the part with the result. Please refer to the JMESPath website for information and tutorials regarding the syntax of expressions. For example, with the following config: jmespath: query: locations[?state == 'WA'].name | sort(@) | {Cities: join(', ', @)} If the initial contents of a message were: { locations : [ { name : Seattle , state : WA }, { name : New York , state : NY }, { name : Bellevue , state : WA }, { name : Olympia , state : WA } ] } Then the resulting contents would be: { Cities : Bellevue, Olympia, Seattle } It is possible to create boolean queries with JMESPath, in order to filter messages with boolean queries please instead use the jmespath condition. json type: json json: operator: get parts: [] path: value: Parses messages as a JSON document, performs a mutation on the data, and then overwrites the previous contents with the new value. If the path is empty or \".\" the root of the data will be targeted. This processor will interpolate functions within the 'value' field, you can find a list of functions here . Operators append Appends a value to an array at a target dot path. If the path does not exist all objects in the path are created (unless there is a collision). If a non-array value already exists in the target path it will be replaced by an array containing the original value as well as the new value. If the value is an array the elements of the array are expanded into the new array. E.g. if the target is an array [0,1] and the value is also an array [2,3] , the result will be [0,1,2,3] as opposed to [0,1,[2,3]] . clean Walks the JSON structure and deletes any fields where the value is: An empty array An empty object An empty string null copy Copies the value of a target dot path (if it exists) to a location. The destination path is specified in the value field. If the destination path does not exist all objects in the path are created (unless there is a collision). delete Removes a key identified by the dot path. If the path does not exist this is a no-op. move Moves the value of a target dot path (if it exists) to a new location. The destination path is specified in the value field. If the destination path does not exist all objects in the path are created (unless there is a collision). select Reads the value found at a dot path and replaced the original contents entirely by the new value. set Sets the value of a field at a dot path. If the path does not exist all objects in the path are created (unless there is a collision). The value can be any type, including objects and arrays. When using YAML configuration files a YAML object will be converted into a JSON object, i.e. with the config: json: operator: set parts: [0] path: some.path value: foo: bar: 5 The value will be converted into '{\"foo\":{\"bar\":5}}'. If the YAML object contains keys that aren't strings those fields will be ignored. lambda type: lambda lambda: credentials: id: role: role_external_id: secret: token: endpoint: function: parallel: false rate_limit: region: eu-west-1 retries: 3 timeout: 5s Invokes an AWS lambda for each message part of a batch. The contents of the message part is the payload of the request, and the result of the invocation will become the new contents of the message. It is possible to perform requests per message of a batch in parallel by setting the parallel flag to true . The rate_limit field can be used to specify a rate limit resource to cap the rate of requests across parallel components service wide. In order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the process_map or process_field processors. Error Handling When all retry attempts for a message are exhausted the processor cancels the attempt. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about these patterns here . log type: log log: fields: {} level: INFO message: Log is a processor that prints a log event each time it processes a message. The message is then sent onwards unchanged. The log message can be set using function interpolations described here which allows you to log the contents and metadata of a message. For example, if we wished to create a debug log event for each message in a pipeline in order to expose the JSON field foo.bar as well as the metadata field kafka_partition we can achieve that with the following config: type: log log: level: DEBUG message: field: ${!json_field:foo.bar}, part: ${!metadata:kafka_partition} The level field determines the log level of the printed events and can be any of the following values: TRACE, DEBUG, INFO, WARN, ERROR. Structured Fields It's also possible to output a map of structured fields, this only works when the service log is set to output as JSON. The field values are function interpolated, meaning it's possible to output structured fields containing message contents and metadata, e.g.: type: log log: level: DEBUG message: foo fields: id: ${!json_field:id} kafka_topic: ${!metadata:kafka_topic} merge_json type: merge_json merge_json: parts: [] retain_parts: false Parses selected messages of a batch as JSON documents, attempts to merge them into one single JSON document and then writes it to a new message at the end of the batch. Merged parts are removed unless retain_parts is set to true. The new merged message will contain the metadata of the first part to be merged. metadata type: metadata metadata: key: example operator: set parts: [] value: ${!hostname} Performs operations on the metadata of a message. Metadata are key/value pairs that are associated with message parts of a batch. Metadata values can be referred to using configuration interpolation functions , which allow you to set fields in certain outputs using these dynamic values. This processor will interpolate functions within the value field, you can find a list of functions here . This allows you to set the contents of a metadata field using values taken from the message payload. Operators set Sets the value of a metadata key. delete_all Removes all metadata values from the message. delete_prefix Removes all metadata values from the message where the key is prefixed with the value provided. metric type: metric metric: labels: {} path: type: counter value: Creates metrics by extracting values from messages. The path field should be a dot separated path of the metric to be set and will automatically be converted into the correct format of the configured metric aggregator. The value field can be set using function interpolations described here and is used according to the specific type. Types counter Increments a counter by exactly 1, the contents of value are ignored by this type. counter_parts Increments a counter by the number of parts within the message batch, the contents of value are ignored by this type. counter_by If the contents of value can be parsed as a positive integer value then the counter is incremented by this value. For example, the following configuration will increment the value of the count.custom.field metric by the contents of field.some.value : type: metric metric: type: counter_by path: count.custom.field value: ${!json_field:field.some.value} gauge If the contents of value can be parsed as a positive integer value then the gauge is set to this value. For example, the following configuration will set the value of the gauge.custom.field metric to the contents of field.some.value : type: metric metric: type: gauge path: gauge.custom.field value: ${!json_field:field.some.value} timing Equivalent to gauge where instead the metric is a timing. Labels Some metrics aggregators, such as Prometheus, support arbitrary labels, in which case the labels field can be used in order to create them. Label values can also be set using function interpolations in order to dynamically populate them with context about the message. noop type: noop Noop is a no-op processor that does nothing, the message passes through unchanged. process_batch type: process_batch process_batch: [] A processor that applies a list of child processors to messages of a batch as though they were each a batch of one message. This is useful for forcing batch wide processors such as dedupe to apply to individual message parts of a batch instead. Please note that most processors already process per message of a batch, and this processor is not needed in those cases. process_dag type: process_dag process_dag: {} A processor that manages a map of process_map processors and calculates a Directed Acyclic Graph (DAG) of their dependencies by referring to their postmap targets for provided fields and their premap targets for required fields. The DAG is then used to execute the children in the necessary order with the maximum parallelism possible. The field dependencies is an optional array of fields that a child depends on. This is useful for when fields are required but don't appear within a premap such as those used in conditions. This processor is extremely useful for performing a complex mesh of enrichments where network requests mean we desire maximum parallelism across those enrichments. For example, if we had three target HTTP services that we wished to enrich each document with - foo, bar and baz - where baz relies on the result of both foo and bar, we might express that relationship here like so: type: process_dag process_dag: foo: premap: .: . processors: - type: http http: request: url: http://foo/enrich postmap: foo_result: . bar: premap: .: msg.sub.path processors: - type: http http: request: url: http://bar/enrich postmap: bar_result: . baz: premap: foo_obj: foo_result bar_obj: bar_result processors: - type: http http: request: url: http://baz/enrich postmap: baz_obj: . With this config the DAG would determine that the children foo and bar can be executed in parallel, and once they are both finished we may proceed onto baz. process_field type: process_field process_field: parts: [] path: processors: [] A processor that extracts the value of a field within payloads (currently only JSON format is supported) then applies a list of processors to the extracted value, and finally sets the field within the original payloads to the processed result. If the number of messages resulting from the processing steps does not match the original count then this processor fails and the messages continue unchanged. Therefore, you should avoid using batch and filter type processors in this list. process_map type: process_map process_map: conditions: [] parts: [] postmap: {} postmap_optional: {} premap: {} premap_optional: {} processors: [] A processor that extracts and maps fields from the original payload into new objects, applies a list of processors to the newly constructed objects, and finally maps the result back into the original payload. This processor is useful for performing processors on subsections of a payload. For example, you could extract sections of a JSON object in order to construct a request object for an http processor, then map the result back into a field within the original object. The order of stages of this processor are as follows: Conditions are applied to each individual message part in the batch, determining whether the part will be mapped. If the conditions are empty all message parts will be mapped. If the field parts is populated the message parts not in this list are also excluded from mapping. Message parts that are flagged for mapping are mapped according to the premap fields, creating a new object. If the premap stage fails (targets are not found) the message part will not be processed. Message parts that are mapped are processed as a batch. You may safely break the batch into individual parts during processing with the split processor. After all child processors are applied to the mapped messages they are mapped back into the original message parts they originated from as per your postmap. If the postmap stage fails the mapping is skipped and the message payload remains as it started. Map paths are arbitrary dot paths, target path hierarchies are constructed if they do not yet exist. Processing is skipped for message parts where the premap targets aren't found, for optional premap targets use premap_optional . Map target paths that are parents of other map target paths will always be mapped first, therefore it is possible to map subpath overrides. If postmap targets are not found the merge is abandoned, for optional postmap targets use postmap_optional . If the premap is empty then the full payload is sent to the processors, if the postmap is empty then the processed result replaces the original contents entirely. Maps can reference the root of objects either with an empty string or '.', for example the maps: premap: .: foo.bar postmap: foo.bar: . Would create a new object where the root is the value of foo.bar and would map the full contents of the result back into foo.bar . If the number of total message parts resulting from the processing steps does not match the original count then this processor fails and the messages continue unchanged. Therefore, you should avoid using batch and filter type processors in this list. Batch Ordering This processor supports batch messages. When message parts are post-mapped after processing they will be correctly aligned with the original batch. However, the ordering of premapped message parts as they are sent through processors are not guaranteed to match the ordering of the original batch. sample type: sample sample: retain: 10 seed: 0 Retains a randomly sampled percentage of message batches (0 to 100) and drops all others. The random seed is static in order to sample deterministically, but can be set in config to allow parallel samples that are unique. select_parts type: select_parts select_parts: parts: - 0 Cherry pick a set of messages from a batch by their index. Indexes larger than the number of messages are simply ignored. The selected parts are added to the new message batch in the same order as the selection array. E.g. with 'parts' set to [ 2, 0, 1 ] and the message parts [ '0', '1', '2', '3' ], the output will be [ '2', '0', '1' ]. If none of the selected parts exist in the input batch (resulting in an empty output message) the batch is dropped entirely. Message indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if index = -1 then the selected part will be the last part of the message, if index = -2 then the part before the last element with be selected, and so on. sleep type: sleep sleep: duration: 100us Sleep for a period of time specified as a duration string. This processor will interpolate functions within the duration field, you can find a list of functions here . split type: split split: byte_size: 0 size: 1 Breaks message batches (synonymous with multiple part messages) into smaller batches. The size of the resulting batches are determined either by a discrete size or, if the field byte_size is non-zero, then by total size in bytes (which ever limit is reached first). If there is a remainder of messages after splitting a batch the remainder is also sent as a single batch. For example, if your target size was 10, and the processor received a batch of 95 message parts, the result would be 9 batches of 10 messages followed by a batch of 5 messages. subprocess type: subprocess subprocess: args: [] name: cat parts: [] Subprocess is a processor that runs a process in the background and, for each message, will pipe its contents to the stdin stream of the process followed by a newline. The subprocess must then either return a line over stdout or stderr. If a response is returned over stdout then its contents will replace the message. If a response is instead returned from stderr will be logged and the message will continue unchanged and will be marked as failed. Subprocess requirements It is required that subprocesses flush their stdout and stderr pipes for each line. Benthos will attempt to keep the process alive for as long as the pipeline is running. If the process exits early it will be restarted. Messages containing line breaks If a message contains line breaks each line of the message is piped to the subprocess and flushed, and a response is expected from the subprocess before another line is fed in. switch type: switch switch: [] Switch is a processor that lists child case objects each containing a condition and processors. Each batch of messages is tested against the condition of each child case until a condition passes, whereby the processors of that case will be executed on the batch. Each case may specify a boolean fallthrough field indicating whether the next case should be executed after it (the default is false .) A case takes this form: - condition: type: foo processors: - type: foo fallthrough: false In order to switch each message of a batch individually use this processor with the process_batch processor. You can find a full list of conditions here . text type: text text: arg: operator: trim_space parts: [] value: Performs text based mutations on payloads. This processor will interpolate functions within the value field, you can find a list of functions here . Operators append Appends text to the end of the payload. escape_url_query Escapes text so that it is safe to place within the query section of a URL. unescape_url_query Unescapes text that has been url escaped. find_regexp Extract the matching section of the argument regular expression in a message. prepend Prepends text to the beginning of the payload. replace Replaces all occurrences of the argument in a message with a value. replace_regexp Replaces all occurrences of the argument regular expression in a message with a value. Inside the value $ signs are interpreted as submatch expansions, e.g. $1 represents the text of the first submatch. set Replace the contents of a message entirely with a value. strip_html Removes all HTML tags from a message. to_lower Converts all text into lower case. to_upper Converts all text into upper case. trim Removes all leading and trailing occurrences of characters within the arg field. trim_space Removes all leading and trailing whitespace from the payload. throttle type: throttle throttle: period: 100us Throttles the throughput of a pipeline to a maximum of one message batch per period. This throttle is per processing pipeline, and therefore four threads each with a throttle would result in four times the rate specified. The period should be specified as a time duration string. For example, '1s' would be 1 second, '10ms' would be 10 milliseconds, etc. try type: try try: [] Behaves similarly to the process_batch processor, where a list of child processors are applied to individual messages of a batch. However, if a processor fails for a message then that message will skip all following processors. For example, with the following config: - type: try try: - type: foo - type: bar - type: baz If the processor foo fails for a particular message, that message will skip the processors bar and baz . This processor is useful for when child processors depend on the successful output of previous processors. This processor can be followed with a catch processor for defining child processors to be applied only to failed messages. More information about error handing can be found here . unarchive type: unarchive unarchive: format: binary parts: [] Unarchives messages according to the selected archive format into multiple messages within a batch. Supported archive formats are: tar , zip , binary , lines , json_documents and json_array . When a message is unarchived the new messages replaces the original message in the batch. Messages that are selected but fail to unarchive (invalid format) will remain unchanged in the message batch but will be flagged as having failed. The json_documents format attempts to parse the message as a stream of concatenated JSON documents. Each parsed document is expanded into a new message. The json_array format attempts to parse the message as a JSON array and for each element of the array expands its contents into a new message. For the unarchive formats that contain file information (tar, zip), a metadata field is added to each message called archive_filename with the extracted filename. while type: while while: at_least_once: false condition: type: text text: arg: operator: equals_cs part: 0 max_loops: 0 processors: [] While is a processor that has a condition and a list of child processors. The child processors are executed continously on a message batch for as long as the child condition resolves to true. The field at_least_once , if true, ensures that the child processors are always executed at least one time (like a do .. while loop.) The field max_loops , if greater than zero, caps the number of loops for a message batch to this value. If following a loop execution the number of messages in a batch is reduced to zero the loop is exited regardless of the condition result. If following a loop execution there are more than 1 message batches the condition is checked against the first batch only. You can find a full list of conditions here .","title":"Processors"},{"location":"processors/#processors","text":"This document was generated with benthos --list-processors . Benthos processors are functions applied to messages passing through a pipeline. The function signature allows a processor to mutate or drop messages depending on the content of the message. Processors are set via config, and depending on where in the config they are placed they will be run either immediately after a specific input (set in the input section), on all messages (set in the pipeline section) or before a specific output (set in the output section). Most processors apply to all messages and can be placed in the pipeline section: pipeline: threads: 1 processors: - type: foo foo: bar: baz The threads field in the pipeline section determines how many parallel processing threads are created. You can read more about parallel processing in the pipeline guide . By organising processors you can configure complex behaviours in your pipeline. You can find some examples here .","title":"Processors"},{"location":"processors/#error-handling","text":"Some processors have conditions whereby they might fail. Benthos has mechanisms for detecting and recovering from these failures which can be read about here .","title":"Error Handling"},{"location":"processors/#batching-and-multiple-part-messages","text":"All Benthos processors support multiple part messages, which are synonymous with batches. Some processors such as batch and split are able to create, expand and break down batches. Many processors are able to perform their behaviours on specific parts of a message batch, or on all parts, and have a field parts for specifying an array of part indexes they should apply to. If the list of target parts is empty these processors will be applied to all message parts. Part indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if part = -1 then the selected part will be the last part of the message, if part = -2 then the part before the last element will be selected, and so on. Some processors such as filter and dedupe act across an entire batch, when instead we'd like to perform them on individual messages of a batch. In this case the process_batch processor can be used.","title":"Batching and Multiple Part Messages"},{"location":"processors/#contents","text":"archive awk batch bounds_check cache catch compress conditional decode decompress dedupe encode filter filter_parts grok group_by group_by_value hash hash_sample http insert_part jmespath json lambda log merge_json metadata metric noop process_batch process_dag process_field process_map sample select_parts sleep split subprocess switch text throttle try unarchive while","title":"Contents"},{"location":"processors/#archive","text":"type: archive archive: format: binary path: ${!count:files}-${!timestamp_unix_nano}.txt Archives all the messages of a batch into a single message according to the selected archive format. Supported archive formats are: tar , zip , binary , lines and json_array . Some archive formats (such as tar, zip) treat each archive item (message part) as a file with a path. Since message parts only contain raw data a unique path must be generated for each part. This can be done by using function interpolations on the 'path' field as described here . For types that aren't file based (such as binary) the file field is ignored. The json_array format attempts to JSON parse each message and append the result to an array, which becomes the contents of the resulting message. The resulting archived message adopts the metadata of the first message part of the batch.","title":"archive"},{"location":"processors/#awk","text":"type: awk awk: codec: text parts: [] program: BEGIN { x = 0 } { print $0, x; x++ } Executes an AWK program on messages by feeding contents as the input based on a codec and replaces the contents with the result. If the result is empty (nothing is printed by the program) then the original message contents remain unchanged. Comes with a wide range of custom functions for accessing message metadata, json fields, printing logs, etc. These functions can be overridden by functions within the program.","title":"awk"},{"location":"processors/#codecs","text":"A codec can be specified that determines how the contents of the message are fed into the program. This does not change the custom functions. none An empty string is fed into the program. Functions can still be used in order to extract and mutate metadata and message contents. This is useful for when your program only uses functions and doesn't need the full text of the message to be parsed by the program. text The full contents of the message are fed into the program as a string, allowing you to reference tokenised segments of the message with variables ($0, $1, etc). Custom functions can still be used with this codec. This is the default codec as it behaves most similar to typical usage of the awk command line tool. json No contents are fed into the program. Instead, variables are extracted from the message by walking the flattened JSON structure. Each value is converted into a variable by taking its full path, e.g. the object: { foo : { bar : { value : 10 }, created_at : 2018-12-18T11:57:32 } } Would result in the following variable declarations: foo_bar_value = 10 foo_created_at = 2018-12-18T11:57:32 Custom functions can also still be used with this codec.","title":"Codecs"},{"location":"processors/#batch","text":"type: batch batch: byte_size: 0 condition: type: static static: false count: 0 period: Reads a number of discrete messages, buffering (but not acknowledging) the message parts until either: The byte_size field is non-zero and the total size of the batch in bytes matches or exceeds it. The count field is non-zero and the total number of messages in the batch matches or exceeds it. A message added to the batch causes the condition to resolve true . The period field is non-empty and the time since the last batch exceeds its value. Once one of these events trigger the parts are combined into a single batch of messages and sent through the pipeline. After reaching a destination the acknowledgment is sent out for all messages inside the batch at the same time, preserving at-least-once delivery guarantees. The period field - when non-empty - defines a period of time whereby a batch is sent even if the byte_size has not yet been reached. Batch parameters are only triggered when a message is added, meaning a pending batch can last beyond this period if no messages are added since the period was reached. When a batch is sent to an output the behaviour will differ depending on the protocol. If the output type supports multipart messages then the batch is sent as a single message with multiple parts. If the output only supports single part messages then the parts will be sent as a batch of single part messages. If the output supports neither multipart or batches of messages then Benthos falls back to sending them individually. If a Benthos stream contains multiple brokered inputs or outputs then the batch operator should always be applied directly after an input in order to avoid unexpected behaviour and message ordering.","title":"batch"},{"location":"processors/#bounds_check","text":"type: bounds_check bounds_check: max_part_size: 1.073741824e+09 max_parts: 100 min_part_size: 1 min_parts: 1 Checks whether each message batch fits within certain boundaries, and drops batches that do not.","title":"bounds_check"},{"location":"processors/#cache","text":"type: cache cache: cache: key: operator: set parts: [] value: Performs operations against a cache resource for each message of a batch, allowing you to store or retrieve data within message payloads. This processor will interpolate functions within the key and value fields individually for each message of the batch. This allows you to specify dynamic keys and values based on the contents of the message payloads and metadata. You can find a list of functions here .","title":"cache"},{"location":"processors/#operators","text":"","title":"Operators"},{"location":"processors/#set","text":"Set a key in the cache to a value. If the key already exists the contents are overridden.","title":"set"},{"location":"processors/#add","text":"Set a key in the cache to a value. If the key already exists the action fails with a 'key already exists' error, which can be detected with processor error handling .","title":"add"},{"location":"processors/#get","text":"Retrieve the contents of a cached key and replace the original message payload with the result. If the key does not exist the action fails with an error, which can be detected with processor error handling .","title":"get"},{"location":"processors/#examples","text":"The cache processor can be used in combination with other processors in order to solve a variety of data stream problems.","title":"Examples"},{"location":"processors/#deduplication","text":"Deduplication can be done using the add operator with a key extracted from the message payload, since it fails when a key already exists we can remove the duplicates using a processor_failed condition: - type: cache cache: cache: TODO operator: add key: ${!json_field:message.id} value: storeme - type: filter_parts filter_parts: type: processor_failed","title":"Deduplication"},{"location":"processors/#hydration","text":"It's possible to enrich payloads with content previously stored in a cache by using the process_dag processor: - type: process_map process_map: processors: - type: cache cache: cache: TODO operator: get key: ${!json_field:message.document_id} postmap: message.document: .","title":"Hydration"},{"location":"processors/#catch","text":"type: catch catch: [] Behaves similarly to the process_batch processor, where a list of child processors are applied to individual messages of a batch. However, processors are only applied to messages that failed a processing step prior to the catch. For example, with the following config: - type: foo - type: catch catch: - type: bar - type: baz If the processor foo fails for a particular message, that message will be fed into the processors bar and baz . Messages that do not fail for the processor foo will skip these processors. When messages leave the catch block their fail flags are cleared. This processor is useful for when it's possible to recover failed messages, or when special actions (such as logging/metrics) are required before dropping them. More information about error handing can be found here .","title":"catch"},{"location":"processors/#compress","text":"type: compress compress: algorithm: gzip level: -1 parts: [] Compresses messages according to the selected algorithm. Supported compression algorithms are: gzip, zlib, flate. The 'level' field might not apply to all algorithms.","title":"compress"},{"location":"processors/#conditional","text":"type: conditional conditional: condition: type: text text: arg: operator: equals_cs part: 0 else_processors: [] processors: [] Conditional is a processor that has a list of child processors , else_processors , and a condition. For each message batch, if the condition passes, the child processors will be applied, otherwise the else_processors are applied. This processor is useful for applying processors based on the content of message batches. In order to conditionally process each message of a batch individually use this processor with the process_batch processor. You can find a full list of conditions here .","title":"conditional"},{"location":"processors/#decode","text":"type: decode decode: parts: [] scheme: base64 Decodes messages according to the selected scheme. Supported available schemes are: base64.","title":"decode"},{"location":"processors/#decompress","text":"type: decompress decompress: algorithm: gzip parts: [] Decompresses messages according to the selected algorithm. Supported decompression types are: gzip, zlib, bzip2, flate.","title":"decompress"},{"location":"processors/#dedupe","text":"type: dedupe dedupe: cache: drop_on_err: true hash: none key: parts: - 0 Dedupes message batches by caching selected (and optionally hashed) messages, dropping batches that are already cached. The hash type can be chosen from: none or xxhash. This processor acts across an entire batch, in order to deduplicate individual messages within a batch use this processor with the process_batch processor. Optionally, the key field can be populated in order to hash on a function interpolated string rather than the full contents of messages. This allows you to deduplicate based on dynamic fields within a message, such as its metadata, JSON fields, etc. A full list of interpolation functions can be found here . For example, the following config would deduplicate based on the concatenated values of the metadata field kafka_key and the value of the JSON path id within the message contents: dedupe: cache: foocache key: ${!metadata:kafka_key}-${!json_field:id} Caches should be configured as a resource, for more information check out the documentation here . When using this processor with an output target that might fail you should always wrap the output within a retry block. This ensures that during outages your messages aren't reprocessed after failures, which would result in messages being dropped.","title":"dedupe"},{"location":"processors/#delivery-guarantees","text":"Performing deduplication on a stream using a distributed cache voids any at-least-once guarantees that it previously had. This is because the cache will preserve message signatures even if the message fails to leave the Benthos pipeline, which would cause message loss in the event of an outage at the output sink followed by a restart of the Benthos instance. If you intend to preserve at-least-once delivery guarantees you can avoid this problem by using a memory based cache. This is a compromise that can achieve effective deduplication but parallel deployments of the pipeline as well as service restarts increase the chances of duplicates passing undetected.","title":"Delivery Guarantees"},{"location":"processors/#encode","text":"type: encode encode: parts: [] scheme: base64 Encodes messages according to the selected scheme. Supported schemes are: base64.","title":"encode"},{"location":"processors/#filter","text":"type: filter filter: type: text text: arg: operator: equals_cs part: 0 Tests each message batch against a condition, if the condition fails then the batch is dropped. You can find a full list of conditions here . In order to filter individual messages of a batch use the filter_parts processor.","title":"filter"},{"location":"processors/#filter_parts","text":"type: filter_parts filter_parts: type: text text: arg: operator: equals_cs part: 0 Tests each individual message of a batch against a condition, if the condition fails then the message is dropped. If the resulting batch is empty it will be dropped. You can find a full list of conditions here , in this case each condition will be applied to a message as if it were a single message batch. This processor is useful if you are combining messages into batches using the batch processor and wish to remove specific parts.","title":"filter_parts"},{"location":"processors/#grok","text":"type: grok grok: named_captures_only: true output_format: json parts: [] patterns: [] remove_empty_values: true use_default_patterns: true Parses message payloads by attempting to apply a list of Grok patterns, if a pattern returns at least one value a resulting structured object is created according to the chosen output format and will replace the payload. Currently only json is a valid output format. This processor respects type hints in the grok patterns, therefore with the pattern %{WORD:first},%{INT:second:int} and a payload of foo,1 the resulting payload would be {\"first\":\"foo\",\"second\":1} .","title":"grok"},{"location":"processors/#group_by","text":"type: group_by group_by: [] Splits a batch of messages into N batches, where each resulting batch contains a group of messages determined by conditions that are applied per message of the original batch. Once the groups are established a list of processors are applied to their respective grouped batch, which can be used to label the batch as per their grouping. Each group is configured in a list with a condition and a list of processors: type: group_by group_by: - condition: type: static static: true processors: - type: noop Messages are added to the first group that passes and can only belong to a single group. Messages that do not pass the conditions of any group are placed in a final batch with no processors applied. For example, imagine we have a batch of messages that we wish to split into two groups - the foos and the bars - which should be sent to different output destinations based on those groupings. We also need to send the foos as a tar gzip archive. For this purpose we can use the group_by processor with a switch output: pipeline: processors: - type: group_by group_by: - condition: type: text text: operator: contains arg: this is a foo processors: - type: archive archive: format: tar - type: compress compress: algorithm: gzip - type: metadata metadata: operator: set key: grouping value: foo output: type: switch switch: outputs: - output: type: foo_output condition: type: metadata metadata: operator: equals key: grouping arg: foo - output: type: bar_output Since any message that isn't a foo is a bar, and bars do not require their own processing steps, we only need a single grouping configuration.","title":"group_by"},{"location":"processors/#group_by_value","text":"type: group_by_value group_by_value: value: ${!metadata:example} Splits a batch of messages into N batches, where each resulting batch contains a group of messages determined by a function interpolated string evaluated per message. This allows you to group messages using arbitrary fields within their content or metadata, process them individually, and send them to unique locations as per their group. For example, if we were consuming Kafka messages and needed to group them by their key, archive the groups, and send them to S3 with the key as part of the path we could achieve that with the following: pipeline: processors: - type: group_by_value group_by_value: value: ${!metadata:kafka_key} - type: archive archive: format: tar - type: compress compress: algorithm: gzip output: type: s3 s3: bucket: TODO path: docs/${!metadata:kafka_key}/${!count:files}-${!timestamp_unix_nano}.tar.gz","title":"group_by_value"},{"location":"processors/#hash","text":"type: hash hash: algorithm: sha256 parts: [] Hashes messages according to the selected algorithm. Supported algorithms are: sha256, sha512, xxhash64. This processor is mostly useful when combined with the process_field processor as it allows you to hash a specific field of a document like this: # Hash the contents of 'foo.bar' process_field: path: foo.bar processors: - type: hash hash: algorithm: sha256","title":"hash"},{"location":"processors/#hash_sample","text":"type: hash_sample hash_sample: parts: - 0 retain_max: 10 retain_min: 0 Retains a percentage of message batches deterministically by hashing selected messages and checking the hash against a valid range, dropping all others. For example, setting retain_min to 0.0 and remain_max to 50.0 results in dropping half of the input stream, and setting retain_min to 50.0 and retain_max to 100.1 will drop the other half. In order to sample individual messages of a batch use this processor with the process_batch processor.","title":"hash_sample"},{"location":"processors/#http","text":"type: http http: max_parallel: 0 parallel: false request: backoff_on: - 429 basic_auth: enabled: false password: username: drop_on: [] headers: Content-Type: application/octet-stream max_retry_backoff: 300s oauth: access_token: access_token_secret: consumer_key: consumer_secret: enabled: false request_url: rate_limit: retries: 3 retry_period: 1s timeout: 5s tls: client_certs: [] enabled: false root_cas_file: skip_cert_verify: false url: http://localhost:4195/post verb: POST Performs an HTTP request using a message batch as the request body, and replaces the original message parts with the body of the response. If the batch contains only a single message part then it will be sent as the body of the request. If the batch contains multiple messages then they will be sent as a multipart HTTP request using a Content-Type: multipart header. If you are sending batches and wish to avoid this behaviour then you can set the parallel flag to true and the messages of a batch will be sent as individual requests in parallel. You can also cap the max number of parallel requests with max_parallel . Alternatively, you can use the archive processor to create a single message from the batch. The rate_limit field can be used to specify a rate limit resource to cap the rate of requests across all parallel components service wide. The URL and header values of this type can be dynamically set using function interpolations described here . In order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the process_map or process_field processors.","title":"http"},{"location":"processors/#error-handling_1","text":"When all retry attempts for a message are exhausted the processor cancels the attempt. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about these patterns here .","title":"Error Handling"},{"location":"processors/#insert_part","text":"type: insert_part insert_part: content: index: -1 Insert a new message into a batch at an index. If the specified index is greater than the length of the existing batch it will be appended to the end. The index can be negative, and if so the message will be inserted from the end counting backwards starting from -1. E.g. if index = -1 then the new message will become the last of the batch, if index = -2 then the new message will be inserted before the last message, and so on. If the negative index is greater than the length of the existing batch it will be inserted at the beginning. This processor will interpolate functions within the 'content' field, you can find a list of functions here .","title":"insert_part"},{"location":"processors/#jmespath","text":"type: jmespath jmespath: parts: [] query: Parses a message as a JSON document and attempts to apply a JMESPath expression to it, replacing the contents of the part with the result. Please refer to the JMESPath website for information and tutorials regarding the syntax of expressions. For example, with the following config: jmespath: query: locations[?state == 'WA'].name | sort(@) | {Cities: join(', ', @)} If the initial contents of a message were: { locations : [ { name : Seattle , state : WA }, { name : New York , state : NY }, { name : Bellevue , state : WA }, { name : Olympia , state : WA } ] } Then the resulting contents would be: { Cities : Bellevue, Olympia, Seattle } It is possible to create boolean queries with JMESPath, in order to filter messages with boolean queries please instead use the jmespath condition.","title":"jmespath"},{"location":"processors/#json","text":"type: json json: operator: get parts: [] path: value: Parses messages as a JSON document, performs a mutation on the data, and then overwrites the previous contents with the new value. If the path is empty or \".\" the root of the data will be targeted. This processor will interpolate functions within the 'value' field, you can find a list of functions here .","title":"json"},{"location":"processors/#operators_1","text":"","title":"Operators"},{"location":"processors/#append","text":"Appends a value to an array at a target dot path. If the path does not exist all objects in the path are created (unless there is a collision). If a non-array value already exists in the target path it will be replaced by an array containing the original value as well as the new value. If the value is an array the elements of the array are expanded into the new array. E.g. if the target is an array [0,1] and the value is also an array [2,3] , the result will be [0,1,2,3] as opposed to [0,1,[2,3]] .","title":"append"},{"location":"processors/#clean","text":"Walks the JSON structure and deletes any fields where the value is: An empty array An empty object An empty string null","title":"clean"},{"location":"processors/#copy","text":"Copies the value of a target dot path (if it exists) to a location. The destination path is specified in the value field. If the destination path does not exist all objects in the path are created (unless there is a collision).","title":"copy"},{"location":"processors/#delete","text":"Removes a key identified by the dot path. If the path does not exist this is a no-op.","title":"delete"},{"location":"processors/#move","text":"Moves the value of a target dot path (if it exists) to a new location. The destination path is specified in the value field. If the destination path does not exist all objects in the path are created (unless there is a collision).","title":"move"},{"location":"processors/#select","text":"Reads the value found at a dot path and replaced the original contents entirely by the new value.","title":"select"},{"location":"processors/#set_1","text":"Sets the value of a field at a dot path. If the path does not exist all objects in the path are created (unless there is a collision). The value can be any type, including objects and arrays. When using YAML configuration files a YAML object will be converted into a JSON object, i.e. with the config: json: operator: set parts: [0] path: some.path value: foo: bar: 5 The value will be converted into '{\"foo\":{\"bar\":5}}'. If the YAML object contains keys that aren't strings those fields will be ignored.","title":"set"},{"location":"processors/#lambda","text":"type: lambda lambda: credentials: id: role: role_external_id: secret: token: endpoint: function: parallel: false rate_limit: region: eu-west-1 retries: 3 timeout: 5s Invokes an AWS lambda for each message part of a batch. The contents of the message part is the payload of the request, and the result of the invocation will become the new contents of the message. It is possible to perform requests per message of a batch in parallel by setting the parallel flag to true . The rate_limit field can be used to specify a rate limit resource to cap the rate of requests across parallel components service wide. In order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the process_map or process_field processors.","title":"lambda"},{"location":"processors/#error-handling_2","text":"When all retry attempts for a message are exhausted the processor cancels the attempt. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about these patterns here .","title":"Error Handling"},{"location":"processors/#log","text":"type: log log: fields: {} level: INFO message: Log is a processor that prints a log event each time it processes a message. The message is then sent onwards unchanged. The log message can be set using function interpolations described here which allows you to log the contents and metadata of a message. For example, if we wished to create a debug log event for each message in a pipeline in order to expose the JSON field foo.bar as well as the metadata field kafka_partition we can achieve that with the following config: type: log log: level: DEBUG message: field: ${!json_field:foo.bar}, part: ${!metadata:kafka_partition} The level field determines the log level of the printed events and can be any of the following values: TRACE, DEBUG, INFO, WARN, ERROR.","title":"log"},{"location":"processors/#structured-fields","text":"It's also possible to output a map of structured fields, this only works when the service log is set to output as JSON. The field values are function interpolated, meaning it's possible to output structured fields containing message contents and metadata, e.g.: type: log log: level: DEBUG message: foo fields: id: ${!json_field:id} kafka_topic: ${!metadata:kafka_topic}","title":"Structured Fields"},{"location":"processors/#merge_json","text":"type: merge_json merge_json: parts: [] retain_parts: false Parses selected messages of a batch as JSON documents, attempts to merge them into one single JSON document and then writes it to a new message at the end of the batch. Merged parts are removed unless retain_parts is set to true. The new merged message will contain the metadata of the first part to be merged.","title":"merge_json"},{"location":"processors/#metadata","text":"type: metadata metadata: key: example operator: set parts: [] value: ${!hostname} Performs operations on the metadata of a message. Metadata are key/value pairs that are associated with message parts of a batch. Metadata values can be referred to using configuration interpolation functions , which allow you to set fields in certain outputs using these dynamic values. This processor will interpolate functions within the value field, you can find a list of functions here . This allows you to set the contents of a metadata field using values taken from the message payload.","title":"metadata"},{"location":"processors/#operators_2","text":"","title":"Operators"},{"location":"processors/#set_2","text":"Sets the value of a metadata key.","title":"set"},{"location":"processors/#delete_all","text":"Removes all metadata values from the message.","title":"delete_all"},{"location":"processors/#delete_prefix","text":"Removes all metadata values from the message where the key is prefixed with the value provided.","title":"delete_prefix"},{"location":"processors/#metric","text":"type: metric metric: labels: {} path: type: counter value: Creates metrics by extracting values from messages. The path field should be a dot separated path of the metric to be set and will automatically be converted into the correct format of the configured metric aggregator. The value field can be set using function interpolations described here and is used according to the specific type.","title":"metric"},{"location":"processors/#types","text":"","title":"Types"},{"location":"processors/#counter","text":"Increments a counter by exactly 1, the contents of value are ignored by this type.","title":"counter"},{"location":"processors/#counter_parts","text":"Increments a counter by the number of parts within the message batch, the contents of value are ignored by this type.","title":"counter_parts"},{"location":"processors/#counter_by","text":"If the contents of value can be parsed as a positive integer value then the counter is incremented by this value. For example, the following configuration will increment the value of the count.custom.field metric by the contents of field.some.value : type: metric metric: type: counter_by path: count.custom.field value: ${!json_field:field.some.value}","title":"counter_by"},{"location":"processors/#gauge","text":"If the contents of value can be parsed as a positive integer value then the gauge is set to this value. For example, the following configuration will set the value of the gauge.custom.field metric to the contents of field.some.value : type: metric metric: type: gauge path: gauge.custom.field value: ${!json_field:field.some.value}","title":"gauge"},{"location":"processors/#timing","text":"Equivalent to gauge where instead the metric is a timing.","title":"timing"},{"location":"processors/#labels","text":"Some metrics aggregators, such as Prometheus, support arbitrary labels, in which case the labels field can be used in order to create them. Label values can also be set using function interpolations in order to dynamically populate them with context about the message.","title":"Labels"},{"location":"processors/#noop","text":"type: noop Noop is a no-op processor that does nothing, the message passes through unchanged.","title":"noop"},{"location":"processors/#process_batch","text":"type: process_batch process_batch: [] A processor that applies a list of child processors to messages of a batch as though they were each a batch of one message. This is useful for forcing batch wide processors such as dedupe to apply to individual message parts of a batch instead. Please note that most processors already process per message of a batch, and this processor is not needed in those cases.","title":"process_batch"},{"location":"processors/#process_dag","text":"type: process_dag process_dag: {} A processor that manages a map of process_map processors and calculates a Directed Acyclic Graph (DAG) of their dependencies by referring to their postmap targets for provided fields and their premap targets for required fields. The DAG is then used to execute the children in the necessary order with the maximum parallelism possible. The field dependencies is an optional array of fields that a child depends on. This is useful for when fields are required but don't appear within a premap such as those used in conditions. This processor is extremely useful for performing a complex mesh of enrichments where network requests mean we desire maximum parallelism across those enrichments. For example, if we had three target HTTP services that we wished to enrich each document with - foo, bar and baz - where baz relies on the result of both foo and bar, we might express that relationship here like so: type: process_dag process_dag: foo: premap: .: . processors: - type: http http: request: url: http://foo/enrich postmap: foo_result: . bar: premap: .: msg.sub.path processors: - type: http http: request: url: http://bar/enrich postmap: bar_result: . baz: premap: foo_obj: foo_result bar_obj: bar_result processors: - type: http http: request: url: http://baz/enrich postmap: baz_obj: . With this config the DAG would determine that the children foo and bar can be executed in parallel, and once they are both finished we may proceed onto baz.","title":"process_dag"},{"location":"processors/#process_field","text":"type: process_field process_field: parts: [] path: processors: [] A processor that extracts the value of a field within payloads (currently only JSON format is supported) then applies a list of processors to the extracted value, and finally sets the field within the original payloads to the processed result. If the number of messages resulting from the processing steps does not match the original count then this processor fails and the messages continue unchanged. Therefore, you should avoid using batch and filter type processors in this list.","title":"process_field"},{"location":"processors/#process_map","text":"type: process_map process_map: conditions: [] parts: [] postmap: {} postmap_optional: {} premap: {} premap_optional: {} processors: [] A processor that extracts and maps fields from the original payload into new objects, applies a list of processors to the newly constructed objects, and finally maps the result back into the original payload. This processor is useful for performing processors on subsections of a payload. For example, you could extract sections of a JSON object in order to construct a request object for an http processor, then map the result back into a field within the original object. The order of stages of this processor are as follows: Conditions are applied to each individual message part in the batch, determining whether the part will be mapped. If the conditions are empty all message parts will be mapped. If the field parts is populated the message parts not in this list are also excluded from mapping. Message parts that are flagged for mapping are mapped according to the premap fields, creating a new object. If the premap stage fails (targets are not found) the message part will not be processed. Message parts that are mapped are processed as a batch. You may safely break the batch into individual parts during processing with the split processor. After all child processors are applied to the mapped messages they are mapped back into the original message parts they originated from as per your postmap. If the postmap stage fails the mapping is skipped and the message payload remains as it started. Map paths are arbitrary dot paths, target path hierarchies are constructed if they do not yet exist. Processing is skipped for message parts where the premap targets aren't found, for optional premap targets use premap_optional . Map target paths that are parents of other map target paths will always be mapped first, therefore it is possible to map subpath overrides. If postmap targets are not found the merge is abandoned, for optional postmap targets use postmap_optional . If the premap is empty then the full payload is sent to the processors, if the postmap is empty then the processed result replaces the original contents entirely. Maps can reference the root of objects either with an empty string or '.', for example the maps: premap: .: foo.bar postmap: foo.bar: . Would create a new object where the root is the value of foo.bar and would map the full contents of the result back into foo.bar . If the number of total message parts resulting from the processing steps does not match the original count then this processor fails and the messages continue unchanged. Therefore, you should avoid using batch and filter type processors in this list.","title":"process_map"},{"location":"processors/#batch-ordering","text":"This processor supports batch messages. When message parts are post-mapped after processing they will be correctly aligned with the original batch. However, the ordering of premapped message parts as they are sent through processors are not guaranteed to match the ordering of the original batch.","title":"Batch Ordering"},{"location":"processors/#sample","text":"type: sample sample: retain: 10 seed: 0 Retains a randomly sampled percentage of message batches (0 to 100) and drops all others. The random seed is static in order to sample deterministically, but can be set in config to allow parallel samples that are unique.","title":"sample"},{"location":"processors/#select_parts","text":"type: select_parts select_parts: parts: - 0 Cherry pick a set of messages from a batch by their index. Indexes larger than the number of messages are simply ignored. The selected parts are added to the new message batch in the same order as the selection array. E.g. with 'parts' set to [ 2, 0, 1 ] and the message parts [ '0', '1', '2', '3' ], the output will be [ '2', '0', '1' ]. If none of the selected parts exist in the input batch (resulting in an empty output message) the batch is dropped entirely. Message indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if index = -1 then the selected part will be the last part of the message, if index = -2 then the part before the last element with be selected, and so on.","title":"select_parts"},{"location":"processors/#sleep","text":"type: sleep sleep: duration: 100us Sleep for a period of time specified as a duration string. This processor will interpolate functions within the duration field, you can find a list of functions here .","title":"sleep"},{"location":"processors/#split","text":"type: split split: byte_size: 0 size: 1 Breaks message batches (synonymous with multiple part messages) into smaller batches. The size of the resulting batches are determined either by a discrete size or, if the field byte_size is non-zero, then by total size in bytes (which ever limit is reached first). If there is a remainder of messages after splitting a batch the remainder is also sent as a single batch. For example, if your target size was 10, and the processor received a batch of 95 message parts, the result would be 9 batches of 10 messages followed by a batch of 5 messages.","title":"split"},{"location":"processors/#subprocess","text":"type: subprocess subprocess: args: [] name: cat parts: [] Subprocess is a processor that runs a process in the background and, for each message, will pipe its contents to the stdin stream of the process followed by a newline. The subprocess must then either return a line over stdout or stderr. If a response is returned over stdout then its contents will replace the message. If a response is instead returned from stderr will be logged and the message will continue unchanged and will be marked as failed.","title":"subprocess"},{"location":"processors/#subprocess-requirements","text":"It is required that subprocesses flush their stdout and stderr pipes for each line. Benthos will attempt to keep the process alive for as long as the pipeline is running. If the process exits early it will be restarted.","title":"Subprocess requirements"},{"location":"processors/#messages-containing-line-breaks","text":"If a message contains line breaks each line of the message is piped to the subprocess and flushed, and a response is expected from the subprocess before another line is fed in.","title":"Messages containing line breaks"},{"location":"processors/#switch","text":"type: switch switch: [] Switch is a processor that lists child case objects each containing a condition and processors. Each batch of messages is tested against the condition of each child case until a condition passes, whereby the processors of that case will be executed on the batch. Each case may specify a boolean fallthrough field indicating whether the next case should be executed after it (the default is false .) A case takes this form: - condition: type: foo processors: - type: foo fallthrough: false In order to switch each message of a batch individually use this processor with the process_batch processor. You can find a full list of conditions here .","title":"switch"},{"location":"processors/#text","text":"type: text text: arg: operator: trim_space parts: [] value: Performs text based mutations on payloads. This processor will interpolate functions within the value field, you can find a list of functions here .","title":"text"},{"location":"processors/#operators_3","text":"","title":"Operators"},{"location":"processors/#append_1","text":"Appends text to the end of the payload.","title":"append"},{"location":"processors/#escape_url_query","text":"Escapes text so that it is safe to place within the query section of a URL.","title":"escape_url_query"},{"location":"processors/#unescape_url_query","text":"Unescapes text that has been url escaped.","title":"unescape_url_query"},{"location":"processors/#find_regexp","text":"Extract the matching section of the argument regular expression in a message.","title":"find_regexp"},{"location":"processors/#prepend","text":"Prepends text to the beginning of the payload.","title":"prepend"},{"location":"processors/#replace","text":"Replaces all occurrences of the argument in a message with a value.","title":"replace"},{"location":"processors/#replace_regexp","text":"Replaces all occurrences of the argument regular expression in a message with a value. Inside the value $ signs are interpreted as submatch expansions, e.g. $1 represents the text of the first submatch.","title":"replace_regexp"},{"location":"processors/#set_3","text":"Replace the contents of a message entirely with a value.","title":"set"},{"location":"processors/#strip_html","text":"Removes all HTML tags from a message.","title":"strip_html"},{"location":"processors/#to_lower","text":"Converts all text into lower case.","title":"to_lower"},{"location":"processors/#to_upper","text":"Converts all text into upper case.","title":"to_upper"},{"location":"processors/#trim","text":"Removes all leading and trailing occurrences of characters within the arg field.","title":"trim"},{"location":"processors/#trim_space","text":"Removes all leading and trailing whitespace from the payload.","title":"trim_space"},{"location":"processors/#throttle","text":"type: throttle throttle: period: 100us Throttles the throughput of a pipeline to a maximum of one message batch per period. This throttle is per processing pipeline, and therefore four threads each with a throttle would result in four times the rate specified. The period should be specified as a time duration string. For example, '1s' would be 1 second, '10ms' would be 10 milliseconds, etc.","title":"throttle"},{"location":"processors/#try","text":"type: try try: [] Behaves similarly to the process_batch processor, where a list of child processors are applied to individual messages of a batch. However, if a processor fails for a message then that message will skip all following processors. For example, with the following config: - type: try try: - type: foo - type: bar - type: baz If the processor foo fails for a particular message, that message will skip the processors bar and baz . This processor is useful for when child processors depend on the successful output of previous processors. This processor can be followed with a catch processor for defining child processors to be applied only to failed messages. More information about error handing can be found here .","title":"try"},{"location":"processors/#unarchive","text":"type: unarchive unarchive: format: binary parts: [] Unarchives messages according to the selected archive format into multiple messages within a batch. Supported archive formats are: tar , zip , binary , lines , json_documents and json_array . When a message is unarchived the new messages replaces the original message in the batch. Messages that are selected but fail to unarchive (invalid format) will remain unchanged in the message batch but will be flagged as having failed. The json_documents format attempts to parse the message as a stream of concatenated JSON documents. Each parsed document is expanded into a new message. The json_array format attempts to parse the message as a JSON array and for each element of the array expands its contents into a new message. For the unarchive formats that contain file information (tar, zip), a metadata field is added to each message called archive_filename with the extracted filename.","title":"unarchive"},{"location":"processors/#while","text":"type: while while: at_least_once: false condition: type: text text: arg: operator: equals_cs part: 0 max_loops: 0 processors: [] While is a processor that has a condition and a list of child processors. The child processors are executed continously on a message batch for as long as the child condition resolves to true. The field at_least_once , if true, ensures that the child processors are always executed at least one time (like a do .. while loop.) The field max_loops , if greater than zero, caps the number of loops for a message batch to this value. If following a loop execution the number of messages in a batch is reduced to zero the loop is exited regardless of the condition result. If following a loop execution there are more than 1 message batches the condition is checked against the first batch only. You can find a full list of conditions here .","title":"while"},{"location":"processors/awk_functions/","text":"AWK Functions The Benthos AWK processor comes with custom functions listed in this document. These functions can be overridden by functions in the program. Contents JSON Functions Metadata Functions Date Functions Logging Functions JSON Functions json_get(path) Attempts to find a JSON value in the input message payload by a dot separated path and returns it as a string. This function is always available even when the json codec is not used. json_set(path, value) Attempts to set a JSON value in the input message payload identified by a dot separated path. This function is always available even when the json codec is not used. create_json_object(key1, val1, key2, val2, ...) Generates a valid JSON object of key value pair arguments. The arguments are variadic, meaning any number of pairs can be listed. The value will always resolve to a string regardless of the value type. E.g. the following call: create_json_object(\"a\", \"1\", \"b\", 2, \"c\", \"3\") Would result in this string: {\"a\":\"1\",\"b\":\"2\",\"c\":\"3\"} create_json_array(val1, val2, ...) Generates a valid JSON array of value arguments. The arguments are variadic, meaning any number of values can be listed. The value will always resolve to a string regardless of the value type. E.g. the following call: create_json_array(\"1\", 2, \"3\") Would result in this string: [\"1\",\"2\",\"3\"] Metadata Functions metadata_set(key, value) Set a metadata key for the message to a value. The value will always resolve to a string regardless of the value type. metadata_get(key) string Get the value of a metadata key from the message. Date Functions timestamp_unix() int Returns the current unix timestamp (the number of seconds since 01-01-1970). timestamp_unix(date) int Attempts to parse a date string by detecting its format and returns the equivalent unix timestamp (the number of seconds since 01-01-1970). timestamp_unix(date, format) int Attempts to parse a date string according to a format and returns the equivalent unix timestamp (the number of seconds since 01-01-1970). The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. timestamp_unix_nano() int Returns the current unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970). timestamp_unix_nano(date) int Attempts to parse a date string by detecting its format and returns the equivalent unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970). timestamp_unix_nano(date, format) int Attempts to parse a date string according to a format and returns the equivalent unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970). The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. timestamp_format(unix, format) string Formats a unix timestamp. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. The format is optional, and if omitted RFC3339 ( 2006-01-02T15:04:05Z07:00 ) will be used. timestamp_format_nano(unixNano, format) string Formats a unix timestamp in nanoseconds. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. The format is optional, and if omitted RFC3339 ( 2006-01-02T15:04:05Z07:00 ) will be used. Logging Functions print_log(message, level) Prints a Benthos log message at a particular log level. The log level is optional, and if omitted the level INFO will be used.","title":"Awk functions"},{"location":"processors/awk_functions/#awk-functions","text":"The Benthos AWK processor comes with custom functions listed in this document. These functions can be overridden by functions in the program.","title":"AWK Functions"},{"location":"processors/awk_functions/#contents","text":"JSON Functions Metadata Functions Date Functions Logging Functions","title":"Contents"},{"location":"processors/awk_functions/#json-functions","text":"","title":"JSON Functions"},{"location":"processors/awk_functions/#json_getpath","text":"Attempts to find a JSON value in the input message payload by a dot separated path and returns it as a string. This function is always available even when the json codec is not used.","title":"json_get(path)"},{"location":"processors/awk_functions/#json_setpath-value","text":"Attempts to set a JSON value in the input message payload identified by a dot separated path. This function is always available even when the json codec is not used.","title":"json_set(path, value)"},{"location":"processors/awk_functions/#create_json_objectkey1-val1-key2-val2","text":"Generates a valid JSON object of key value pair arguments. The arguments are variadic, meaning any number of pairs can be listed. The value will always resolve to a string regardless of the value type. E.g. the following call: create_json_object(\"a\", \"1\", \"b\", 2, \"c\", \"3\") Would result in this string: {\"a\":\"1\",\"b\":\"2\",\"c\":\"3\"}","title":"create_json_object(key1, val1, key2, val2, ...)"},{"location":"processors/awk_functions/#create_json_arrayval1-val2","text":"Generates a valid JSON array of value arguments. The arguments are variadic, meaning any number of values can be listed. The value will always resolve to a string regardless of the value type. E.g. the following call: create_json_array(\"1\", 2, \"3\") Would result in this string: [\"1\",\"2\",\"3\"]","title":"create_json_array(val1, val2, ...)"},{"location":"processors/awk_functions/#metadata-functions","text":"","title":"Metadata Functions"},{"location":"processors/awk_functions/#metadata_setkey-value","text":"Set a metadata key for the message to a value. The value will always resolve to a string regardless of the value type.","title":"metadata_set(key, value)"},{"location":"processors/awk_functions/#metadata_getkey-string","text":"Get the value of a metadata key from the message.","title":"metadata_get(key) string"},{"location":"processors/awk_functions/#date-functions","text":"","title":"Date Functions"},{"location":"processors/awk_functions/#timestamp_unix-int","text":"Returns the current unix timestamp (the number of seconds since 01-01-1970).","title":"timestamp_unix() int"},{"location":"processors/awk_functions/#timestamp_unixdate-int","text":"Attempts to parse a date string by detecting its format and returns the equivalent unix timestamp (the number of seconds since 01-01-1970).","title":"timestamp_unix(date) int"},{"location":"processors/awk_functions/#timestamp_unixdate-format-int","text":"Attempts to parse a date string according to a format and returns the equivalent unix timestamp (the number of seconds since 01-01-1970). The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value.","title":"timestamp_unix(date, format) int"},{"location":"processors/awk_functions/#timestamp_unix_nano-int","text":"Returns the current unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970).","title":"timestamp_unix_nano() int"},{"location":"processors/awk_functions/#timestamp_unix_nanodate-int","text":"Attempts to parse a date string by detecting its format and returns the equivalent unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970).","title":"timestamp_unix_nano(date) int"},{"location":"processors/awk_functions/#timestamp_unix_nanodate-format-int","text":"Attempts to parse a date string according to a format and returns the equivalent unix timestamp in nanoseconds (the number of nanoseconds since 01-01-1970). The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value.","title":"timestamp_unix_nano(date, format) int"},{"location":"processors/awk_functions/#timestamp_formatunix-format-string","text":"Formats a unix timestamp. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. The format is optional, and if omitted RFC3339 ( 2006-01-02T15:04:05Z07:00 ) will be used.","title":"timestamp_format(unix, format) string"},{"location":"processors/awk_functions/#timestamp_format_nanounixnano-format-string","text":"Formats a unix timestamp in nanoseconds. The format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006 would be displayed if it were the value. The format is optional, and if omitted RFC3339 ( 2006-01-02T15:04:05Z07:00 ) will be used.","title":"timestamp_format_nano(unixNano, format) string"},{"location":"processors/awk_functions/#logging-functions","text":"","title":"Logging Functions"},{"location":"processors/awk_functions/#print_logmessage-level","text":"Prints a Benthos log message at a particular log level. The log level is optional, and if omitted the level INFO will be used.","title":"print_log(message, level)"},{"location":"rate_limits/","text":"Rate Limits This document was generated with benthos --list-rate-limits A rate limit is a strategy for limiting the usage of a shared resource across parallel components in a Benthos instance, or potentially across multiple instances. For example, if we wanted to protect an HTTP service with a local rate limit we could configure one like so: input: type: foo pipeline: threads: 8 processors: - type: http http: request: url: http://foo.bar/baz rate_limit: foobar parallel: true resources: rate_limits: foobar: type: local local: count: 500 interval: 1s In this example if the messages from the input foo are batches the requests of a batch will be sent in parallel. This is usually going to be what we want, but could potentially stress our HTTP server if a batch is large. However, by using a rate limit we can guarantee that even across parallel processing pipelines and variable sized batches we wont hit the service more than 500 times per second. Contents local local type: local local: count: 1000 interval: 1s The local rate limit is a simple X every Y type rate limit that can be shared across any number of components within the pipeline.","title":"Rate Limits"},{"location":"rate_limits/#rate-limits","text":"This document was generated with benthos --list-rate-limits A rate limit is a strategy for limiting the usage of a shared resource across parallel components in a Benthos instance, or potentially across multiple instances. For example, if we wanted to protect an HTTP service with a local rate limit we could configure one like so: input: type: foo pipeline: threads: 8 processors: - type: http http: request: url: http://foo.bar/baz rate_limit: foobar parallel: true resources: rate_limits: foobar: type: local local: count: 500 interval: 1s In this example if the messages from the input foo are batches the requests of a batch will be sent in parallel. This is usually going to be what we want, but could potentially stress our HTTP server if a batch is large. However, by using a rate limit we can guarantee that even across parallel processing pipelines and variable sized batches we wont hit the service more than 500 times per second.","title":"Rate Limits"},{"location":"rate_limits/#contents","text":"local","title":"Contents"},{"location":"rate_limits/#local","text":"type: local local: count: 1000 interval: 1s The local rate limit is a simple X every Y type rate limit that can be shared across any number of components within the pipeline.","title":"local"},{"location":"streams/","text":"Streams Mode A Benthos stream consists of four components; an input, an optional buffer, processor pipelines and an output. Under normal use a Benthos instance is a single stream, and these components are configured within the service config file. Alternatively, Benthos can be run in --streams mode, where a single running Benthos instance is able to run multiple entirely isolated streams. Adding streams in this mode can be done in two ways: Static configuration files allows you to maintain a directory of static stream configuration files that will be traversed by Benthos. An HTTP REST API allows you to dynamically create, read the status of, update, and delete streams at runtime. These two methods can be used in combination, i.e. it's possible to update and delete streams that were created with static files.","title":"Home"},{"location":"streams/#streams-mode","text":"A Benthos stream consists of four components; an input, an optional buffer, processor pipelines and an output. Under normal use a Benthos instance is a single stream, and these components are configured within the service config file. Alternatively, Benthos can be run in --streams mode, where a single running Benthos instance is able to run multiple entirely isolated streams. Adding streams in this mode can be done in two ways: Static configuration files allows you to maintain a directory of static stream configuration files that will be traversed by Benthos. An HTTP REST API allows you to dynamically create, read the status of, update, and delete streams at runtime. These two methods can be used in combination, i.e. it's possible to update and delete streams that were created with static files.","title":"Streams Mode"},{"location":"streams/using_REST_API/","text":"Streams Via REST API By using the Benthos --streams mode REST API you can dynamically control which streams are active at runtime. The full spec for the Benthos streams mode REST API can be found here . Note that stream configs created and updated using this API do not benefit from [environment variable interpolation][interpolation] (function interpolation will still work). Walkthrough Start by running Benthos in streams mode: $ benthos --streams On a separate terminal we can add our first stream foo by POST ing a JSON or YAML config to the /streams/foo endpoint: $ curl http://localhost:4195/streams/foo -X POST --data-binary @- EOF input: type: http_server buffer: type: memory pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF Now we can check the full set of streams loaded by GET ing the /streams endpoint: $ curl http://localhost:4195/streams | jq '.' { foo : { active : true, uptime : 7.223545951, uptime_str : 7.223545951s } } Good, now let's add another stream bar the same way: $ curl http://localhost:4195/streams/bar -X POST --data-binary @- EOF input: type: kafka kafka: addresses: - localhost:9092 topic: my_topic buffer: type: none pipeline: threads: 1 processors: - type: sample sample: retain: 10 output: type: elasticsearch elasticsearch: urls: - http://localhost:9200 EOF And check the set again: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 10.121344484, uptime_str : 10.121344484s }, foo : { active : true, uptime : 19.380582951, uptime_str : 19.380583306s } } It's also possible to get the configuration of a loaded stream by GET ing the path /streams/{id} : $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 30.123488951, uptime_str : 30.123488951s config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : memory , memory : { limit : 10000000 } }, ... etc ... } } Next, we might want to update stream foo by PUT ing a new config to the path /streams/foo : $ curl http://localhost:4195/streams/foo -X PUT --data-binary @- EOF input: type: http_server buffer: type: none pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF We have removed the memory buffer with this change, let's check that the config has actually been updated: $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 12.328482951, uptime_str : 12.328482951s config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : none }, ... etc ... } } Good, we are done with stream bar now, so let's delete it by DELETE ing the /streams/bar endpoint: $ curl http://localhost:4195/streams/bar -X DELETE And let's GET the /streams endpoint to see the new set: $ curl http://localhost:4195/streams | jq '.' { foo : { active : true, uptime : 31.872448851, uptime_str : 31.872448851s } } Great. Another useful feature is POST ing to /streams , this allows us to set the entire set of streams with a single request. The payload is a map of stream ids to configurations and this will become the exclusive set of active streams. If there are existing streams that are not on the list they will be removed. $ curl http://localhost:4195/streams -X POST --data-binary @- EOF bar: input: type: http_client http_client: url: http://localhost:4195/baz/get output: type: stdout baz: input: type: http_server output: type: http_server EOF Let's check our new set of streams: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 3.183883444, uptime_str : 3.183883444s }, baz : { active : true, uptime : 3.183883449, uptime_str : 3.183883449s } } Done.","title":"using REST API"},{"location":"streams/using_REST_API/#streams-via-rest-api","text":"By using the Benthos --streams mode REST API you can dynamically control which streams are active at runtime. The full spec for the Benthos streams mode REST API can be found here . Note that stream configs created and updated using this API do not benefit from [environment variable interpolation][interpolation] (function interpolation will still work).","title":"Streams Via REST API"},{"location":"streams/using_REST_API/#walkthrough","text":"Start by running Benthos in streams mode: $ benthos --streams On a separate terminal we can add our first stream foo by POST ing a JSON or YAML config to the /streams/foo endpoint: $ curl http://localhost:4195/streams/foo -X POST --data-binary @- EOF input: type: http_server buffer: type: memory pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF Now we can check the full set of streams loaded by GET ing the /streams endpoint: $ curl http://localhost:4195/streams | jq '.' { foo : { active : true, uptime : 7.223545951, uptime_str : 7.223545951s } } Good, now let's add another stream bar the same way: $ curl http://localhost:4195/streams/bar -X POST --data-binary @- EOF input: type: kafka kafka: addresses: - localhost:9092 topic: my_topic buffer: type: none pipeline: threads: 1 processors: - type: sample sample: retain: 10 output: type: elasticsearch elasticsearch: urls: - http://localhost:9200 EOF And check the set again: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 10.121344484, uptime_str : 10.121344484s }, foo : { active : true, uptime : 19.380582951, uptime_str : 19.380583306s } } It's also possible to get the configuration of a loaded stream by GET ing the path /streams/{id} : $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 30.123488951, uptime_str : 30.123488951s config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : memory , memory : { limit : 10000000 } }, ... etc ... } } Next, we might want to update stream foo by PUT ing a new config to the path /streams/foo : $ curl http://localhost:4195/streams/foo -X PUT --data-binary @- EOF input: type: http_server buffer: type: none pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF We have removed the memory buffer with this change, let's check that the config has actually been updated: $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 12.328482951, uptime_str : 12.328482951s config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : none }, ... etc ... } } Good, we are done with stream bar now, so let's delete it by DELETE ing the /streams/bar endpoint: $ curl http://localhost:4195/streams/bar -X DELETE And let's GET the /streams endpoint to see the new set: $ curl http://localhost:4195/streams | jq '.' { foo : { active : true, uptime : 31.872448851, uptime_str : 31.872448851s } } Great. Another useful feature is POST ing to /streams , this allows us to set the entire set of streams with a single request. The payload is a map of stream ids to configurations and this will become the exclusive set of active streams. If there are existing streams that are not on the list they will be removed. $ curl http://localhost:4195/streams -X POST --data-binary @- EOF bar: input: type: http_client http_client: url: http://localhost:4195/baz/get output: type: stdout baz: input: type: http_server output: type: http_server EOF Let's check our new set of streams: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 3.183883444, uptime_str : 3.183883444s }, baz : { active : true, uptime : 3.183883449, uptime_str : 3.183883449s } } Done.","title":"Walkthrough"},{"location":"streams/using_config_files/","text":"Streams Via Config Files When running Benthos in --streams mode it's possible to create streams with their own static configurations by setting the --streams-dir flag to a directory containing a config file for each stream ( /benthos/streams by default). Note that stream configs loaded in this way can benefit from interpolation . Walkthrough Make a directory of stream configs: $ mkdir ./streams $ cat ./streams/foo.yaml EOF input: type: http_server buffer: type: memory pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF $ cat ./streams/bar.yaml EOF input: type: kafka kafka: addresses: - localhost:9092 topic: my_topic buffer: type: none pipeline: threads: 1 processors: - type: sample sample: retain: 10 output: type: elasticsearch elasticsearch: urls: - http://localhost:9200 EOF Run Benthos in streams mode, pointing to our directory of streams: $ benthos --streams --streams-dir ./streams On a separate terminal you can query the set of streams loaded: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 19.381001424, uptime_str : 19.381001552s }, foo : { active : true, uptime : 19.380582951, uptime_str : 19.380583306s } } You can also query a specific stream to see the loaded configuration: $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 69.334717193, uptime_str : 1m9.334717193s , config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : memory , memory : { limit : 10000000 } }, pipeline : { processors : [ { type : jmespath , jmespath : { parts : [], query : {id: user.id, content: body.content} } } ], threads : 4 }, output : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /get , stream_path : /get/stream , timeout : 5s } } } } There are other endpoints in the REST API for creating, updating and deleting streams.","title":"Using config files"},{"location":"streams/using_config_files/#streams-via-config-files","text":"When running Benthos in --streams mode it's possible to create streams with their own static configurations by setting the --streams-dir flag to a directory containing a config file for each stream ( /benthos/streams by default). Note that stream configs loaded in this way can benefit from interpolation .","title":"Streams Via Config Files"},{"location":"streams/using_config_files/#walkthrough","text":"Make a directory of stream configs: $ mkdir ./streams $ cat ./streams/foo.yaml EOF input: type: http_server buffer: type: memory pipeline: threads: 4 processors: - type: jmespath jmespath: query: {id: user.id, content: body.content} output: type: http_server EOF $ cat ./streams/bar.yaml EOF input: type: kafka kafka: addresses: - localhost:9092 topic: my_topic buffer: type: none pipeline: threads: 1 processors: - type: sample sample: retain: 10 output: type: elasticsearch elasticsearch: urls: - http://localhost:9200 EOF Run Benthos in streams mode, pointing to our directory of streams: $ benthos --streams --streams-dir ./streams On a separate terminal you can query the set of streams loaded: $ curl http://localhost:4195/streams | jq '.' { bar : { active : true, uptime : 19.381001424, uptime_str : 19.381001552s }, foo : { active : true, uptime : 19.380582951, uptime_str : 19.380583306s } } You can also query a specific stream to see the loaded configuration: $ curl http://localhost:4195/streams/foo | jq '.' { active : true, uptime : 69.334717193, uptime_str : 1m9.334717193s , config : { input : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /post , timeout : 5s } }, buffer : { type : memory , memory : { limit : 10000000 } }, pipeline : { processors : [ { type : jmespath , jmespath : { parts : [], query : {id: user.id, content: body.content} } } ], threads : 4 }, output : { type : http_server , http_server : { address : , cert_file : , key_file : , path : /get , stream_path : /get/stream , timeout : 5s } } } } There are other endpoints in the REST API for creating, updating and deleting streams.","title":"Walkthrough"},{"location":"tracers/","text":"Tracer Types This document was generated with benthos --list-tracers A tracer type represents a destination for Benthos to send opentracing events to such as Jaeger. When a tracer is configured all messages will be allocated a root span during ingestion that represents their journey through a Benthos pipeline. Many Benthos processors create spans, and so opentracing is a great way to analyse the pathways of individual messages as they progress through a Benthos instance. Some inputs, such as http_server and http_client , are capable of extracting a root span from the source of the message (HTTP headers). This is a work in progress and should eventually expand so that all inputs have a way of doing so. A tracer config section looks like this: tracer: type: foo foo: bar: baz WARNING: Although the configuration spec of this component is stable the format of spans, tags and logs created by Benthos is subject to change as it is tuned for improvement. jaeger type: jaeger jaeger: agent_address: localhost:6831 flush_interval: sampler_manager_address: sampler_param: 1 sampler_type: const service_name: benthos tags: {} Send spans to a Jaeger agent. Available sampler types are: const, probabilistic, ratelimiting and remote. none type: none none: {} Do not send opentracing events anywhere.","title":"Tracers"},{"location":"tracers/#tracer-types","text":"This document was generated with benthos --list-tracers A tracer type represents a destination for Benthos to send opentracing events to such as Jaeger. When a tracer is configured all messages will be allocated a root span during ingestion that represents their journey through a Benthos pipeline. Many Benthos processors create spans, and so opentracing is a great way to analyse the pathways of individual messages as they progress through a Benthos instance. Some inputs, such as http_server and http_client , are capable of extracting a root span from the source of the message (HTTP headers). This is a work in progress and should eventually expand so that all inputs have a way of doing so. A tracer config section looks like this: tracer: type: foo foo: bar: baz WARNING: Although the configuration spec of this component is stable the format of spans, tags and logs created by Benthos is subject to change as it is tuned for improvement.","title":"Tracer Types"},{"location":"tracers/#jaeger","text":"type: jaeger jaeger: agent_address: localhost:6831 flush_interval: sampler_manager_address: sampler_param: 1 sampler_type: const service_name: benthos tags: {} Send spans to a Jaeger agent. Available sampler types are: const, probabilistic, ratelimiting and remote.","title":"jaeger"},{"location":"tracers/#none","text":"type: none none: {} Do not send opentracing events anywhere.","title":"none"}]}